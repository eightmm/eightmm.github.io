---
title: "Improved Protein Structure Prediction Using Potentials from Deep Learning"
date: 2026-02-20 11:00:00 +0900
description: "AlphaFold 1ì´ CASP13ì—ì„œ deep learning ê¸°ë°˜ distogram ì˜ˆì¸¡ê³¼ gradient descentë¡œ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì˜ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí•œ ë°©ë²•ì„ ìì„¸íˆ ë¶„ì„í•œë‹¤."
categories: [AI, Protein Structure]
tags: [protein-structure, AlphaFold, distance-prediction, ResNet, CASP13, deep-learning]
math: true
mermaid: true
image:
  path: /assets/img/posts/alphafold1-improved-protein-structure-prediction/fig2.png
  alt: "AlphaFold 1ì˜ folding ê³¼ì • (CASP13 target T0986s2)"
---

> ì´ ê¸€ì€ AlphaFold ì‹œë¦¬ì¦ˆì˜ ì²« ë²ˆì§¸ ê¸€ì´ë‹¤. ì‹œë¦¬ì¦ˆ êµ¬ì„±: AlphaFold 1 (ì´ ê¸€), AlphaFold 2, AlphaFold 3, ì‹œë¦¬ì¦ˆ ì •ë¦¬.
{: .prompt-info }

## Hook

ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì€ ìƒë¬¼í•™ì˜ grand challengeì˜€ë‹¤. 50ë…„ê°„ ìˆ˜ë§ì€ ì´ë¡ ì  ì‹œë„ê°€ ìˆì—ˆì§€ë§Œ, ì‹¤í—˜ êµ¬ì¡°ë§Œí¼ ì •í™•í•œ ì˜ˆì¸¡ì€ ë“œë¬¼ì—ˆë‹¤. 2018ë…„ CASP13ì—ì„œ, DeepMindì˜ AlphaFoldëŠ” free modelling (FM) categoryì—ì„œ 2ìœ„ì™€ ì••ë„ì  ê²©ì°¨(52.8 vs 36.6 summed z-score)ë¡œ ìš°ìŠ¹í•˜ë©° ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì˜ íŒë„ë¥¼ ë°”ê¿¨ë‹¤. 

Fragment assemblyì™€ simulated annealingì´ ì§€ë°°í•˜ë˜ ë¶„ì•¼ì—ì„œ, AlphaFoldëŠ” deep learningìœ¼ë¡œ í•™ìŠµí•œ ë‹¨ë°±ì§ˆë³„ potentialì„ gradient descentë¡œ ìµœì í™”í•˜ëŠ” ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ì„ ì œì‹œí–ˆë‹¤. ì´ì „ì—ëŠ” ë¶ˆê°€ëŠ¥í–ˆë˜ ìƒˆë¡œìš´ foldë“¤ì„ ë†’ì€ ì •í™•ë„ë¡œ ì˜ˆì¸¡í•˜ëŠ” ì‹œëŒ€ê°€ ì—´ë ¸ë‹¤.

## Problem

ê¸°ì¡´ì˜ free modelling ì ‘ê·¼ë²•ë“¤ì€ í¬ê²Œ ë‘ ê°€ì§€ í•œê³„ë¥¼ ê°€ì§€ê³  ìˆì—ˆë‹¤.

### 1. Fragment Assemblyì˜ ë¹„íš¨ìœ¨ì„±

ê°€ì¥ ì„±ê³µì ì¸ FM ë°©ë²•ë“¤(Rosetta, QUARK ë“±)ì€ fragment assemblyì— ì˜ì¡´í–ˆë‹¤. ì´ ë°©ë²•ì€ PDB êµ¬ì¡°ë“¤ì—ì„œ ì¶”ì¶œí•œ í†µê³„ì  potentialì„ ì‚¬ìš©í•˜ì—¬, simulated annealing ê°™ì€ stochastic samplingìœ¼ë¡œ êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. ë¬¸ì œëŠ” êµ¬ì¡° ê°€ì„¤ì„ ë°˜ë³µì ìœ¼ë¡œ ìˆ˜ì •í•˜ë©° ë‚®ì€ potential êµ¬ì¡°ë¥¼ ì°¾ê¸° ìœ„í•´ ìˆ˜ì²œ ë²ˆì˜ moveê°€ í•„ìš”í•˜ê³ , ì´ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì•¼ low-potential êµ¬ì¡°ë“¤ì„ ì¶©ë¶„íˆ íƒìƒ‰í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. 

ê³„ì‚° ë¹„ìš©ì´ í¬ê³ , ì „ì—­ ìµœì í•´ë¥¼ ì°¾ëŠ”ë‹¤ëŠ” ë³´ì¥ë„ ì—†ë‹¤.

### 2. Contact Predictionì˜ ì œí•œì  ì •ë³´

ìµœê·¼ ëª‡ ë…„ê°„ evolutionary covariationì„ ì‚¬ìš©í•œ contact predictionì´ êµ¬ì¡° ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ê°œì„ í–ˆë‹¤. MSAì—ì„œ ë‘ residue ìœ„ì¹˜ì˜ ìƒê´€ê´€ê³„ ë³€í™”ë¥¼ ë¶„ì„í•´ contact (CÎ² atomsê°€ 8 Ã… ì´ë‚´) ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ statistical potentialì— ë°˜ì˜í•˜ì—¬ folding ê³¼ì •ì„ guideí•œë‹¤.

í•˜ì§€ë§Œ contact predictionì€ binary ì •ë³´ë‹¤. "8 Ã… ì´ë‚´ì¸ê°€, ì•„ë‹Œê°€"ë§Œ ì•Œ ìˆ˜ ìˆë‹¤. 4 Ã…ê³¼ 7.9 Ã…ì€ ëª¨ë‘ contactì§€ë§Œ, êµ¬ì¡°ì  ì˜ë¯¸ëŠ” ì™„ì „íˆ ë‹¤ë¥´ë‹¤. ë” ì •í™•í•œ êµ¬ì¡°ë¥¼ ë§Œë“¤ë ¤ë©´ ë” ì„¸ë°€í•œ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤.

## Key Idea

AlphaFoldëŠ” ë‘ ê°€ì§€ í•µì‹¬ ì•„ì´ë””ì–´ë¡œ ì´ ë¬¸ì œë¥¼ í•´ê²°í•œë‹¤.

### Idea 1: Distogram â€” Distance Distribution Prediction

Binary contact ëŒ€ì‹ , **ëª¨ë“  residue pairì˜ ê±°ë¦¬ ë¶„í¬(distance distribution)**ë¥¼ ì˜ˆì¸¡í•œë‹¤. 2-22 Ã… ë²”ìœ„ë¥¼ 64ê°œ binìœ¼ë¡œ ë‚˜ëˆ , ê° binì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•˜ëŠ” ê²ƒì´ distogramì´ë‹¤. 

ì´ëŠ” contact predictionë³´ë‹¤ í›¨ì”¬ í’ë¶€í•œ ì •ë³´ë¥¼ ì œê³µí•œë‹¤. 4 Ã…ê³¼ 7 Ã…ì„ êµ¬ë¶„í•  ìˆ˜ ìˆê³ , ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„±(ë¶„í¬ì˜ ë„“ì´)ë„ ëª¨ë¸ë§í•œë‹¤. ë˜í•œ ë§ì€ ê±°ë¦¬ë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•˜ë©´ì„œ, networkê°€ covariation, local structure, nearby residueì˜ identity ì •ë³´ë¥¼ ì „íŒŒí•˜ê³  í†µí•©í•  ìˆ˜ ìˆë‹¤.

### Idea 2: Gradient Descent Structure Realization

Distogram ì˜ˆì¸¡ìœ¼ë¡œë¶€í„° ë‹¨ë°±ì§ˆë³„ potential $V_{\text{total}}(\phi, \psi)$ë¥¼ êµ¬ì„±í•˜ê³ , ì´ë¥¼ **gradient descentë¡œ ì§ì ‘ ìµœì í™”**í•œë‹¤. Fragment assemblyë‚˜ stochastic sampling ì—†ì´, ë¯¸ë¶„ ê°€ëŠ¥í•œ potentialì„ backbone torsion angles $(\phi, \psi)$ì— ëŒ€í•´ greedyí•˜ê²Œ ìµœì†Œí™”í•œë‹¤.

ì´ˆê¸°í™”ë§Œ ì—¬ëŸ¬ ë²ˆ ë°”ê¿”ê°€ë©° gradient descentë¥¼ ë°˜ë³µí•˜ë©´, ìˆ˜ë°± ë²ˆì˜ iterationë§Œìœ¼ë¡œ ë‚®ì€ potentialì˜ ì •í™•í•œ êµ¬ì¡°ì— ìˆ˜ë ´í•œë‹¤. ê³„ì‚° íš¨ìœ¨ì„±ê³¼ êµ¬ì¡° í’ˆì§ˆì„ ë™ì‹œì— ë‹¬ì„±í•˜ëŠ” ìš°ì•„í•œ í•´ë²•ì´ë‹¤.

## How It Works

AlphaFoldì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ í¬ê²Œ ì„¸ ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤: (1) MSA êµ¬ì„± ë° feature ì¶”ì¶œ, (2) Distogram ì˜ˆì¸¡, (3) Structure realization.

```mermaid
graph TD
    A[Amino acid sequence S] --> B["MSA construction / HHblits + PSI-BLAST"]
    B --> C["Feature extraction / Profile, Covariation, Potts"]
    C --> D["Deep ResNet / 220 residual blocks"]
    D --> E[Distogram P_d_ij|S, MSA]
    D --> F["Torsion distributions / P_Ï†_i,Ïˆ_i|S, MSA"]
    E --> G[Distance potential V_distance]
    F --> H[Torsion potential V_torsion]
    G --> I["Combined potential / V_total = V_dist + V_torsion + V_vdW"]
    H --> I
    I --> J["Gradient descent / L-BFGS on Ï†,Ïˆ"]
    J --> K[Realized structure x = GÏ†,Ïˆ]
    K --> L[Repeat with noisy restarts]
    L --> M[Select lowest-potential structure]
    
```

### 4.1 Overall Pipeline

ì „ì²´ ì‹œìŠ¤í…œì˜ íë¦„ì„ pseudocodeë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.

<details markdown="1">
<summary>ğŸ“ Overall AlphaFold Pipeline Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class AlphaFold:
    def __init__(self):
        self.distogram_net = DistogramNetwork()  # 220 residual blocks
        self.torsion_net = TorsionNetwork()      # Same network, different head
    
    def predict_structure(self, sequence: str) -> Structure:
        # Step 1: MSA construction
        msa = build_msa(sequence)  # HHblits + PSI-BLAST
        
        # Step 2: Feature extraction
        features = extract_features(sequence, msa)
        # - Profile: PSI-BLAST (21), HHblits (22), non-gapped (21)
        # - Covariation: Potts model parameters (484), Frobenius norm (1)
        # - Gap/deletion features
        
        # Step 3: Distogram and torsion prediction
        distogram = self.distogram_net(features)  # LÃ—LÃ—64 bins (2-22 Ã…)
        torsion_dist = self.torsion_net(features)  # LÃ—1296 bins (Ï†,Ïˆ)
        
        # Step 4: Construct protein-specific potential
        V_distance = self.build_distance_potential(distogram)
        V_torsion = self.build_torsion_potential(torsion_dist)
        V_total = V_distance + V_torsion + V_vdW  # Rosetta score2_smooth
        
        # Step 5: Structure realization by gradient descent
        structures = []
        for _ in range(num_restarts):
            # Initialize from predicted torsion distributions
            phi, psi = sample_from(torsion_dist)
            
            # Gradient descent (L-BFGS)
            phi, psi = optimize(V_total, phi, psi, method='L-BFGS')
            
            # Convert torsions to 3D coordinates
            structure = geometry_builder(phi, psi)
            structures.append((V_total(phi, psi), structure))
        
        # Step 6: Noisy restarts from low-potential pool
        pool = sorted(structures)[:20]  # Keep 20 lowest-potential
        for _ in range(num_noisy_restarts):
            potential, structure = random.choice(pool)
            phi, psi = structure.torsions + noise(30Â°)  # Add 30Â° noise
            phi, psi = optimize(V_total, phi, psi, method='L-BFGS')
            structure = geometry_builder(phi, psi)
            structures.append((V_total(phi, psi), structure))
        
        # Return lowest-potential structure
        return min(structures, key=lambda x: x[0])[1]
```

</details>

### 4.2 MSA Construction and Feature Representation

ì…ë ¥ì€ amino acid sequence $S$ë‹¤. ë¨¼ì € Uniclust30 databaseì—ì„œ HHblitsë¡œ ìœ ì‚¬ ì„œì—´ì„ ê²€ìƒ‰í•˜ì—¬ MSAë¥¼ êµ¬ì„±í•œë‹¤ (3 iterations, E-value = $10^{-3}$). ì¶”ê°€ë¡œ PSI-BLASTë¡œ nr datasetì„ ê²€ìƒ‰í•œë‹¤.

MSAë¡œë¶€í„° ë‹¤ìŒ featureë“¤ì„ ì¶”ì¶œí•œë‹¤:

**1ì°¨ì› features (residue ë‹¹):**
- One-hot amino acid type (21)
- Profile features: PSI-BLAST profile (21), HHblits profile (22), non-gapped profile (21), HMM profile (30)
- Potts model bias (22)
- Deletion probability (1)
- Residue index (5 bits + scalar)

**2ì°¨ì› features (residue pair ë‹¹):**
- Potts model parameters (484): MSAë¡œë¶€í„° regularized pseudolikelihoodë¡œ í•™ìŠµí•œ covariation ì •ë³´
- Frobenius norm of Potts parameters (1)
- Gap matrix (1)

ì´ ~650ê°œì˜ featureê°€ ê° 64Ã—64 cropì— ì…ë ¥ëœë‹¤. MSA ê¹Šì´(Neff, effective number of sequences)ê°€ í´ìˆ˜ë¡ covariation signalì´ ê°•í•´ì ¸ distogram ì •í™•ë„ê°€ ì˜¬ë¼ê°„ë‹¤.

> MSA subsampling (ì ˆë°˜ë§Œ ì‚¬ìš©)ê³¼ coordinate noise ì¶”ê°€ë¥¼ data augmentationìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬, shallow MSAì—ì„œë„ robustí•˜ê²Œ ì˜ˆì¸¡í•˜ê³  overfittingì„ ë°©ì§€í•œë‹¤.
{: .prompt-tip }

### 4.3 Distance Prediction Neural Network

Distogramì„ ì˜ˆì¸¡í•˜ëŠ” neural networkëŠ” **220 residual blocksë¡œ êµ¬ì„±ëœ deep 2D convolutional network**ë‹¤. ì´ì „ contact prediction ì—°êµ¬ë“¤ì€ 1D embedding í›„ 2D networkë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, AlphaFoldëŠ” ì²˜ìŒë¶€í„° ëê¹Œì§€ 2Dë¡œ ì²˜ë¦¬í•œë‹¤.

#### Architecture Details

ê° residual blockì€ ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°–ëŠ”ë‹¤:

<details markdown="1">
<summary>ğŸ“ Residual Block Architecture (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class ResidualBlock(nn.Module):
    """AlphaFold distogram prediction residual block
    
    220 blocks total:
    - 7 groups Ã— 4 blocks with 256 channels
    - 48 groups Ã— 4 blocks with 128 channels
    Cycling through dilations: 1, 2, 4, 8
    """
    def __init__(self, channels: int, dilation: int):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(channels)
        self.projection1 = nn.Conv2d(channels, channels, kernel_size=1)
        
        self.bn2 = nn.BatchNorm2d(channels)
        self.dilated_conv = nn.Conv2d(
            channels, channels, 
            kernel_size=3, 
            dilation=dilation,  # 1, 2, 4, or 8
            padding=dilation     # Keep spatial dimensions
        )
        
        self.bn3 = nn.BatchNorm2d(channels)
        self.projection2 = nn.Conv2d(channels, channels, kernel_size=1)
    
    def forward(self, x: Tensor) -> Tensor:
        # x: (batch, channels, 64, 64)
        residual = x
        
        x = self.bn1(x)
        x = F.elu(x)
        x = self.projection1(x)
        
        x = self.bn2(x)
        x = F.elu(x)
        x = self.dilated_conv(x)
        
        x = self.bn3(x)
        x = F.elu(x)
        x = self.projection2(x)
        
        return x + residual  # Skip connection


class DistogramNetwork(nn.Module):
    """Full distogram prediction network"""
    def __init__(self):
        super().__init__()
        
        # Input projection
        self.input_proj = nn.Conv2d(num_features, 256, kernel_size=1)
        
        # 7 groups Ã— 4 blocks with 256 channels
        self.blocks_256 = nn.ModuleList([
            ResidualBlock(256, dilation=(i % 4) * 2 + 1)  # 1,2,4,8 cycle
            for i in range(7 * 4)
        ])
        
        # 48 groups Ã— 4 blocks with 128 channels
        self.downsample = nn.Conv2d(256, 128, kernel_size=1)
        self.blocks_128 = nn.ModuleList([
            ResidualBlock(128, dilation=(i % 4) * 2 + 1)
            for i in range(48 * 4)
        ])
        
        # Output head: distance distribution (64 bins)
        self.output = nn.Conv2d(128, 64, kernel_size=1)
        # Position-specific bias: indexed by residue offset (capped at 32)
        self.position_bias = nn.Parameter(torch.randn(32, 64))
    
    def forward(self, features: Tensor) -> Tensor:
        # features: (batch, num_features, 64, 64)
        x = self.input_proj(features)
        
        # 256-channel blocks
        for block in self.blocks_256:
            x = block(x)
        
        # Downsample to 128 channels
        x = self.downsample(x)
        
        # 128-channel blocks
        for block in self.blocks_128:
            x = block(x)
        
        # Output: distance distribution
        logits = self.output(x)  # (batch, 64, 64, 64) - last 64 is bins
        
        # Add position-specific bias
        for i in range(64):
            for j in range(64):
                offset = min(abs(i - j), 31)
                logits[:, :, i, j] += self.position_bias[offset, :]
        
        # Softmax over bins
        distogram = F.softmax(logits, dim=1)  # (batch, 64_bins, 64, 64)
        return distogram
```

</details>

**í•µì‹¬ ì„¤ê³„:**
- **Dilated convolutions**: dilationì„ 1, 2, 4, 8ë¡œ ìˆœí™˜í•˜ë©° ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì „íŒŒ. 64Ã—64 cropì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ residue pair ê°„ì—ë„ ì •ë³´ êµí™˜ ê°€ëŠ¥.
- **Deep architecture**: 220ê°œ residual blocksê°€ ë³µì¡í•œ covariation íŒ¨í„´ê³¼ local structure ì œì•½ì„ í•™ìŠµ.
- **2D throughout**: 1D embedding ì—†ì´ ì²˜ìŒë¶€í„° 2D feature mapìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ spatial correlationì„ ìµœëŒ€í•œ í™œìš©.

ì¶œë ¥ì€ $L \times L \times 64$ í¬ê¸°ì˜ distogramìœ¼ë¡œ, ê° residue pair $(i,j)$ì— ëŒ€í•´ 64ê°œ ê±°ë¦¬ bin (2-22 Ã…)ì˜ í™•ë¥  ë¶„í¬ $P(d_{ij} | S, \text{MSA}(S))$ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

### 4.4 Cropped Distograms and Ensembling

ë©”ëª¨ë¦¬ ì œì•½ê³¼ overfitting ë°©ì§€ë¥¼ ìœ„í•´, networkëŠ” í•­ìƒ **64Ã—64 crop**ì—ì„œ í•™ìŠµí•˜ê³  í…ŒìŠ¤íŠ¸í•œë‹¤. í•˜ë‚˜ì˜ ë‹¨ë°±ì§ˆë¡œë¶€í„° ìˆ˜ì²œ ê°œì˜ ë‹¤ë¥¸ cropì„ ìƒì„±í•  ìˆ˜ ìˆì–´ ê°•ë ¥í•œ data augmentation íš¨ê³¼ë¥¼ ë‚¸ë‹¤.

ì „ì²´ $L \times L$ distogramì„ ì˜ˆì¸¡í•˜ë ¤ë©´:
1. ì—¬ëŸ¬ offsetìœ¼ë¡œ 64Ã—64 cropì„ tileí•˜ì—¬ ì „ì²´ ê±°ë¦¬ í–‰ë ¬ì„ ì»¤ë²„
2. ê° cropì˜ ì˜ˆì¸¡ì„ í‰ê·  (crop ì¤‘ì•™ë¶€ì— ë†’ì€ ê°€ì¤‘ì¹˜)
3. ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµí•œ 4ê°œ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ensemble

ì´ë ‡ê²Œ êµ¬ì„±í•œ distogramì€ ë†’ì€ ì •í™•ë„ì™€ ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§ì„ ë³´ì¸ë‹¤ (Fig. 3). ì˜ˆì¸¡ ë¶„í¬ì˜ í‘œì¤€í¸ì°¨ê°€ ë‚®ì„ìˆ˜ë¡ ì‹¤ì œ ê±°ë¦¬ì™€ì˜ ì˜¤ì°¨ê°€ ì‘ë‹¤.

### 4.5 Potential Construction

Distogramê³¼ torsion distributionìœ¼ë¡œë¶€í„° ë¯¸ë¶„ ê°€ëŠ¥í•œ potentialì„ êµ¬ì„±í•œë‹¤.

#### Distance Potential

ê° ê±°ë¦¬ ë¶„í¬ë¥¼ cubic splineìœ¼ë¡œ ë³´ê°„í•˜ì—¬ smooth functionì„ ë§Œë“¤ê³ , negative log probabilityë¥¼ í•©ì‚°í•œë‹¤:

$$
V_{\text{distance}}(\phi, \psi) = \sum_{i < j} -\log P(d_{ij}(\phi, \psi) | S, \text{MSA}(S))
$$

ì—¬ê¸°ì„œ $d_{ij}(\phi, \psi) = \|x_i(\phi, \psi) - x_j(\phi, \psi)\|$ëŠ” torsion anglesë¡œë¶€í„° ê³„ì‚°í•œ CÎ² ì¢Œí‘œ ê°„ ê±°ë¦¬ë‹¤.

**Reference distribution correction**: ë‹¨ìˆœíˆ negative log probabilityë¥¼ ì“°ë©´ priorê°€ ê³¼ëŒ€í‘œí˜„ëœë‹¤. ì„œì—´ê³¼ ë¬´ê´€í•˜ê²Œ ë‹¨ë°±ì§ˆ ê¸¸ì´ë§Œìœ¼ë¡œ í•™ìŠµí•œ reference distribution $P(d_{ij}|\text{length})$ë¥¼ ë¹¼ì„œ ë³´ì •í•œë‹¤:

$$
V_{\text{distance}} = \sum_{i < j} \left[ -\log P(d_{ij} | S, \text{MSA}) + \log P(d_{ij} | \text{length}) \right]
$$

ì´ëŠ” log-likelihood ratio í˜•íƒœë¡œ, sequence-specific informationë§Œ ë‚¨ê¸´ë‹¤.

#### Torsion Potential

Networkì˜ ë³„ë„ output headëŠ” ê° residueì˜ $(\phi_i, \psi_i)$ marginal distributionì„ 1296ê°œ bin (10Â° ê°„ê²©)ìœ¼ë¡œ ì˜ˆì¸¡í•œë‹¤. ì´ë¥¼ unimodal von Mises distributionìœ¼ë¡œ fittingí•˜ì—¬:

$$
V_{\text{torsion}}(\phi, \psi) = \sum_i -\log P(\phi_i, \psi_i | S, \text{MSA}(S))
$$

#### Combined Potential

ìµœì¢… potentialì€ ì„¸ í•­ì˜ í•©ì´ë‹¤:

$$
V_{\text{total}}(\phi, \psi) = V_{\text{distance}} + V_{\text{torsion}} + V_{\text{vdW}}
$$

ì—¬ê¸°ì„œ $V_{\text{vdW}}$ëŠ” Rosettaì˜ score2_smooth van der Waals termìœ¼ë¡œ steric clashë¥¼ ë°©ì§€í•œë‹¤. Cross-validation ê²°ê³¼ ì„¸ í•­ì— equal weightingì„ ì ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¢‹ì•˜ë‹¤.

### 4.6 Structure Realization by Gradient Descent

Potentialì´ ë¯¸ë¶„ ê°€ëŠ¥í•˜ë¯€ë¡œ, backbone torsion angles $(\phi, \psi)$ë¥¼ ë³€ìˆ˜ë¡œ gradient descentë¥¼ ì ìš©í•œë‹¤.

<details markdown="1">
<summary>ğŸ“ Gradient Descent Structure Realization (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
def realize_structure(distogram, torsion_dist, sequence):
    """Realize protein structure by gradient descent
    
    Args:
        distogram: LÃ—LÃ—64 distance distribution predictions
        torsion_dist: LÃ—1296 torsion angle distribution predictions
        sequence: amino acid sequence (length L)
    
    Returns:
        Best structure (lowest potential)
    """
    L = len(sequence)
    
    # Build differentiable potentials
    V_distance = build_distance_potential(distogram, sequence)
    V_torsion = build_torsion_potential(torsion_dist)
    V_vdW = lambda phi, psi: rosetta_score2_smooth(phi, psi)
    
    def V_total(phi, psi):
        return V_distance(phi, psi) + V_torsion(phi, psi) + V_vdW(phi, psi)
    
    # Pool of low-potential structures
    pool = []
    
    # Phase 1: Initial sampling from predicted torsion distributions
    for restart in range(500):
        # Sample initial torsions from von Mises fitted distributions
        phi_init = sample_von_mises(torsion_dist[:, :18])  # Ï†
        psi_init = sample_von_mises(torsion_dist[:, 18:])  # Ïˆ
        
        # Gradient descent with L-BFGS
        phi, psi = lbfgs_optimize(
            V_total, 
            x0=(phi_init, psi_init),
            max_iter=1200,
            tolerance=1e-5
        )
        
        # Build 3D structure from optimized torsions
        structure = geometry_builder(phi, psi, sequence)
        potential = V_total(phi, psi)
        
        pool.append((potential, structure))
        pool = sorted(pool)[:20]  # Keep 20 lowest
    
    # Phase 2: Noisy restarts from pool
    for restart in range(4500):
        if random.random() < 0.9:
            # 90%: noisy restart from pool
            _, structure = random.choice(pool)
            phi_init, psi_init = structure.torsions
            # Add 30Â° noise
            phi_init += np.random.normal(0, 30Â°, size=L)
            psi_init += np.random.normal(0, 30Â°, size=L)
        else:
            # 10%: fresh sample from torsion distributions
            phi_init = sample_von_mises(torsion_dist[:, :18])
            psi_init = sample_von_mises(torsion_dist[:, 18:])
        
        # Gradient descent
        phi, psi = lbfgs_optimize(
            V_total, 
            x0=(phi_init, psi_init),
            max_iter=1200
        )
        
        structure = geometry_builder(phi, psi, sequence)
        potential = V_total(phi, psi)
        
        pool.append((potential, structure))
        pool = sorted(pool)[:20]
    
    # Return lowest-potential structure
    best_potential, best_structure = pool[0]
    return best_structure


def lbfgs_optimize(V, x0, max_iter=1200, tolerance=1e-5):
    """L-BFGS optimization of torsion angles
    
    Each step:
    1. Compute V(Ï†, Ïˆ) and gradients âˆ‡_Ï† V, âˆ‡_Ïˆ V
    2. Update Ï†, Ïˆ with L-BFGS step
    3. Check convergence
    """
    phi, psi = x0
    
    for step in range(max_iter):
        # Compute potential and gradients
        potential = V(phi, psi)
        grad_phi = gradient(V, phi, wrt='phi')
        grad_psi = gradient(V, psi, wrt='psi')
        
        # L-BFGS update (maintains history of gradients)
        phi, psi = lbfgs_step(phi, psi, grad_phi, grad_psi)
        
        # Check convergence
        if np.linalg.norm([grad_phi, grad_psi]) < tolerance:
            break
    
    return phi, psi


def geometry_builder(phi, psi, sequence):
    """Build 3D coordinates from torsion angles
    
    Uses ideal bond lengths and angles:
    - N-CÎ±: 1.46 Ã…
    - CÎ±-C: 1.52 Ã…
    - C-N: 1.33 Ã…
    - Bond angles: N-CÎ±-C = 110Â°, CÎ±-C-N = 117Â°
    """
    coords = []
    # Initialize first residue at origin
    coords.append(np.array([0, 0, 0]))  # N
    
    for i, aa in enumerate(sequence):
        # Build backbone atoms using Ï†, Ïˆ
        N = coords[-1]
        Ca = N + rotation(phi[i]) @ np.array([1.46, 0, 0])
        C = Ca + rotation(psi[i]) @ np.array([1.52, 0, 0])
        
        # CÎ² (or CÎ± for glycine)
        if aa == 'G':
            Cb = Ca
        else:
            Cb = Ca + np.array([0, 1.52, 0])  # Simplified
        
        coords.extend([Ca, C, Cb])
    
    return Structure(coords, sequence)
```

</details>

**Gradient Descent ê³¼ì • (Fig. 2c ì°¸ì¡°):**
1. **Initialization**: Predicted torsion distributionì—ì„œ $(\phi, \psi)$ sampling
2. **Optimization**: L-BFGSë¡œ $V_{\text{total}}$ë¥¼ 1200 steps ìµœì í™”
3. **Pooling**: ë‚®ì€ potentialì˜ êµ¬ì¡° 20ê°œë¥¼ poolì— ìœ ì§€
4. **Noisy restarts**: Poolì—ì„œ ì„ íƒí•œ êµ¬ì¡°ì— 30Â° noiseë¥¼ ì¶”ê°€í•´ ì¬ìµœì í™” (90%), ë˜ëŠ” fresh sampling (10%)
5. **Convergence**: ìˆ˜ë°± ë²ˆ ë°˜ë³µ í›„ lowest-potential êµ¬ì¡° ì„ íƒ

ê° gradient descent stepì€ greedyí•˜ê²Œ potentialì„ ë‚®ì¶”ì§€ë§Œ, ì „ì—­ì ì¸ conformational changeë¥¼ ì¼ìœ¼ì¼œ ì˜ packingëœ êµ¬ì¡°ë¡œ ìˆ˜ë ´í•œë‹¤. Noisy restart ë•ë¶„ì— fresh samplingë³´ë‹¤ ë†’ì€ TM scoreë¥¼ ë‹¬ì„±í•œë‹¤ (í‰ê·  0.641 vs 0.636).

> Gradient descentëŠ” simulated annealingë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ë‹¤. ìˆ˜ë°± ë²ˆì˜ restartë¡œ ìˆ˜ë ´í•˜ëŠ” ë°˜ë©´, fragment assemblyëŠ” ìˆ˜ì²œ-ìˆ˜ë§Œ ë²ˆì˜ moveê°€ í•„ìš”í•˜ë‹¤.
{: .prompt-tip }

### 4.7 Training and Auxiliary Losses

NetworkëŠ” cross-entropy lossë¡œ í•™ìŠµí•œë‹¤:

$$
\mathcal{L}_{\text{distance}} = -\sum_{i,j} \log P(d_{ij}^{\text{true}} | S, \text{MSA}(S))
$$

ì—¬ê¸°ì„œ $d_{ij}^{\text{true}}$ëŠ” PDB êµ¬ì¡°ì˜ ì‹¤ì œ CÎ² ê±°ë¦¬ê°€ ì†í•œ binì´ë‹¤.

ì¶”ê°€ë¡œ auxiliary lossesë¥¼ ì‚¬ìš©í•˜ì—¬ one-dimensional representationì„ ê°œì„ í•œë‹¤:
- **Secondary structure prediction**: 8-class DSSP labels (weight 0.005)
- **Accessible surface area**: Relative ASA prediction (weight 0.001)

ì´ auxiliary headsëŠ” 2D activationì„ mean/max poolingí•˜ì—¬ 1Dë¡œ ë³€í™˜ í›„ ì˜ˆì¸¡í•œë‹¤. Secondary structure Q3 accuracy 84%ë¡œ state-of-the-art ìˆ˜ì¤€ì´ë‹¤.

**Training setup:**
- Batch size: 4 crops Ã— 8 GPUs = 32
- Optimizer: Synchronized SGD with 0.85 dropout
- Learning rate: 0.06, decayed by 50% at 150k, 200k, 250k, 350k steps
- Training time: 5 days for 600k steps

### 4.8 Full Chains Without Domain Segmentation

ê¸´ ë‹¨ë°±ì§ˆì€ ì „í†µì ìœ¼ë¡œ domainsë¡œ ë¶„í• í•˜ì—¬ ë…ë¦½ì ìœ¼ë¡œ foldingí–ˆë‹¤. í•˜ì§€ë§Œ domain segmentation ìì²´ê°€ ì–´ë µê³  error-proneí•˜ë‹¤.

AlphaFoldëŠ” **ì „ì²´ chainì„ í•œ ë²ˆì— folding**í•œë‹¤. Sliding window ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ í¬ê¸°(64, 128, 256 residues)ì˜ subsequence MSAë¥¼ ê³„ì‚°í•˜ê³ , ê°ê°ì˜ distogramì„ í‰ê· í•˜ì—¬ full-chain distogramì„ ë§Œë“ ë‹¤. MSA ê¹Šì´ë¡œ ê°€ì¤‘ í‰ê· í•˜ë©´, alignmentê°€ ë§ì€ regionì—ì„œ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ ì–»ëŠ”ë‹¤.

ì´ ë°©ì‹ì€ domain boundaryë¥¼ ëª¨ë¥´ëŠ” ìƒí™©ì—ì„œë„ ì „ì²´ êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

## Results

AlphaFoldëŠ” CASP13ì—ì„œ ì••ë„ì  ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

### Free Modelling Performance

| Metric | AlphaFold | 2nd Place (Group 322) |
|--------|-----------|----------------------|
| **FM summed z-score** | **52.8** | 36.6 |
| **FM+FM/TBM z-score** | **68.3** | 48.2 |
| FM domains with TM > 0.6 | **22** | 10 |

AlphaFoldëŠ” FM categoryì—ì„œ 2ìœ„ë³´ë‹¤ **44% ë†’ì€ ì ìˆ˜**ë¥¼ ê¸°ë¡í–ˆë‹¤. íŠ¹íˆ 0.6-0.7 TM score ë²”ìœ„ì—ì„œ ë‹¤ë¥¸ ëª¨ë“  ì‹œìŠ¤í…œì„ ì••ë„í•˜ë©°, ì´ì „ì—ëŠ” ë¶ˆê°€ëŠ¥í–ˆë˜ ì •í™•ë„ì˜ ìƒˆë¡œìš´ fold ì˜ˆì¸¡ë“¤ì„ ìƒì‚°í–ˆë‹¤ (Fig. 1a).

![AlphaFold CASP13 performance](/assets/img/posts/alphafold1-improved-protein-structure-prediction/fig1.png)
_Figure 1: (a) FM domains predicted at given TM-score threshold. AlphaFoldê°€ 0.6-0.7 ë²”ìœ„ì—ì„œ ì••ë„ì . (b) ìƒˆë¡œìš´ 6ê°œ foldì— ëŒ€í•œ TM score ë¹„êµ. (c) Long-range contact prediction precision â€” AlphaFold distogramì´ ìµœê³  ì •í™•ë„._

### Contact Prediction Accuracy

Distogramì„ 8 Ã… thresholdë¡œ binary contact predictionìœ¼ë¡œ ë³€í™˜í•˜ë©´, long-range contact predictionì—ì„œë„ state-of-the-artë¥¼ ë‹¬ì„±í•œë‹¤ (Fig. 1c). Top L, L/2, L/5 contactsì—ì„œ ëª¨ë‘ highest precisionì„ ê¸°ë¡í–ˆë‹¤.

ì´ëŠ” distogramì´ í’ë¶€í•œ ì •ë³´ë¥¼ ë‹´ê³  ìˆì–´, ë‹¨ìˆœíˆ thresholdingí•´ë„ ê¸°ì¡´ contact prediction ì „ìš© ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•¨ì„ ë³´ì—¬ì¤€ë‹¤.

### Distogram Accuracy and Structure Quality

Distogram lDDT (DLDDT12)ì™€ realized structureì˜ TM score ê°„ ê°•í•œ ìƒê´€ê´€ê³„ê°€ ìˆë‹¤ (Pearson r = 0.92, Fig. 4a). ì¦‰, distogram ìì²´ê°€ ì •í™•í•˜ë©´ ìµœì¢… êµ¬ì¡°ë„ ì •í™•í•˜ë‹¤.

![Distogram accuracy vs TM score](/assets/img/posts/alphafold1-improved-protein-structure-prediction/fig4.png)
_Figure 4: (a) TM score vs distogram lDDT â€” ë†’ì€ ìƒê´€ê´€ê³„. (b) Potentialì˜ ê° componentë¥¼ ì œê±°í–ˆì„ ë•Œ TM score ë³€í™” â€” distance potentialì´ ê°€ì¥ ì¤‘ìš”._

Distance potentialì„ ì™„ì „íˆ ì œê±°í•˜ë©´ TM scoreê°€ 0.266ìœ¼ë¡œ ë–¨ì–´ì§„ë‹¤ (Fig. 4b). Torsion potential, reference correction, van der Waals termì€ ê°ê° ì†Œí­ ê¸°ì—¬í•˜ì§€ë§Œ, distance potentialì´ ì••ë„ì ìœ¼ë¡œ ì¤‘ìš”í•˜ë‹¤.

### Template-Based Modelling

AlphaFoldëŠ” FM ë°©ë²•ì„ì—ë„ TBM categoryì—ì„œë„ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. Assessors' formulaë¡œ **TBM top-oneì—ì„œ 4ìœ„, best-of-fiveì—ì„œ 1ìœ„**ë¥¼ ì°¨ì§€í–ˆë‹¤. Template ì—†ì´ë„ homology modeling ìˆ˜ì¤€ì˜ ì •í™•ë„ì— ë„ë‹¬í•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.

## Discussion

AlphaFoldëŠ” protein structure predictionì—ì„œ ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì„ ì œì‹œí–ˆì§€ë§Œ, ë…¼ë¬¸ì€ ëª‡ ê°€ì§€ í•œê³„ì™€ í–¥í›„ ë°©í–¥ì„ ë°íˆê³  ìˆë‹¤.

### MSA Depth Dependency

Distogram ì •í™•ë„ëŠ” MSAì˜ effective number of sequences (Neff)ì— í¬ê²Œ ì˜ì¡´í•œë‹¤. Shallow MSA (Neffê°€ ë‚®ì€ ê²½ìš°)ì—ì„œëŠ” covariation signalì´ ì•½í•´ ì˜ˆì¸¡ ì •í™•ë„ê°€ ë–¨ì–´ì§„ë‹¤. MSA subsampling augmentationìœ¼ë¡œ ì–´ëŠ ì •ë„ ì™„í™”í–ˆì§€ë§Œ, orphan proteinsì´ë‚˜ ìµœê·¼ì— ë°œê²¬ëœ ì„œì—´ì€ ì—¬ì „íˆ ì–´ë µë‹¤.

### FM vs TBM Gap

FM ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒë˜ì—ˆì§€ë§Œ, TBMì— ë¹„í•˜ë©´ ì—¬ì „íˆ gapì´ ìˆë‹¤. ë…¼ë¬¸ì€ "FM targets still lag behind TBM targets and cannot yet be relied on for detailed understanding of hard structures"ë¼ê³  ë°í˜”ë‹¤. Side-chain configurationì´ë‚˜ binding siteì˜ ì„¸ë°€í•œ êµ¬ì¡°ê¹Œì§€ ì‹ ë¢°í•˜ê¸°ëŠ” ì–´ë µë‹¤.

### Gradient Descent Local Minima

Gradient descentëŠ” local minimaì— ë¹ ì§ˆ ìˆ˜ ìˆë‹¤. Noisy restartë¡œ ì–´ëŠ ì •ë„ í•´ê²°í•˜ì§€ë§Œ, ë§¤ìš° ë³µì¡í•œ topologyë¥¼ ê°€ì§„ ë‹¨ë°±ì§ˆì—ì„œëŠ” global optimumì„ ì°¾ì§€ ëª»í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ë…¼ë¬¸ì€ "no guarantee of finding global optimum"ì„ ì¸ì •í•œë‹¤.

### Biological Applications

ë…¼ë¬¸ì€ AlphaFold ì˜ˆì¸¡ì´ biological insightsë¥¼ ì œê³µí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì— ë„ë‹¬í•˜ê¸° ì‹œì‘í–ˆë‹¤ê³  ì£¼ì¥í•œë‹¤. Contact predictionsë§Œìœ¼ë¡œë„ mutation targetingì— ìœ ìš©í•˜ê³ , ì˜ˆì¸¡ êµ¬ì¡°ê°€ protein-protein interface prediction, binding pocket identification, molecular replacement in crystallographyì—ì„œ ê°œì„ ì„ ë³´ì˜€ë‹¤ (Extended Data Figs. 6-8 ì°¸ì¡°).

ì €ìë“¤ì€ "we hope that the methods we have described can be developed further and applied to benefit all areas of protein science"ë¼ë©° í–¥í›„ ë°œì „ ë°©í–¥ì„ ì œì‹œí–ˆë‹¤. ì´ëŠ” 2ë…„ í›„ AlphaFold 2ë¡œ ì´ì–´ì§„ë‹¤.

## Limitations

1. **MSA ì˜ì¡´ì„±**: ìœ ì‚¬ ì„œì—´ì´ ì ì€ ë‹¨ë°±ì§ˆ(orphan protein)ì—ì„œëŠ” MSA qualityê°€ ë–¨ì–´ì ¸ ì •í™•ë„ê°€ ê¸‰ê²©íˆ ê°ì†Œí•œë‹¤.
2. **ë‹¨ì¼ ë„ë©”ì¸ ì œí•œ**: Multi-domain proteinì˜ domain ê°„ ìƒëŒ€ì  ë°°ì¹˜ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤. ê° domainì„ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡í•œ í›„ ì¡°í•©í•˜ëŠ” ë°©ì‹ì˜ í•œê³„ê°€ ìˆë‹¤.
3. **Gradient descent ìµœì í™”ì˜ local minima**: L-BFGSë¡œ ì—ë„ˆì§€ landscapeë¥¼ íƒìƒ‰í•˜ë¯€ë¡œ, ì´ˆê¸°ê°’ì— ë”°ë¼ local minimumì— ë¹ ì§ˆ ìˆ˜ ìˆë‹¤. ì—¬ëŸ¬ random seedë¡œ ë°˜ë³µ ìµœì í™”ê°€ í•„ìš”í•˜ë‹¤.
4. **Distogram í•´ìƒë„ í•œê³„**: 64 binìœ¼ë¡œ ì´ì‚°í™”ëœ ê±°ë¦¬ ë¶„í¬ëŠ” ë¯¸ì„¸í•œ ì›ì ê°„ ê±°ë¦¬ ì°¨ì´ë¥¼ í¬ì°©í•˜ê¸° ì–´ë µê³ , backbone torsion angleë§Œ ì˜ˆì¸¡í•˜ë¯€ë¡œ side-chain ë°°ì¹˜ê°€ ë¶€ì •í™•í•˜ë‹¤.
5. **End-to-endê°€ ì•„ë‹˜**: Feature extraction â†’ distance prediction â†’ structure optimizationì´ ë¶„ë¦¬ë˜ì–´ ìˆì–´, ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ joint optimizationì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.

## Conclusion

AlphaFold 1ì€ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì˜ íŒ¨ëŸ¬ë‹¤ì„ì„ fragment assemblyì—ì„œ distance distribution predictionìœ¼ë¡œ ì „í™˜ì‹œí‚¨ íšê¸°ì ì¸ ì—°êµ¬ë‹¤. Distogramì´ë¼ëŠ” í’ë¶€í•œ inter-residue distance distribution í‘œí˜„ê³¼, ì´ë¥¼ differentiableí•œ potentialë¡œ ë³€í™˜í•˜ì—¬ gradient descentë¡œ êµ¬ì¡°ë¥¼ ìµœì í™”í•˜ëŠ” ì ‘ê·¼ë²•ì€ CASP13ì—ì„œ 1ìœ„ë¥¼ ì°¨ì§€í–ˆë‹¤. Deep ResNet ê¸°ë°˜ì˜ distance predictionê³¼ torsion predictionì˜ ì¡°í•©ì€ ì´í›„ AlphaFold 2ì˜ end-to-end êµ¬ì¡° ì˜ˆì¸¡ìœ¼ë¡œ ê°€ëŠ” í•µì‹¬ ë°œíŒì´ ë˜ì—ˆë‹¤.

## TL;DR

- **ë¬¸ì œ**: Fragment assemblyëŠ” ëŠë¦¬ê³ , contact predictionì€ binary ì •ë³´ë§Œ ì œê³µí•˜ì—¬ ì •í™•í•œ êµ¬ì¡° ì˜ˆì¸¡ì´ ì–´ë ¤ì›€
- **í•´ë²•**: Deep ResNet (220 blocks)ìœ¼ë¡œ inter-residue distance distribution (distogram)ì„ ì˜ˆì¸¡í•˜ê³ , ì´ë¡œë¶€í„° ë‹¨ë°±ì§ˆë³„ potentialì„ êµ¬ì„±í•˜ì—¬ gradient descentë¡œ êµ¬ì¡° ìµœì í™”
- **ê²°ê³¼**: CASP13 FM categoryì—ì„œ ì••ë„ì  1ìœ„ (52.8 vs 36.6 z-score), ì´ì „ì— ë¶ˆê°€ëŠ¥í–ˆë˜ ìƒˆë¡œìš´ foldë“¤ì„ ë†’ì€ ì •í™•ë„ë¡œ ì˜ˆì¸¡

## Paper Info

| í•­ëª© | ë‚´ìš© |
|------|------|
| **Title** | Improved protein structure prediction using potentials from deep learning |
| **Authors** | Andrew W. Senior et al. (DeepMind) |
| **Venue** | Nature, Volume 577 (2020) |
| **Published** | 2020-01-15 |
| **Link** | [doi:10.1038/s41586-019-1923-7](https://doi.org/10.1038/s41586-019-1923-7) |
| **Paper** | [Nature](https://www.nature.com/articles/s41586-019-1923-7) |
| **Code** | [GitHub](https://github.com/deepmind/deepmind-research/tree/master/alphafold_casp13) |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
