---
title: "SeedFold: Scaling Biomolecular Structure Prediction"
date: 2026-02-24 13:30:00 +0900
description: "ByteDanceì˜ SeedFoldëŠ” Pairformerì˜ width scaling, linear triangular attention, ëŒ€ê·œëª¨ distillation ë°ì´í„°ë¥¼ í†µí•´ AlphaFold3ë¥¼ ëŠ¥ê°€í•˜ëŠ” biomolecular structure prediction ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤."
categories: [Paper Review, Protein Structure]
tags: [protein-folding, alphafold3, linear-attention, scaling, pairformer, triangular-attention, seedfold, distillation]
math: true
mermaid: true
image:
  path: /assets/img/posts/seedfold-scaling-biomolecular-structure-prediction/fig1_overview.png
  alt: "SeedFold ê°œìš”: model scaling, linear triangular attention, data distillation"
---

AlphaFold3 ì´í›„ folding ëª¨ë¸ë“¤ì€ ëŒ€ë¶€ë¶„ ë¹„ìŠ·í•œ ì•„í‚¤í…ì²˜ ì„¤ì •(128-width Pairformer, 48 layers)ì„ ë‹µìŠµí•´ì™”ë‹¤. ByteDance Seed íŒ€ì˜ SeedFoldëŠ” ì´ ê´€ì„±ì— ì •ë©´ìœ¼ë¡œ ì˜ë¬¸ì„ ë˜ì§„ë‹¤: **í˜„ì¬ ëª¨ë¸ ìš©ëŸ‰ì´ ì •ë§ ì¶©ë¶„í•œê°€? ìŠ¤ì¼€ì¼ë§ì˜ ì˜¬ë°”ë¥¸ ë°©í–¥ì€ ë¬´ì—‡ì¸ê°€?**

ê²°ë¡ ë¶€í„° ë§í•˜ë©´, **widthê°€ ë‹µì´ë‹¤.** Pairformerì˜ pair representation ì°¨ì›ì„ 128ì—ì„œ 512ë¡œ í‚¤ìš°ëŠ” ê²ƒì´ depthë¥¼ ë‘ ë°°ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ íš¨ê³¼ì ì´ë¼ëŠ” ê²ƒì„ ì‹¤í—˜ìœ¼ë¡œ ë³´ì—¬ì¤€ë‹¤. ì—¬ê¸°ì— linear triangular attentionìœ¼ë¡œ ê³„ì‚° ë³‘ëª©ì„ í•´ì†Œí•˜ê³ , 26.5M ê·œëª¨ì˜ distillation ë°ì´í„°ì…‹ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ FoldBenchì—ì„œ AlphaFold3ë¥¼ ëŒ€ë¶€ë¶„ì˜ taskì—ì„œ ëŠ¥ê°€í•œë‹¤.

## Problem: AlphaFold3 ì•„í‚¤í…ì²˜ì˜ ìŠ¤ì¼€ì¼ë§ ë³‘ëª©

AlphaFold3 ì´í›„ open-source folding ëª¨ë¸ë“¤(Boltz-1, Protenix, Chai-1)ì€ Pairformer êµ¬ì¡°ë¥¼ ê±°ì˜ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•œë‹¤. ì¼ë°˜ì ì¸ ì„¤ì •:

- **Pair representation dimension**: 128
- **Pairformer layers**: 48
- **Recycling**: 3~9íšŒ

ë¬¸ì œëŠ” **triangular attentionì˜ $O(n^3 d)$ ë³µì¡ë„**ê°€ ëª¨ë¸ ìŠ¤ì¼€ì¼ë§ì„ ê·¼ë³¸ì ìœ¼ë¡œ ì œí•œí•œë‹¤ëŠ” ì ì´ë‹¤. Widthë¥¼ í‚¤ìš°ë©´ ê³„ì‚° ë¹„ìš©ì´ í­ë°œí•˜ê³ , depthë¥¼ í‚¤ìš°ë©´ recyclingê³¼ ê²¹ì¹˜ë©´ì„œ í•œê³„ íš¨ê³¼ê°€ ì¤„ì–´ë“ ë‹¤.

ë˜í•œ AlphaFold3ì˜ Structure Moduleì€ AlphaFold2ì˜ IPA(Invariant Point Attention)ë¥¼ ë²”ìš© Transformerë¡œ ëŒ€ì²´í–ˆëŠ”ë°, ì´ëŠ” íšŒì „/ë³‘ì§„ ë¶ˆë³€ì„±ì´ë¼ëŠ” inductive biasë¥¼ ì œê±°í•œ ê²ƒì´ë‹¤. ë”°ë¼ì„œ **ëŒ€ê·œëª¨ ë°ì´í„° ì—†ì´ëŠ” ì¼ë°˜í™”ê°€ ì–´ë µë‹¤.**

> SeedFoldì˜ í•µì‹¬ ì§ˆë¬¸: depthê°€ ì•„ë‹Œ **width**ë¥¼ í‚¤ìš°ê³ , triangular attentionì„ **linear**ìœ¼ë¡œ ë°”ê¾¸ê³ , ë°ì´í„°ë¥¼ **147ë°°** ëŠ˜ë¦¬ë©´ ì–´ë–»ê²Œ ë˜ëŠ”ê°€?
{: .prompt-tip }

## Key Idea: Width > Depth, ê·¸ë¦¬ê³  Linear Triangular Attention

SeedFoldì˜ ì„¸ ê°€ì§€ ê¸°ì—¬ë¥¼ í•œ ë¬¸ì¥ì”© ìš”ì•½í•˜ë©´:

1. **Width Scaling**: Pairformerì˜ pair representation ì°¨ì›ì„ 128 â†’ 512ë¡œ í‚¤ìš°ëŠ” ê²ƒì´ ê°€ì¥ íš¨ê³¼ì ì¸ ìŠ¤ì¼€ì¼ë§ ì „ëµ
2. **Linear Triangular Attention**: softmax ê¸°ë°˜ triangular attentionì„ linear attentionìœ¼ë¡œ ëŒ€ì²´í•˜ì—¬ $O(n^3 d) \to O(n^2 d^2)$ë¡œ ë³µì¡ë„ ê°ì†Œ
3. **ëŒ€ê·œëª¨ Distillation**: AlphaFold2ë¡œ ìƒì„±í•œ 26.5M êµ¬ì¡° (PDB 180Kì˜ 147ë°°)ë¡œ í•™ìŠµ

## How it works

### Overview

SeedFoldëŠ” AlphaFold3ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë”°ë¥´ë˜, ì„¸ ê°€ì§€ ì¶•ì—ì„œ ìŠ¤ì¼€ì¼ì—…í•œë‹¤.

![SeedFold Overview](/assets/img/posts/seedfold-scaling-biomolecular-structure-prediction/fig1_overview.png)
_Figure 1: SeedFold ê°œìš”. Model(width scaling), Architecture(linear triangular attention), Data(26.5M distillation). ì¶œì²˜: ì› ë…¼ë¬¸_

```mermaid
graph TD
    subgraph Input
        MSA["MSA Module<br/>(Evolutionary Features)"]
        SEQ["Sequence + Templates"]
    end
    
    subgraph Trunk["Pairformer Trunk (Width Scaled)"]
        direction TB
        PAIR["Pair Representation<br/>128â†’256â†’384â†’512 dim"]
        TRI["Triangular Attention<br/>(Vanilla or Linear)"]
        TMUL["Triangular Multiplication"]
        PAIR --> TRI --> TMUL --> PAIR
    end
    
    subgraph Structure["Structure Module"]
        TRANS["Token Transformer<br/>(24 layers)"]
        DIFF["Diffusion Head"]
    end
    
    SEQ --> MSA
    MSA --> Trunk
    Trunk --> Structure
    Structure --> OUT["All-Atom 3D Coordinates"]
    
    style PAIR fill:#fff3e0
    style TRI fill:#e8f5e9
    style OUT fill:#e1f5fe
```

### Representation: Width Scalingì´ ì¤‘ìš”í•œ ì´ìœ 

ê¸°ì¡´ folding ëª¨ë¸ì˜ ìŠ¤ì¼€ì¼ë§ ì „ëµì€ ì£¼ë¡œ **depth**(layer ìˆ˜ ì¦ê°€)ì— ì§‘ì¤‘í–ˆë‹¤. SeedFoldëŠ” ì„¸ ê°€ì§€ ì „ëµì„ ì²´ê³„ì ìœ¼ë¡œ ë¹„êµí•œë‹¤:

| Configuration | Pair Dim | MSA Dim | Pairformer Layers | Structure Layers | Params | Efficiency |
|---|---|---|---|---|---|---|
| Base (128-width) | 128 | 64 | 48 | 24 | 432M | 0.15 iter/s |
| Medium (256-width) | 256 | 128 | 48 | 24 | 533M | 0.10 iter/s |
| **Large (512-width)** | **512** | **256** | **48** | **24** | **923M** | **0.06 iter/s** |
| Deep Pairformer | 128 | 64 | **96** | 24 | 582M | 0.10 iter/s |
| Deep Structure | 128 | 64 | 48 | **48** | 706M | 0.10 iter/s |

![Scaling Strategies Comparison](/assets/img/posts/seedfold-scaling-biomolecular-structure-prediction/fig2_scaling.png)
_Figure 2: ìŠ¤ì¼€ì¼ë§ ì „ëµ ë¹„êµ. Width scaling(128â†’256â†’512)ì´ depth scaling(deeper trunk, deeper structure module)ë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜. ì¶œì²˜: ì› ë…¼ë¬¸_

Width scalingì´ ë” íš¨ê³¼ì ì¸ ì´ìœ ì— ëŒ€í•´ ë…¼ë¬¸ì€ ë‘ ê°€ì§€ ì„¤ëª…ì„ ì œì‹œí•œë‹¤:

1. **Recyclingì´ ì´ë¯¸ depthë¥¼ ê·¼ì‚¬**: 9íšŒ recycling Ã— 48 layers = íš¨ê³¼ì ìœ¼ë¡œ 432 layers. ë¬¼ë¦¬ì  depthë¥¼ 96ìœ¼ë¡œ ëŠ˜ë ¤ë„ ì¶”ê°€ ì´ë“ì´ ì ë‹¤.
2. **Pair representationì´ í•µì‹¬ ë³‘ëª©**: ëª¨ë“  pairwise interactionì´ 128ì°¨ì›ìœ¼ë¡œ ì••ì¶•ë˜ë©´ representation capacityê°€ ë¶€ì¡±. DeepSeek-V3ê°€ layer ìˆ˜ëŠ” 61ì¸ë° hidden sizeë¥¼ 7168ê¹Œì§€ í‚¤ìš´ ê²ƒê³¼ ê°™ì€ ë…¼ë¦¬.

128 â†’ 256 ì „í™˜ì—ì„œ ê°€ì¥ í° ì„±ëŠ¥ í–¥ìƒì´ ë‚˜íƒ€ë‚˜ê³ , 256 â†’ 512ì—ì„œëŠ” diminishing returnsê°€ ê´€ì°°ë˜ì—ˆë‹¤.

### Core Architecture: Linear Triangular Attention

Triangular attentionì€ AlphaFoldì˜ í•µì‹¬ ì—°ì‚°ì´ë‹¤. Pair representation $\mathbf{Z} \in \mathbb{R}^{n \times n \times d}$ì˜ ê° í–‰ $\mathbf{Z}_i$ì— ëŒ€í•´:

$$\text{TriAtt}(\mathbf{Z}_i) = \text{softmax}(\mathbf{Q}_i \mathbf{K}_i^T + \mathbf{B}) \mathbf{V}_i$$

ì—¬ê¸°ì„œ bias $\mathbf{B} = \text{Linear}(\mathbf{Z}) \in \mathbb{R}^{n \times n}$ê°€ $(j,k)$-th coupling ì •ë³´ë¥¼ ë°˜ì˜í•˜ì—¬ "ì‚¼ê°í˜•" ê´€ê³„ë¥¼ ëª¨ë¸ë§í•œë‹¤. ì´ ì—°ì‚°ì˜ ë³µì¡ë„ëŠ” $O(n^3 d)$ â€” ë‹¨ë°±ì§ˆ ê¸¸ì´ì— ëŒ€í•´ **cubic**ì´ë‹¤.

![Linear Triangular Attention](/assets/img/posts/seedfold-scaling-biomolecular-structure-prediction/fig3_linear_attention.png)
_Figure 3: (a) Linear Triangular Attention ì•„í‚¤í…ì²˜. (b) Vanilla vs Linear attentionì˜ peak memory ë° ì‹œê°„ ë¹„êµ. ì¶œì²˜: ì› ë…¼ë¬¸_

SeedFoldëŠ” LLMì—ì„œ ë°œì „í•œ linear attention ê¸°ë²•ì„ triangular attentionì— ì ìš©í•œë‹¤. Softmaxë¥¼ feature map $\phi(\cdot)$ë¡œ ëŒ€ì²´í•˜ë©´ "right product trick"ìœ¼ë¡œ ë³µì¡ë„ë¥¼ ì¤„ì¼ ìˆ˜ ìˆë‹¤:

$$\underbrace{\phi(\mathbf{Q}_i) \phi(\mathbf{K}_i)^T}_{O(n^2 d)} \mathbf{V}_i \to \phi(\mathbf{Q}_i) \underbrace{\phi(\mathbf{K}_i)^T \mathbf{V}_i}_{O(n d^2)}$$

í•˜ì§€ë§Œ ë¬¸ì œê°€ ìˆë‹¤: triangular attentionì˜ **bias term** $\mathbf{B}$ë¥¼ ì–´ë–»ê²Œ linear attentionì— í†µí•©í•˜ëŠ”ê°€? SeedFoldëŠ” ë‘ ê°€ì§€ ë³€í˜•ì„ ì œì•ˆí•œë‹¤:

**Additive Linear Triangular Attention:**

$$\text{AdditiveLinearTriAtt}(\mathbf{Z}_i) = \phi(\mathbf{Q}_i) \underbrace{(\phi(\mathbf{K}_i)^T \mathbf{V}_i)}_{\text{linearized}} + \underbrace{\psi(\mathbf{B}) \mathbf{V}_i}_{\text{amortized}}$$

Bias termì´ ì—¬ì „íˆ $\mathbb{R}^{n \times n}$ì´ì§€ë§Œ, **ëª¨ë“  í–‰ì—ì„œ ê³µìœ **ë˜ë¯€ë¡œ ë©”ëª¨ë¦¬ê°€ amortizeëœë‹¤.

**Gated Linear Triangular Attention:**

$$\text{GatedLinearTriAtt}(\mathbf{Z}_i) = \left(\phi(\mathbf{Q}_i) \phi(\mathbf{K}_i^T) \odot \psi(\mathbf{B})\right) \mathbf{V}_i$$

$\psi = \text{sigmoid}$ë¡œ biasë¥¼ gating ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì‚¬ìš©. Right product trickì€ ì ìš© ë¶ˆê°€í•˜ì§€ë§Œ, CUDA ìµœì í™”ëœ tiled êµ¬í˜„ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ í™•ë³´.

<details markdown="1">
<summary>ğŸ“ Linear Triangular Attention Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class LinearTriangularAttention(nn.Module):
    """SeedFold's Linear Triangular Attention (Gated variant)"""
    
    def __init__(self, d_pair: int, n_heads: int, mode: str = "gated"):
        super().__init__()
        self.mode = mode
        self.n_heads = n_heads
        self.d_head = d_pair // n_heads
        
        # Q, K, V projections from pair representation
        self.proj_q = nn.Linear(d_pair, d_pair)
        self.proj_k = nn.Linear(d_pair, d_pair)
        self.proj_v = nn.Linear(d_pair, d_pair)
        
        # Bias from pair representation â†’ (n, n) per head
        self.proj_bias = nn.Linear(d_pair, n_heads)
        
        # Output: gating + layer norm + linear
        self.gate = nn.Linear(d_pair, d_pair)
        self.layer_norm = nn.LayerNorm(d_pair)
        self.out_proj = nn.Linear(d_pair, d_pair)
    
    def forward(self, Z: Tensor) -> Tensor:
        """
        Z: (batch, n, n, d_pair) â€” pair representation
        Returns: (batch, n, n, d_pair) â€” updated pair representation
        """
        B, n, _, d = Z.shape
        
        for i in range(n):  # row-wise (ì‹¤ì œ êµ¬í˜„ì€ batched)
            Z_i = Z[:, i]  # (B, n, d)
            
            Q_i = self.proj_q(Z_i)  # (B, n, d)
            K_i = self.proj_k(Z_i)  # (B, n, d)
            V_i = self.proj_v(Z_i)  # (B, n, d)
            
            # Feature maps: Ï† = relu, Ïˆ = sigmoid
            phi_Q = F.relu(Q_i)  # (B, n, d)
            phi_K = F.relu(K_i)  # (B, n, d)
            
            # Bias: triangular coupling (j,k)
            B_jk = self.proj_bias(Z)  # â†’ (B, n, n, heads)
            
            if self.mode == "additive":
                # Linearized term: Ï†(Q) @ (Ï†(K)^T @ V) â€” O(n d^2)
                KV = torch.einsum("bnd,bnm->bdm", phi_K, V_i)  # (B, d, d)
                linear_out = torch.einsum("bnd,bdm->bnm", phi_Q, KV)
                # Amortized bias term: Ïˆ(B) @ V â€” still O(n^2) but shared
                bias_out = F.relu(B_jk) @ V_i
                attn_out = linear_out + bias_out
                
            elif self.mode == "gated":
                # Compute full attention but with sigmoid gating
                attn = phi_Q @ phi_K.transpose(-1, -2)  # (B, n, n)
                gate = torch.sigmoid(B_jk)               # (B, n, n)
                attn = attn * gate                        # element-wise
                attn_out = attn @ V_i
            
            # Gated output (Lightning Attention style)
            gate_signal = torch.sigmoid(self.gate(Z_i))
            Z[:, i] = self.out_proj(gate_signal * self.layer_norm(attn_out))
        
        return Z
```

</details>

> Additive variantëŠ” linear attentionì˜ ì´ë¡ ì  ì¥ì (right product trick)ì„ ìœ ì§€í•˜ê³ , Gated variantëŠ” DNA/RNA taskì—ì„œ ë” ê°•í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. SeedFold-LinearëŠ” ìµœì¢…ì ìœ¼ë¡œ GatedLinearTriAttë¥¼ ì±„íƒí–ˆë‹¤.
{: .prompt-info }

### Key Innovation: Width + Linear Attentionì˜ ì‹œë„ˆì§€

Width scalingê³¼ linear attentionì€ ë…ë¦½ì ì´ì§€ ì•Šë‹¤. Widthë¥¼ í‚¤ìš°ë©´ triangular attentionì˜ ê³„ì‚° ë¹„ìš©ì´ $O(n^3 \cdot d_{\text{pair}})$ë¡œ ë” ì»¤ì§€ëŠ”ë°, linear attentionì´ ì´ë¥¼ $O(n^2 \cdot d_{\text{pair}}^2)$ë¡œ ì¤„ì—¬ì¤€ë‹¤. $d_{\text{pair}} < n$ì¸ ê²½ìš°(ëŒ€ë¶€ë¶„ì˜ ë‹¨ë°±ì§ˆ)ì—ëŠ” ì´ê²ƒì´ í° ì ˆê°ì´ ëœë‹¤.

ë‘ ëª¨ë¸ì˜ íŠ¹ì„±ì´ ë‹¤ë¥¸ ì ë„ í¥ë¯¸ë¡­ë‹¤:
- **SeedFold (vanilla, 512-width)**: Antibody-antigenì—ì„œ ìµœê°• (DockQ 53.21%)
- **SeedFold-Linear (gated, 384-width)**: Protein-ligandì—ì„œ ìµœê°• (SR 66.48%)

ë…¼ë¬¸ì€ ì´ë¥¼ heterogeneous attention mechanismì˜ ê°€ì¹˜ë¡œ í•´ì„í•˜ë©°, í–¥í›„ MoE (Mixture of Experts)ë¡œì˜ í™•ì¥ì„ ì œì•ˆí•œë‹¤.

### Training & Data

**ëŒ€ê·œëª¨ Distillation ì „ëµ:**

| Dataset | Type | Samples | Weight |
|---|---|---|---|
| PDB | Experimental | 180K | 0.50 |
| AFDB | Distillation (UniProt) | 3.3M | 0.08 |
| **Mgnify** | **Distillation (Metagenomic)** | **23.1M** | **0.42** |

Mgnify ë°ì´í„°ì…‹ì´ í•µì‹¬ì´ë‹¤: ë©”íƒ€ê²Œë…¸ë¯¹ ë°ì´í„°ë¡œì„œ AFDBì™€ì˜ ì„œì—´ ì¤‘ë³µì´ ê·¹íˆ ë‚®ê³ (2M/23Më§Œ í´ëŸ¬ìŠ¤í„° ë§¤ì¹­), ì¤‘ê°„ ê¸¸ì´ê°€ 435 residuesë¡œ AFDB(95)ë³´ë‹¤ í›¨ì”¬ ê¸¸ë‹¤. ì´ëŠ” ê¸´ ë‹¨ë°±ì§ˆ ëª¨ë¸ë§ì— ìœ ë¦¬í•˜ë‹¤.

AlphaFold2ì˜ IPA â†’ AlphaFold3ì˜ Transformer ì „í™˜ì—ì„œ **inductive biasê°€ ì‚¬ë¼ì¡Œê¸° ë•Œë¬¸ì—**, ë°ì´í„° ê·œëª¨ë¥¼ 147ë°°ë¡œ í‚¤ì›Œ ì´ë¥¼ ë³´ìƒí•œë‹¤ëŠ” ì „ëµì´ë‹¤.

<details markdown="1">
<summary>ğŸ“ Training Configuration (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Two-stage training
# Stage 1: Small crop size â€” fast iteration
config_stage1 = {
    "crop_size": 384,           # tokens
    "diffusion_batch_size": 64,
    "iterations": 60_000,
    "batch_size": 256,
    "optimizer": "AdamW",
    "lr": 0.0018,               # base model
    "warmup": 3000,             # extended for large models
    "msa_dropout": 0.10,
    "distillation_ratio": 0.50,
}

# Stage 2: Large crop size â€” handle longer sequences
config_stage2 = {
    "crop_size": 640,
    "diffusion_batch_size": 32,
    "iterations": 40_000,
}

# Precision: bfloat16 for MSA/Pairformer, float32 for Structure Module
# (bfloat16 in Structure Module â†’ lDDT drops significantly)

# For 512-width model:
# - Learning rate reduced to 0.001 (stability)
# - Extended warmup to 3000 steps
```

</details>

**Training Stability ì´ìŠˆ**: Pairformer widthê°€ 256ì„ ë„˜ìœ¼ë©´ gradient norm explosionê³¼ loss collapseê°€ ë°œìƒí•œë‹¤. Extended warmup (1000 â†’ 3000)ê³¼ reduced learning rate (0.0018 â†’ 0.001)ë¡œ í•´ê²°.

## Results

### FoldBench Main Results

| Model | Monomer lDDT | Prot-Prot DockQ | Ab-Ag DockQ | Prot-Lig SR% | Prot-RNA DockQ | Prot-DNA DockQ |
|---|---|---|---|---|---|---|
| AlphaFold 3 | 0.88_ | 72.93% | 47.90% | 64.90% | 62.32% | **79.18%** |
| Boltz-1 | 0.87_ | 68.25% | 33.54% | 55.04% | 56.90% | 70.97% |
| Chai-1 | 0.87_ | 68.53% | 23.64% | 51.23% | 50.91% | 69.97% |
| Protenix-0.5 | 0.8773 | 71.50% | 41.00% | 62.30% | 50.70% | 71.38% |
| **SeedFold** | **0.8889** | 74.03% | **53.21%** | 63.12% | **65.31%** | 72.60% |
| **SeedFold-Linear** | 0.8861 | **74.14%** | 46.91% | **66.48%** | 61.80% | 76.00% |

SeedFoldëŠ” AlphaFold3ë¥¼ **monomer, protein-protein, antibody-antigen, protein-RNA**ì—ì„œ ëŠ¥ê°€í•œë‹¤. íŠ¹íˆ antibody-antigenì—ì„œ 47.90% â†’ 53.21%ë¡œì˜ í° ë„ì•½ì´ ì£¼ëª©í•  ë§Œí•˜ë‹¤. SeedFold-LinearëŠ” protein-ligandì—ì„œ 66.48%ë¡œ ìµœê³  ì„±ëŠ¥ì„ ê¸°ë¡í•œë‹¤.

AlphaFold3ê°€ ì—¬ì „íˆ protein-DNA(79.18%)ì™€ RNA monomer(0.53)ì—ì„œ ìš°ìœ„ë¥¼ ë³´ì¸ë‹¤.

![Interface Results](/assets/img/posts/seedfold-scaling-biomolecular-structure-prediction/fig4_interface_results.png)
_Figure 4: Interface prediction ì„±ê³µë¥ ì˜ cumulative distribution. SeedFoldê°€ antibody-antigenê³¼ protein-ligandì—ì„œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì¼ê´€ë˜ê²Œ ëŠ¥ê°€. ì¶œì²˜: ì› ë…¼ë¬¸_

### Attention Mechanism Ablation

![Attention Ablation](/assets/img/posts/seedfold-scaling-biomolecular-structure-prediction/fig5_attention_ablation.png)
_Figure 5: Vanilla vs Additive Linear vs Gated Linear attentionì˜ validation ê³¡ì„ . Linear attentionì´ ëŒ€ë¶€ë¶„ì˜ taskì—ì„œ vanillaì™€ ë™ë“±í•˜ë©°, DNA/RNAì—ì„œ GatedLinearTriAttê°€ ë” ìš°ìˆ˜. ì¶œì²˜: ì› ë…¼ë¬¸_

Linear attentionì´ vanilla attentionê³¼ ë™ë“±í•œ ì„±ëŠ¥ì„ ë‚´ë©´ì„œ ë©”ëª¨ë¦¬/ì‹œê°„ì„ í¬ê²Œ ì ˆì•½í•œë‹¤ëŠ” ê²°ê³¼ë‹¤. DNA/RNA taskì—ì„œ GatedLinearTriAttê°€ íŠ¹íˆ ê°•í•œ ì´ìœ ëŠ”, sigmoid gatingì´ nucleic acidì˜ íŠ¹ìˆ˜í•œ pairwise interaction patternì„ ë” ì˜ í¬ì°©í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ ì¶”ì •ëœë‹¤.

### Distillationì˜ íš¨ê³¼

Monomer distillation ë°ì´í„°ë¥¼ í•™ìŠµ ì¤‘ê°„ì— ì œê±°í•˜ë©´ **intra-protein êµ¬ì¡° ì˜ˆì¸¡ ì •í™•ë„ê°€ ì¦‰ì‹œ í•˜ë½**í•œë‹¤. ì´ëŠ” distillation ë°ì´í„°ê°€ ë‹¨ìˆœíˆ ì´ˆê¸° í•™ìŠµì„ ë•ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, í•™ìŠµ ì „ì²´ ê³¼ì •ì—ì„œ ì§€ì†ì ìœ¼ë¡œ í•„ìš”í•¨ì„ ë³´ì—¬ì¤€ë‹¤ â€” "knowledge decay" ë°©ì§€ ì—­í• .

## Discussion

### ì €ìê°€ ë°íŒ í–¥í›„ ë°©í–¥

ë…¼ë¬¸ì€ ë‘ ê°€ì§€ ëª…í™•í•œ ë¯¸ë˜ ë°©í–¥ì„ ì œì‹œí•œë‹¤:

1. **Mixture of Experts (MoE)**: (i) cubic ë³µì¡ë„ ì•„í‚¤í…ì²˜ì—ì„œ ê³„ì‚° íš¨ìœ¨í™”, (ii) ë‹¤ì¤‘ task(nucleic acid, monomer, ligand) ê°„ì˜ gradient conflict í•´ê²°. ì„œë¡œ ë‹¤ë¥¸ attention mechanismì´ taskë³„ë¡œ ê°•ì ì´ ë‹¤ë¥´ë‹¤ëŠ” ê´€ì°°ì´ MoEì˜ ë™ê¸°ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ë’·ë°›ì¹¨í•œë‹¤.

2. **Post-training Scaling**: Diffusion ê¸°ë°˜ folding ëª¨ë¸ì˜ hallucination ë¬¸ì œë¥¼ ì§€ì í•˜ë©°, reinforcement learning from "X" feedback (RLxF)ê³¼ test-time compute (TTC)ë¥¼ í†µí•œ alignment ê°€ëŠ¥ì„±ì„ ì œì‹œí•œë‹¤.

### ì¬í˜„ì„±

- **ì½”ë“œ ê³µê°œ**: âŒ (2026ë…„ 2ì›” ê¸°ì¤€ ë¯¸ê³µê°œ, project page: [seedfold.github.io](https://seedfold.github.io/))
- **í•™ìŠµ ë°ì´í„°**: PDB (ê³µê°œ), AFDB (ê³µê°œ), Mgnify (ê³µê°œ) â€” ì ‘ê·¼ ê°€ëŠ¥í•˜ë‚˜ distillation pipeline ì¬êµ¬ì„± í•„ìš”
- **í•„ìš” GPU**: ë…¼ë¬¸ ë¯¸ëª…ì‹œ, 1B ëª¨ë¸ ê·œëª¨ + 100K iterationsìœ¼ë¡œ ì¶”ì • ì‹œ ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„° í•„ìš”
- **ì¬í˜„ ë‚œì´ë„**: â­â­â­â­â­ (ë§¤ìš° ë†’ìŒ â€” 26.5M distillation dataset êµ¬ì¶• ìì²´ê°€ í° ì‘ì—…)

> ì•ì„œ ë¦¬ë·°í•œ [SimpleFold](/posts/simplefold-folding-proteins-simpler/)ê°€ "ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆì„ ì œê±°í•´ë„ ë˜ëŠ”ê°€?"ë¼ëŠ” ì§ˆë¬¸ì´ì—ˆë‹¤ë©´, SeedFoldëŠ” "ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆì„ **ì œëŒ€ë¡œ ìŠ¤ì¼€ì¼ì—…**í•˜ë©´ ì–´ë””ê¹Œì§€ ê°€ëŠ”ê°€?"ë¼ëŠ” ë°˜ëŒ€í¸ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì´ë‹¤. ë‘ ì ‘ê·¼ ëª¨ë‘ AlphaFold3 ìˆ˜ì¤€ì„ ë„˜ì–´ì„œëŠ” ë° ì„±ê³µí–ˆì§€ë§Œ, ê·¸ ì „ëµì€ ì •ë°˜ëŒ€ë‹¤.
{: .prompt-info }

## TL;DR

- **Width scalingì´ depth scalingë³´ë‹¤ íš¨ê³¼ì **: Pairformerì˜ pair representation ì°¨ì›ì„ 128â†’512ë¡œ í‚¤ìš°ëŠ” ê²ƒì´ layerë¥¼ ë‘ ë°°ë¡œ ëŠ˜ë¦¬ëŠ” ê²ƒë³´ë‹¤ ì¼ê´€ë˜ê²Œ ìš°ìˆ˜.
- **Linear triangular attention**ìœ¼ë¡œ $O(n^3) \to O(n^2)$ ë³µì¡ë„ ê°ì†Œë¥¼ ë‹¬ì„±í•˜ë©´ì„œ ì„±ëŠ¥ì„ ìœ ì§€í•˜ê³ , DNA/RNA taskì—ì„œëŠ” ì˜¤íˆë ¤ vanillaë¥¼ ëŠ¥ê°€.
- FoldBenchì—ì„œ **AlphaFold3ë¥¼ monomer, protein-protein, antibody-antigen, protein-ligand, protein-RNA 5ê°œ taskì—ì„œ ëŠ¥ê°€**.

## Paper Info

| í•­ëª© | ë‚´ìš© |
|---|---|
| **Title** | SeedFold: Scaling Biomolecular Structure Prediction |
| **Authors** | Yi Zhou*, Chan Lu* et al. (ByteDance Seed) |
| **Venue** | arXiv preprint (Dec 2025) |
| **Paper** | [arXiv](https://arxiv.org/abs/2512.24354) |
| **Project** | [seedfold.github.io](https://seedfold.github.io/) |
| **Code** | ë¯¸ê³µê°œ |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
