---
title: "Highly Accurate Protein Structure Prediction with AlphaFold"
date: 2026-02-20 12:00:00 +0900
description: "AlphaFold 2ëŠ” ë‹¨ë°±ì§ˆ ì„œì—´ë§Œìœ¼ë¡œ ì›ì ìˆ˜ì¤€ ì •í™•ë„ì˜ 3D êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìµœì´ˆì˜ AI ì‹œìŠ¤í…œ. Evoformerì™€ Structure Moduleì„ ê²°í•©í•˜ì—¬ CASP14ì—ì„œ ì‹¤í—˜ êµ¬ì¡°ì— í•„ì í•˜ëŠ” ì •í™•ë„ ë‹¬ì„±."
categories: [Paper Review, Protein Structure]
tags: [protein-structure, AlphaFold, Evoformer, attention, end-to-end, CASP14, structure-prediction]
math: true
mermaid: true
image:
  path: /assets/img/posts/alphafold2-highly-accurate-protein-structure-prediction/fig1.png
  alt: "AlphaFold 2 architecture and predictions"
---

> ì´ ê¸€ì€ AlphaFold ì‹œë¦¬ì¦ˆì˜ ë‘ ë²ˆì§¸ ê¸€ì…ë‹ˆë‹¤. ì‹œë¦¬ì¦ˆ: AlphaFold 1, **AlphaFold 2 (ì´ ê¸€)**, AlphaFold 3, AlphaFold Summary.
{: .prompt-info }

## Hook

ë‹¨ë°±ì§ˆ êµ¬ì¡°ë¥¼ ì‹¤í—˜ìœ¼ë¡œ ê·œëª…í•˜ëŠ” ë°ëŠ” ìˆ˜ê°œì›”ì—ì„œ ìˆ˜ë…„ì´ ê±¸ë¦°ë‹¤. 100,000ê°œì˜ ë‹¨ë°±ì§ˆ êµ¬ì¡°ê°€ PDBì— ë“±ë¡ë˜ì–´ ìˆì§€ë§Œ, ì´ëŠ” ìˆ˜ì‹­ì–µ ê°œì˜ ì•Œë ¤ì§„ ë‹¨ë°±ì§ˆ ì„œì—´ì˜ ê·¹íˆ ì¼ë¶€ì— ë¶ˆê³¼í•˜ë‹¤. 50ë…„ ì´ìƒ í’€ë¦¬ì§€ ì•Šì€ **protein folding problem**â€”ì•„ë¯¸ë…¸ì‚° ì„œì—´ë§Œìœ¼ë¡œ ë‹¨ë°±ì§ˆì˜ 3D êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œâ€”ì´ 2020ë…„, AlphaFold 2ì˜ ë“±ì¥ìœ¼ë¡œ ëŒíŒŒêµ¬ë¥¼ ë§ì•˜ë‹¤.

AlphaFold 2ëŠ” CASP14ì—ì„œ ì‹¤í—˜ êµ¬ì¡°ì™€ ê²½ìŸí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì£¼ë©°, **ì²˜ìŒìœ¼ë¡œ ì›ì ìˆ˜ì¤€(atomic accuracy)ì˜ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡**ì„ ê°€ëŠ¥í•˜ê²Œ í•œ ì‹œìŠ¤í…œì´ë‹¤. íƒ„ì†Œ ì›ìì˜ í­ì´ 1.4Ã…ì¸ë°, AlphaFold 2ì˜ median backbone accuracyëŠ” 0.96Ã… r.m.s.d.95ì— ë‹¬í•œë‹¤.

## Problem

ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì€ í¬ê²Œ ë‘ ê°€ì§€ ì ‘ê·¼ë²•ìœ¼ë¡œ ë°œì „í•´ì™”ë‹¤. **ë¬¼ë¦¬ ê¸°ë°˜ ì ‘ê·¼ë²•(physical interaction programme)**ì€ ë¶„ì ê°„ ìƒí˜¸ì‘ìš©ì„ ì‹œë®¬ë ˆì´ì…˜í•˜ì—¬ êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ì§€ë§Œ, ì¤‘ê°„ í¬ê¸° ë‹¨ë°±ì§ˆì¡°ì°¨ ê³„ì‚°ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ì–´ë µê³  ì¶©ë¶„íˆ ì •í™•í•œ ë¬¼ë¦¬ ëª¨ë¸ì„ ë§Œë“¤ê¸° í˜ë“¤ë‹¤ëŠ” í•œê³„ê°€ ìˆì—ˆë‹¤. **ì§„í™” ê¸°ë°˜ ì ‘ê·¼ë²•(evolutionary programme)**ì€ ë‹¤ì¤‘ ì„œì—´ ì •ë ¬(MSA)ê³¼ ì§„í™”ì  ìƒê´€ê´€ê³„ë¥¼ í™œìš©í•˜ì§€ë§Œ, homologous structureê°€ ì—†ê±°ë‚˜ MSA depthê°€ ì–•ì„ ë•Œ ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì¡Œë‹¤.

ê¸°ì¡´ ë°©ë²•ë“¤ì€ ëŒ€ë¶€ë¶„ **distance matrixë¥¼ ì¤‘ê°„ ì˜ˆì¸¡ ë‹¨ê³„ë¡œ** ì‚¬ìš©í–ˆë‹¤. ì¦‰, MSAë¡œë¶€í„° pairwise distanceë¥¼ ì˜ˆì¸¡í•œ ë’¤, heuristic systemìœ¼ë¡œ 3D ì¢Œí‘œë¥¼ ì¬êµ¬ì„±í•˜ëŠ” 2ë‹¨ê³„ ê³¼ì •ì´ì—ˆë‹¤. ì´ ì ‘ê·¼ë²•ì€ ì •ë³´ ì†ì‹¤ê³¼ ìµœì í™” ì–´ë ¤ì›€ì„ ì´ˆë˜í–ˆë‹¤.

ë˜í•œ **homologueê°€ ì—†ëŠ” ê²½ìš°** ì •í™•ë„ê°€ ê¸‰ê²©íˆ ë–¨ì–´ì§€ëŠ” ë¬¸ì œê°€ ìˆì—ˆë‹¤. Template-based modelingì€ ìœ ì‚¬í•œ êµ¬ì¡°ê°€ PDBì— ìˆì„ ë•Œë§Œ ì˜ ì‘ë™í•˜ë©°, ìƒˆë¡œìš´ foldì— ëŒ€í•´ì„œëŠ” ë¬´ë ¥í–ˆë‹¤.

## Key Idea

AlphaFold 2ì˜ í•µì‹¬ì€ **ì§„í™”ì , ë¬¼ë¦¬ì , ê¸°í•˜í•™ì  ì œì•½ì„ neural network architectureì— ì§ì ‘ í†µí•©**í•˜ëŠ” ê²ƒì´ë‹¤. ì„¸ ê°€ì§€ í˜ì‹ ì´ ëŒíŒŒêµ¬ë¥¼ ì œê³µí–ˆë‹¤:

**1. Evoformer: MSAì™€ pair representationì˜ ê³µë™ ì„ë² ë”©**

ê¸°ì¡´ ë°©ë²•ë“¤ì´ MSAì—ì„œ featureë¥¼ ì¶”ì¶œí•œ ë’¤ ê³ ì •ëœ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œ ë°˜ë©´, AlphaFold 2ëŠ” MSA representationê³¼ pair representationì„ **ë§¤ layerì—ì„œ ìƒí˜¸ êµí™˜**í•˜ë©° ì§„í™”ì‹œí‚¨ë‹¤. Triangle multiplicative updateì™€ triangle self-attentionì„ í†µí•´ pairwise ê´€ê³„ì—ì„œ ì‚¼ê° ë¶€ë“±ì‹(triangle inequality) ê°™ì€ ê¸°í•˜í•™ì  ì œì•½ì„ ì•”ë¬µì ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

**2. End-to-end 3D ì¢Œí‘œ ì˜ˆì¸¡**

Distance matrixë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëŒ€ì‹ , AlphaFold 2ëŠ” **ê° residueì˜ rotationê³¼ translationì„ ì§ì ‘ ì˜ˆì¸¡**í•œë‹¤. ì´ë¥¼ "residue gas"ë¼ ë¶€ë¥´ë©°, ê° residueëŠ” ë…ë¦½ì ì¸ rigid bodyë¡œ í‘œí˜„ëœë‹¤. ì´ëŠ” peptide bond constraintë¥¼ ì„ì‹œë¡œ ìœ„ë°˜í•˜ë©° ì „ì—­ ìµœì í™” ë¬¸ì œë¥¼ ë³‘ë ¬í™” ê°€ëŠ¥í•œ local refinementë¡œ ë°”ê¾¼ë‹¤.

**3. Frame Aligned Point Error (FAPE) loss**

FAPEëŠ” ì˜ˆì¸¡ëœ ì›ì ì¢Œí‘œë¥¼ **ê° residueì˜ local frameì—ì„œ** í‰ê°€í•œë‹¤. ì „ì—­ ì •ë ¬(global alignment) ëŒ€ì‹  ìˆ˜ë§ì€ local alignmentì—ì„œ ì˜¤ì°¨ë¥¼ ì¸¡ì •í•¨ìœ¼ë¡œì¨, side chainì˜ orientationê³¼ chiralityë¥¼ ì •í™•í•˜ê²Œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

> AlphaFold 2ëŠ” ë¬¼ë¦¬ ë²•ì¹™ì„ ëª…ì‹œì ìœ¼ë¡œ ì½”ë”©í•˜ì§€ ì•Šê³ ë„, ìˆ˜ì†Œ ê²°í•©ì´ë‚˜ side chain packing ê°™ì€ ìƒí˜¸ì‘ìš©ì„ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµí•œë‹¤.
{: .prompt-tip }

## How It Works

### 4.1 Overview

AlphaFold 2ëŠ” í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ êµ¬ì„±ëœë‹¤: **Evoformer trunk**ì™€ **Structure Module**.

```mermaid
graph TD
    A[Primary Sequence + MSA + Templates] --> B["MSA Representation  /  Nseq Ã— Nres"]
    A --> C["Pair Representation  /  Nres Ã— Nres"]
    B --> D[Evoformer Block Ã—48]
    C --> D
    D --> E[Updated MSA]
    D --> F[Updated Pair]
    E --> G[Structure Module Ã—8]
    F --> G
    G --> H["Residue Gas  /  Nres rigid bodies"]
    H --> I[IPA + Backbone Update]
    I --> J[3D Coordinates]
    J --> K[Side-chain Angles]
    J --> L[Per-residue Confidence pLDDT]
    
```

ì „ì²´ ì•„í‚¤í…ì²˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

<details markdown="1">
<summary>ğŸ“ Overall Architecture Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class AlphaFold2(nn.Module):
    """AlphaFold 2 end-to-end architecture"""
    def __init__(self, c_m=256, c_z=128, n_evo_blocks=48, n_struct_blocks=8):
        super().__init__()
        self.evoformer = EvoformerStack(c_m, c_z, n_blocks=n_evo_blocks)
        self.structure_module = StructureModule(c_s=c_m, c_z=c_z, n_blocks=n_struct_blocks)
        self.side_chain_net = SideChainNet(c_s=c_m)
        self.plddt_head = PerResidueLDDTHead(c_s=c_m)
        
    def forward(self, sequence, msa, templates):
        # Step 1: Initialize representations
        msa_repr = self.init_msa_repr(sequence, msa)  # (Nseq, Nres, c_m)
        pair_repr = self.init_pair_repr(sequence, templates)  # (Nres, Nres, c_z)
        
        # Step 2: Evoformer trunk (48 blocks)
        for recycle in range(3):  # Recycling iterations
            msa_repr, pair_repr = self.evoformer(msa_repr, pair_repr)
        
        # Step 3: Structure module (8 blocks)
        single_repr = msa_repr[0]  # First row: query sequence
        frames = self.structure_module(single_repr, pair_repr)
        # frames: (Nres,) of SE(3) transformations (R, t)
        
        # Step 4: Side chains and confidence
        chi_angles = self.side_chain_net(single_repr, frames)
        plddt = self.plddt_head(single_repr)
        
        # Step 5: Convert to 3D coordinates
        coords = frames_to_atom_coords(frames, chi_angles)
        return coords, plddt
```

</details>

### 4.2 Representation

**MSA representation** ($N_{seq} \times N_{res} \times c_m$): ê° í–‰(row)ì€ homologous sequence, ê° ì—´(column)ì€ query sequenceì˜ residue positionì„ ë‚˜íƒ€ë‚¸ë‹¤. ì´ˆê¸°ê°’ì€ raw MSAë¥¼ one-hot encodingí•œ í›„ linear projectionìœ¼ë¡œ ì„ë² ë”©í•œë‹¤.

**Pair representation** ($N_{res} \times N_{res} \times c_z$): residue $i$ì™€ $j$ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì¸ì½”ë”©í•œë‹¤. ì´ˆê¸°ê°’ì€ relative position encodingê³¼ template structureì—ì„œ ì¶”ì¶œí•œ pairwise distance/angle featureë¡œ êµ¬ì„±ëœë‹¤.

**Single representation** ($N_{res} \times c_s$): MSA representationì˜ ì²« ë²ˆì§¸ rowë¡œ, query sequence ìì²´ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. Structure moduleì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

### 4.3 Core Architecture: Evoformer

Evoformer blockì€ AlphaFold 2ì˜ í•µì‹¬ìœ¼ë¡œ, MSAì™€ pair representationì„ **ìƒí˜¸ ì—…ë°ì´íŠ¸**í•˜ë©° êµ¬ì¡°ì  ê°€ì„¤ì„ ì ì§„ì ìœ¼ë¡œ ì •ì œí•œë‹¤.

![Evoformer block](/assets/img/posts/alphafold2-highly-accurate-protein-structure-prediction/fig3.png)
_Figure 3: Evoformer blockê³¼ Structure moduleì˜ ì„¸ë¶€ êµ¬ì¡°. ì¶œì²˜: ì› ë…¼ë¬¸_

<details markdown="1">
<summary>ğŸ“ Evoformer Block Implementation (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class EvoformerBlock(nn.Module):
    """Single Evoformer block with MSA and pair updates"""
    def __init__(self, c_m=256, c_z=128, n_heads=8):
        super().__init__()
        # MSA stack
        self.msa_row_attention = MSARowAttentionWithPairBias(c_m, c_z, n_heads)
        self.msa_column_attention = MSAColumnAttention(c_m, n_heads)
        self.msa_transition = Transition(c_m)
        
        # Communication: MSA â†’ Pair
        self.outer_product_mean = OuterProductMean(c_m, c_z)
        
        # Pair stack
        self.triangle_multiplication_outgoing = TriangleMultiplication(c_z, 'outgoing')
        self.triangle_multiplication_incoming = TriangleMultiplication(c_z, 'incoming')
        self.triangle_attention_starting = TriangleAttention(c_z, 'starting')
        self.triangle_attention_ending = TriangleAttention(c_z, 'ending')
        self.pair_transition = Transition(c_z)
        
    def forward(self, msa, pair):
        # MSA row-wise attention with pair bias
        # Pair representation influences MSA through attention bias
        msa = msa + self.msa_row_attention(msa, pair_bias=pair)
        msa = msa + self.msa_column_attention(msa)
        msa = msa + self.msa_transition(msa)
        
        # Outer product: MSA â†’ Pair
        pair = pair + self.outer_product_mean(msa)
        
        # Triangle updates for geometric consistency
        pair = pair + self.triangle_multiplication_outgoing(pair)
        pair = pair + self.triangle_multiplication_incoming(pair)
        pair = pair + self.triangle_attention_starting(pair)
        pair = pair + self.triangle_attention_ending(pair)
        pair = pair + self.pair_transition(pair)
        
        return msa, pair


class TriangleMultiplication(nn.Module):
    """Triangle multiplicative update for pair representation"""
    def __init__(self, c_z=128, mode='outgoing'):
        super().__init__()
        self.mode = mode
        self.layer_norm = nn.LayerNorm(c_z)
        self.linear_a = nn.Linear(c_z, c_z)
        self.linear_b = nn.Linear(c_z, c_z)
        self.linear_g = nn.Linear(c_z, c_z)
        self.linear_out = nn.Linear(c_z, c_z)
        
    def forward(self, z):
        # z: (Nres, Nres, c_z) pair representation
        # Update edge (i,j) using edges (i,k) and (k,j) for all k
        z = self.layer_norm(z)
        
        a = self.linear_a(z).sigmoid()  # (Nres, Nres, c_z)
        b = self.linear_b(z).sigmoid()  # (Nres, Nres, c_z)
        g = self.linear_g(z).sigmoid()  # gate
        
        if self.mode == 'outgoing':
            # z_ij update uses z_ik, z_kj
            ab = torch.einsum('ikc,kjc->ijc', a, b)
        else:  # incoming
            # z_ij update uses z_ki, z_kj
            ab = torch.einsum('kic,kjc->ijc', a, b)
        
        return self.linear_out(g * ab)
```

</details>

**Triangle multiplicative update**ëŠ” ì„¸ residue $i, j, k$ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ê³ ë ¤í•œë‹¤. Edge $(i,j)$ë¥¼ ì—…ë°ì´íŠ¸í•  ë•Œ, ëª¨ë“  ì¤‘ê°„ node $k$ì— ëŒ€í•´ $(i,k)$ì™€ $(k,j)$ edgeì˜ ì •ë³´ë¥¼ ê³±ì…ˆìœ¼ë¡œ ê²°í•©í•œë‹¤. ì´ëŠ” ê±°ë¦¬ì˜ ì‚¼ê° ë¶€ë“±ì‹ $d_{ij} \leq d_{ik} + d_{kj}$ë¥¼ ì•”ë¬µì ìœ¼ë¡œ ê°•ì œí•œë‹¤.

**Triangle self-attention**ì€ axial attentionì— "missing edge" ì •ë³´ë¥¼ biasë¡œ ì¶”ê°€í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, $(i,j)$ì— ëŒ€í•œ attentionì„ ê³„ì‚°í•  ë•Œ, $(i,k)$ì™€ $(k,j)$ì˜ ì •ë³´ë¥¼ logit biasë¡œ ì œê³µí•œë‹¤.

**MSA row attention with pair bias**: MSAì˜ ê° sequence(row)ì— ëŒ€í•´ self-attentionì„ ìˆ˜í–‰í•˜ë˜, pair representationìœ¼ë¡œë¶€í„° ì¶”ê°€ logit biasë¥¼ ë°›ëŠ”ë‹¤. ì´ëŠ” pair â†’ MSA ì •ë³´ íë¦„ì„ ë§Œë“ ë‹¤.

**Outer product mean**: MSA representationì„ pair representationìœ¼ë¡œ ë³€í™˜í•œë‹¤. ê° position $(i, j)$ì— ëŒ€í•´ MSAì˜ ëª¨ë“  sequenceì—ì„œ $(i, j)$ ìœ„ì¹˜ì˜ activationì„ outer productí•œ ë’¤ í‰ê· ì„ ë‚¸ë‹¤.

$$
z_{ij} \gets z_{ij} + \frac{1}{N_{seq}} \sum_s m_{si} \otimes m_{sj}
$$

ì—¬ê¸°ì„œ $m_{si}$ëŠ” MSA representationì˜ $s$ë²ˆì§¸ sequence, $i$ë²ˆì§¸ residueì˜ activationì´ë‹¤.

### 4.4 Key Innovation: Structure Module

Structure moduleì€ **residue gas** í‘œí˜„ì„ ì‚¬ìš©í•œë‹¤. ê° residueëŠ” ë…ë¦½ì ì¸ SE(3) transformation $(R_i, t_i)$ë¡œ í‘œí˜„ë˜ë©°, peptide bond constraintëŠ” ë¬´ì‹œëœë‹¤. ì´ëŠ” ì „ì—­ ë£¨í”„ í´ë¡œì €(loop closure) ë¬¸ì œë¥¼ í”¼í•˜ë©´ì„œ ëª¨ë“  residueë¥¼ ë³‘ë ¬ë¡œ ì •ì œí•  ìˆ˜ ìˆê²Œ í•œë‹¤.

<details markdown="1">
<summary>ğŸ“ Invariant Point Attention (IPA) Implementation (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class InvariantPointAttention(nn.Module):
    """IPA: geometry-aware attention in 3D space"""
    def __init__(self, c_s=384, c_z=128, n_heads=12, n_query_points=4, n_value_points=8):
        super().__init__()
        self.n_heads = n_heads
        self.n_query_points = n_query_points
        self.n_value_points = n_value_points
        
        # Standard attention projections
        self.linear_q = nn.Linear(c_s, n_heads * 16)
        self.linear_k = nn.Linear(c_s, n_heads * 16)
        self.linear_v = nn.Linear(c_s, n_heads * 16)
        
        # 3D point projections (in local frame)
        self.linear_q_points = nn.Linear(c_s, n_heads * n_query_points * 3)
        self.linear_k_points = nn.Linear(c_s, n_heads * n_query_points * 3)
        self.linear_v_points = nn.Linear(c_s, n_heads * n_value_points * 3)
        
        # Pair bias
        self.linear_b = nn.Linear(c_z, n_heads)
        
        self.head_weights = nn.Parameter(torch.zeros(n_heads))
        
    def forward(self, s, z, frames):
        # s: (Nres, c_s) single representation
        # z: (Nres, Nres, c_z) pair representation  
        # frames: (Nres,) list of (R, t) in SE(3)
        
        Nres = s.shape[0]
        
        # Standard attention
        q = self.linear_q(s).view(Nres, self.n_heads, 16)
        k = self.linear_k(s).view(Nres, self.n_heads, 16)
        v = self.linear_v(s).view(Nres, self.n_heads, 16)
        
        # 3D query/key points in local frames
        q_pts = self.linear_q_points(s).view(Nres, self.n_heads, self.n_query_points, 3)
        k_pts = self.linear_k_points(s).view(Nres, self.n_heads, self.n_query_points, 3)
        v_pts = self.linear_v_points(s).view(Nres, self.n_heads, self.n_value_points, 3)
        
        # Transform points to global frame
        q_pts_global = [frames[i].R @ q_pts[i] + frames[i].t for i in range(Nres)]
        k_pts_global = [frames[i].R @ k_pts[i] + frames[i].t for i in range(Nres)]
        
        # Compute attention logits
        attn_logits = torch.einsum('ihc,jhc->hij', q, k) / (16 ** 0.5)
        
        # Add 3D point contribution: squared distances
        for h in range(self.n_heads):
            for i in range(Nres):
                for j in range(Nres):
                    # Squared distance between query points i and key points j
                    dist_sq = torch.sum((q_pts_global[i][h] - k_pts_global[j][h])**2)
                    attn_logits[h, i, j] -= self.head_weights[h] * dist_sq
        
        # Add pair bias
        pair_bias = self.linear_b(z).permute(2, 0, 1)  # (n_heads, Nres, Nres)
        attn_logits = attn_logits + pair_bias
        
        # Softmax attention
        attn = F.softmax(attn_logits, dim=-1)  # (n_heads, Nres, Nres)
        
        # Apply attention to values (both scalar and 3D points)
        out_scalar = torch.einsum('hij,jhc->ihc', attn, v)
        
        # Aggregate value points, then transform back to local frames
        out_points = []
        for i in range(Nres):
            pts_i = torch.zeros(self.n_heads, self.n_value_points, 3)
            for j in range(Nres):
                v_pts_j_global = frames[j].R @ v_pts[j] + frames[j].t
                pts_i += attn[:, i, j].unsqueeze(-1).unsqueeze(-1) * v_pts_j_global
            # Transform back to local frame of residue i
            pts_i_local = frames[i].R.T @ (pts_i - frames[i].t)
            out_points.append(pts_i_local)
        
        return out_scalar, out_points
```

</details>

**Invariant Point Attention (IPA)**ì€ 3D ê³µê°„ì—ì„œ geometry-aware attentionì„ ìˆ˜í–‰í•œë‹¤. ê° residueëŠ” query/key/valueë¥¼ scalarì™€ **3D points**ë¡œ ìƒì„±í•œë‹¤. ì´ ì ë“¤ì€ local frameì—ì„œ ì •ì˜ë˜ë©°, global frameìœ¼ë¡œ ë³€í™˜ë˜ì–´ attention ê³„ì‚°ì— ì‚¬ìš©ëœë‹¤. ì  ê°„ì˜ **squared distance**ê°€ attention logitì— ê¸°ì—¬í•˜ì—¬, ê³µê°„ì ìœ¼ë¡œ ê°€ê¹Œìš´ residueì— ë†’ì€ attention weightë¥¼ ì¤€ë‹¤.

IPAì˜ í•µì‹¬ì€ **SE(3) invariance**ë‹¤. ì „ì²´ êµ¬ì¡°ë¥¼ íšŒì „/í‰í–‰ì´ë™í•´ë„ IPAì˜ ì¶œë ¥ì€ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ëŠ” global frameì„ í†µí•´ ê±°ë¦¬ë¥¼ ê³„ì‚°í•œ ë’¤, ë‹¤ì‹œ local frameìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë‹¬ì„±ëœë‹¤.

### 4.5 Training & Inference

**Frame Aligned Point Error (FAPE)**ëŠ” AlphaFold 2ì˜ ì£¼ìš” loss functionì´ë‹¤. ê° residue $k$ì˜ frame $(R_k, t_k)$ì— ëŒ€í•´ ì˜ˆì¸¡ ì›ì ìœ„ì¹˜ì™€ ì‹¤ì œ ì›ì ìœ„ì¹˜ë¥¼ ì •ë ¬í•œ ë’¤, ëª¨ë“  ì›ìì˜ ê±°ë¦¬ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•œë‹¤:

$$
\text{FAPE} = \frac{1}{N_{\text{frames}} \cdot N_{\text{atoms}}} \sum_{k} \sum_{i} \text{clamp}(\|R_k(x_i^{\text{pred}} - t_k) - R_k(x_i^{\text{true}} - t_k)\|)
$$

ì´ëŠ” ê° residueì˜ local frameì—ì„œ ì›ìë“¤ì´ ì •í™•í•˜ë„ë¡ ê°•ì œí•˜ë©°, side chainì˜ orientationê³¼ chiralityë¥¼ ë³´ì¡´í•œë‹¤.

<details markdown="1">
<summary>ğŸ“ Training Loop Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Training Loop
for epoch in range(num_epochs):
    for batch in dataloader:
        sequence, msa, templates, true_coords, true_frames = batch
        
        # Forward pass with recycling
        for recycle in range(3):
            if recycle > 0:
                # Feed previous prediction back as input
                msa, pair = prev_msa, prev_pair
            
            pred_coords, pred_frames, plddt = model(sequence, msa, templates)
            
            # Intermediate loss at each recycle
            fape_loss = compute_fape(pred_coords, true_coords, pred_frames, true_frames)
            distogram_loss = compute_distogram_loss(pair, true_coords)
            masked_msa_loss = compute_masked_msa_loss(msa, true_msa)
            
            loss = fape_loss + 0.3 * distogram_loss + 0.1 * masked_msa_loss
            
            prev_msa, prev_pair = msa.detach(), pair.detach()
        
        # Backprop
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


def compute_fape(pred_coords, true_coords, pred_frames, true_frames):
    """Frame Aligned Point Error"""
    # pred_coords, true_coords: (Nres, 3) for backbone atoms
    # pred_frames, true_frames: (Nres,) of (R, t)
    
    total_error = 0.0
    for k in range(len(pred_frames)):
        R_k, t_k = true_frames[k]
        
        # Align predicted and true coordinates to frame k
        pred_aligned = R_k @ (pred_coords - t_k)
        true_aligned = R_k @ (true_coords - t_k)
        
        # Clamped L1 distance
        dist = torch.norm(pred_aligned - true_aligned, dim=-1)
        clamped = torch.clamp(dist, max=10.0)
        total_error += clamped.sum()
    
    return total_error / (len(pred_frames) * len(pred_coords))
```

</details>

**Recycling**ì€ ì „ì²´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ ì‹¤í–‰í•˜ì—¬ ì˜ˆì¸¡ì„ ì •ì œí•œë‹¤. ì´ì „ iterationì˜ MSAì™€ pair representationì„ ë‹¤ìŒ iterationì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•œë‹¤. CASP14ì—ì„œëŠ” 3íšŒ recyclingì„ ì‚¬ìš©í–ˆë‹¤.

**Self-distillation**: AlphaFold 2ëŠ” PDB ë°ì´í„°ë¡œ í•™ìŠµí•œ ëª¨ë¸ë¡œ Uniclust30ì˜ 35ë§Œ ê°œ ì„œì—´ì— ëŒ€í•œ êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì´ë¥¼ ì¶”ê°€ í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ëŠ” noisy student self-distillationì„ ìˆ˜í–‰í–ˆë‹¤. ì´ëŠ” unlabeled sequence ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ì •í™•ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œì¼°ë‹¤.

**Masked MSA loss**: BERTì™€ ìœ ì‚¬í•˜ê²Œ, MSAì˜ ì¼ë¶€ residueë¥¼ ë§ˆìŠ¤í‚¹í•˜ê³  ë³µì›í•˜ëŠ” ë³´ì¡° lossë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ë„¤íŠ¸ì›Œí¬ê°€ ì§„í™”ì  ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ë„ë¡ ìœ ë„í•œë‹¤.

### 4.6 Iterative Refinement

AlphaFold 2ëŠ” êµ¬ì¡°ë¥¼ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•œë‹¤. ê° Evoformer block ë’¤ì— ì¤‘ê°„ structure moduleì„ ë°°ì¹˜í•˜ì—¬, ë„¤íŠ¸ì›Œí¬ê°€ ì–´ëŠ ì‹œì ì— ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¥¼ ì°¾ëŠ”ì§€ ê´€ì°°í•  ìˆ˜ ìˆë‹¤.

![Ablation and trajectory](/assets/img/posts/alphafold2-highly-accurate-protein-structure-prediction/fig4.png)
_Figure 4: (a) Ablation study â€” ê° êµ¬ì„± ìš”ì†Œì˜ ê¸°ì—¬ë„. (b) Structure trajectory â€” 48 Evoformer blockê³¼ 4íšŒ recycling ë™ì•ˆ GDT ë³€í™”. ì¶œì²˜: ì› ë…¼ë¬¸_

ì¼ë¶€ ë‹¨ë°±ì§ˆ(T1024)ì€ ì´ˆê¸° ëª‡ blockì—ì„œ ì´ë¯¸ ìµœì¢… êµ¬ì¡°ë¥¼ ì°¾ëŠ” ë°˜ë©´, ì–´ë ¤ìš´ ë‹¨ë°±ì§ˆ(T1064, SARS-CoV-2 ORF8)ì€ ìˆ˜ì‹­ ê°œ layerë¥¼ ê±°ì¹˜ë©° secondary structure elementë¥¼ ì¬ë°°ì—´í•œë‹¤.

### 4.7 Confidence Estimation

AlphaFold 2ëŠ” ë‘ ê°€ì§€ confidence scoreë¥¼ ì¶œë ¥í•œë‹¤:

- **pLDDT** (predicted lDDT-CÎ±): ê° residueì˜ ì˜ˆì¸¡ ì •í™•ë„. 0-100 ì‚¬ì´ ê°’ìœ¼ë¡œ, ì‹¤ì œ lDDT-CÎ±ì™€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¸ë‹¤ (Pearson's r=0.76).
- **pTM** (predicted TM-score): ì „ì²´ chainì˜ global accuracy. pairwise error predictionìœ¼ë¡œë¶€í„° ê³„ì‚°ëœë‹¤.

ì´ confidence scoreë“¤ì€ ì˜ˆì¸¡ êµ¬ì¡°ì˜ ì‹ ë¢°ì„±ì„ íŒë‹¨í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•˜ë‹¤.

## Results

AlphaFold 2ëŠ” CASP14ì—ì„œ ì••ë„ì ì¸ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. Median backbone accuracyëŠ” **0.96Ã… r.m.s.d.95** (95% CI = 0.85-1.16Ã…)ë¡œ, ì°¨ìˆœìœ„ ë°©ë²•ì˜ 2.8Ã… r.m.s.d.95ë¥¼ í¬ê²Œ ì•ì„°ë‹¤. All-atom accuracyëŠ” **1.5Ã… r.m.s.d.95**ë¡œ, íƒ„ì†Œ ì›ìì˜ í­(1.4Ã…)ê³¼ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì´ë‹¤.

![CASP14 performance](/assets/img/posts/alphafold2-highly-accurate-protein-structure-prediction/fig1.png)
_Figure 1: AlphaFold 2ì˜ CASP14 ì„±ëŠ¥. (a) ìƒìœ„ 15ê°œ íŒ€ê³¼ì˜ ë¹„êµ. (b-d) ì˜ˆì¸¡ êµ¬ì¡° ì˜ˆì‹œ â€” T1049(ë‹¨ì¼ ë„ë©”ì¸), T1056(ì•„ì—° ê²°í•© ë¶€ìœ„), T1044(2,180 residue ë‹¨ì¼ ì²´ì¸). ì¶œì²˜: ì› ë…¼ë¬¸_

| Metric | AlphaFold 2 | 2nd Best | Improvement |
|---|---|---|---|
| Backbone r.m.s.d.95 | 0.96 Ã… | 2.8 Ã… | **2.9Ã—** |
| All-atom r.m.s.d.95 | 1.5 Ã… | 3.5 Ã… | **2.3Ã—** |
| Median GDT (CASP14) | ~92 | ~75 | +17 points |

**Table 1:** CASP14 ê²°ê³¼ ìš”ì•½. AlphaFold 2ëŠ” ëª¨ë“  ì§€í‘œì—ì„œ 2ìœ„ë¥¼ ì••ë„ì ìœ¼ë¡œ ì•ì„ ë‹¤.

CASP ì´í›„ ê³µê°œëœ PDB êµ¬ì¡°ì—ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í–ˆë‹¤. Template coverageê°€ 30% ë¯¸ë§Œì¸ ì–´ë ¤ìš´ ë‹¨ë°±ì§ˆì—ì„œë„ median lDDT-CÎ±ëŠ” **70 ì´ìƒ**ì„ ê¸°ë¡í–ˆë‹¤.

![Recent PDB accuracy](/assets/img/posts/alphafold2-highly-accurate-protein-structure-prediction/fig2.png)
_Figure 2: ìµœê·¼ PDB êµ¬ì¡°ì— ëŒ€í•œ ì •í™•ë„. (a) Backbone r.m.s.d. ë¶„í¬. (b) Backbone vs side-chain accuracy ìƒê´€ê´€ê³„. (c) pLDDT vs lDDT-CÎ± ì„ í˜• ê´€ê³„. (d) pTM vs TM-score ìƒê´€ê´€ê³„. ì¶œì²˜: ì› ë…¼ë¬¸_

**Side-chain accuracy**: Backboneì´ ì •í™•í•  ë•Œ(lDDT-CÎ± > 90), **80% ì´ìƒì˜ rotamerê°€ ì •í™•**í•˜ê²Œ ì˜ˆì¸¡ë˜ì—ˆë‹¤ (torsion angle 40Â° ì´ë‚´). ì´ëŠ” AlphaFold 2ê°€ backboneë¿ ì•„ë‹ˆë¼ side chain packingë„ ë§¤ìš° ì˜ ì˜ˆì¸¡í•¨ì„ ë³´ì—¬ì¤€ë‹¤.

**Intertwined homomers**: AlphaFold 2ëŠ” ì…ë ¥ stoichiometry ì—†ì´ë„ intertwined homotrimer (PDB 6SK0) ê°™ì€ ë³µì¡í•œ êµ¬ì¡°ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í–ˆë‹¤.

> AlphaFold 2ëŠ” ìˆ˜ì†Œ ê²°í•© ì ìˆ˜ í•¨ìˆ˜ë‚˜ ëª…ì‹œì  ë¬¼ë¦¬ ë²•ì¹™ ì—†ì´ë„, ë°ì´í„°ë¡œë¶€í„° ìˆ˜ì†Œ ê²°í•©ê³¼ side chain packingì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤.
{: .prompt-tip }

## Discussion

AlphaFold 2ëŠ” **bioinformaticsì™€ physics ì ‘ê·¼ë²•ì˜ í†µí•©**ì„ ë³´ì—¬ì¤€ë‹¤. ë¬¼ë¦¬ì Â·ê¸°í•˜í•™ì  inductive biasë¥¼ neural networkì— ë‚´ì¥í•˜ë˜, handcrafted featureëŠ” ìµœì†Œí™”í•˜ì—¬ PDB ë°ì´í„°ë¡œë¶€í„° íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

**í•œê³„ì **:
1. **MSA depth ì˜ì¡´ì„±**: MSA depthê°€ 30 ë¯¸ë§Œì¼ ë•Œ ì •í™•ë„ê°€ ê¸‰ê²©íˆ ë–¨ì–´ì§„ë‹¤. Shallow MSAë¥¼ ê°€ì§„ orphan proteinì´ë‚˜ ì‹ ê·œ ë‹¨ë°±ì§ˆ familyì—ì„œëŠ” ì„±ëŠ¥ì´ ì œí•œëœë‹¤.
2. **Cross-chain contacts**: Hetero-complexì—ì„œ ëŒ€ë¶€ë¶„ì˜ contactê°€ ë‹¤ë¥¸ chainê³¼ ì´ë£¨ì–´ì§„ ê²½ìš°(bridging domain ë“±) ì •í™•ë„ê°€ ë‚®ë‹¤. Homotypic contactê°€ ë§ì€ homodimer/trimerëŠ” ì˜ ì˜ˆì¸¡í•˜ì§€ë§Œ, heteromerëŠ” ì•„ì§ ì–´ë µë‹¤.
3. **Missing cofactors/ligands**: êµ¬ì¡°ê°€ haem groupì´ë‚˜ íŠ¹ì • ì´ì˜¨ì— ì˜ì¡´í•˜ëŠ” ê²½ìš°, AlphaFold 2ëŠ” ì´ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì¼ë¶€ ì˜¤ì°¨ê°€ ë°œìƒí•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ, ë…¼ë¬¸ì—ì„œëŠ” haem ì—†ì´ë„ ì˜¬ë°”ë¥¸ êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•œ ì‚¬ë¡€ë¥¼ ë³´ê³ í–ˆë‹¤.

![MSA depth and cross-chain contacts](/assets/img/posts/alphafold2-highly-accurate-protein-structure-prediction/fig5.png)
_Figure 5: (a) MSA depthê°€ ì •í™•ë„ì— ë¯¸ì¹˜ëŠ” ì˜í–¥. (b) Intertwined homotrimer (PDB 6SK0) ì˜ˆì¸¡ ì„±ê³µ ì‚¬ë¡€. ì¶œì²˜: ì› ë…¼ë¬¸_

**ì €ìë“¤ì´ ë°íŒ í–¥í›„ ë°©í–¥**:
- **Hetero-complex ì˜ˆì¸¡**: AlphaFoldì˜ ì•„ì´ë””ì–´ë¥¼ í™•ì¥í•˜ì—¬ multi-chain complex ì˜ˆì¸¡ ì‹œìŠ¤í…œ ê°œë°œ
- **Proteome-scale prediction**: GPU ë¶„ ë‹¨ìœ„ì˜ ë¹ ë¥¸ ì˜ˆì¸¡ ì†ë„ë¥¼ í™œìš©í•œ ì „ì²´ proteome êµ¬ì¡° ì˜ˆì¸¡ (companion paperì—ì„œ human proteome ì˜ˆì¸¡ ìˆ˜í–‰)
- **Molecular replacementì™€ cryo-EM map í•´ì„**: ì‹¤í—˜ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ì´ë¯¸ AlphaFoldë¥¼ í™œìš© ì¤‘

**ê³„ì‚° íš¨ìœ¨ì„±**: AlphaFold 2ëŠ” V100 GPUì—ì„œ 384 residue ë‹¨ë°±ì§ˆì„ **ì•½ 1ë¶„**ì— ì˜ˆì¸¡í•œë‹¤ (ensembling ì—†ì´). 2,500 residue ë‹¨ë°±ì§ˆë„ ì•½ 2ì‹œê°„ì´ë©´ ì¶©ë¶„í•˜ë‹¤. ì´ëŠ” ê¸°ì¡´ template-based methodë³´ë‹¤ í›¨ì”¬ ë¹ ë¥´ë©°, proteome-scale ì˜ˆì¸¡ì„ í˜„ì‹¤í™”í•œë‹¤.

## Limitations

1. **MSA ì˜ì¡´ì„± ì§€ì†**: ê¹Šì€ MSAê°€ í™•ë³´ë˜ì§€ ì•ŠëŠ” ë‹¨ë°±ì§ˆ(ì˜ˆ: de novo designed proteins, orphan sequences)ì—ì„œëŠ” ì •í™•ë„ê°€ í¬ê²Œ ë–¨ì–´ì§„ë‹¤.
2. **ë‹¨ë°±ì§ˆë§Œ ì˜ˆì¸¡**: Ligand, nucleic acid, cofactor ë“± non-protein ë¶„ìì™€ì˜ ìƒí˜¸ì‘ìš©ì„ ì˜ˆì¸¡í•˜ì§€ ëª»í•œë‹¤.
3. **Static structureë§Œ ì˜ˆì¸¡**: ë‹¨ë°±ì§ˆì˜ conformational ensembleì´ë‚˜ dynamicsë¥¼ í¬ì°©í•˜ì§€ ëª»í•˜ë©°, ë‹¨ì¼ êµ¬ì¡°ë§Œ ì¶œë ¥í•œë‹¤.
4. **pLDDTì˜ ë¶ˆì™„ì „í•œ ì‹ ë¢°ë„**: pLDDTê°€ ë†’ì•„ë„ ì‹¤ì œë¡œ í‹€ë¦° ê²½ìš°ê°€ ìˆìœ¼ë©°, íŠ¹íˆ intrinsically disordered regionì—ì„œ ê³¼ì‹ í•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤.
5. **í›ˆë ¨ ë°ì´í„° í¸í–¥**: PDBì˜ crystallizable protein í¸í–¥ì´ ëª¨ë¸ì— ë°˜ì˜ë˜ì–´, membrane proteinì´ë‚˜ ëŒ€í˜• complexì—ì„œ ì„±ëŠ¥ì´ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ë‹¤.

## Conclusion

AlphaFold 2ëŠ” ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ ë¬¸ì œë¥¼ ì‚¬ì‹¤ìƒ í•´ê²°í•œ ê²ƒìœ¼ë¡œ í‰ê°€ë°›ëŠ”ë‹¤. Evoformerì˜ MSA-pair representation ìƒí˜¸ì‘ìš©, Structure Moduleì˜ SE(3)-equivariant coordinate generation, ê·¸ë¦¬ê³  iterative recyclingì˜ ì¡°í•©ìœ¼ë¡œ CASP14ì—ì„œ GDT > 90ì˜ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆë‹¤. End-to-end í•™ìŠµìœ¼ë¡œ feature engineeringì˜ í•„ìš”ì„±ì„ ì œê±°í•˜ê³ , FAPE lossë¡œ ë¬¼ë¦¬ì ìœ¼ë¡œ ì˜ë¯¸ ìˆëŠ” êµ¬ì¡°ë¥¼ ì§ì ‘ í•™ìŠµí•œ ê²ƒì´ í•µì‹¬ì´ë‹¤. 200M+ êµ¬ì¡°ì˜ AlphaFold Protein Structure Database ê³µê°œëŠ” êµ¬ì¡° ìƒë¬¼í•™ ì—°êµ¬ì˜ landscapeë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ë³€í™”ì‹œì¼°ë‹¤.

## TL;DR

- AlphaFold 2ëŠ” ë‹¨ë°±ì§ˆ ì„œì—´ë§Œìœ¼ë¡œ **ì›ì ìˆ˜ì¤€ ì •í™•ë„(~1Ã…)**ì˜ 3D êµ¬ì¡°ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìµœì´ˆì˜ AI ì‹œìŠ¤í…œ
- **Evoformer**: MSAì™€ pair representationì„ ê³µë™ìœ¼ë¡œ ì§„í™”ì‹œí‚¤ë©°, triangle updateë¡œ ê¸°í•˜í•™ì  ì œì•½ í•™ìŠµ
- **Structure Module**: Residue gas í‘œí˜„ê³¼ Invariant Point Attention(IPA)ìœ¼ë¡œ 3D ì¢Œí‘œë¥¼ ì§ì ‘ ì˜ˆì¸¡
- **FAPE loss**: Local frameì—ì„œ ì›ì ìœ„ì¹˜ë¥¼ í‰ê°€í•˜ì—¬ side chain orientationê³¼ chirality ë³´ì¡´
- CASP14ì—ì„œ median 0.96Ã… r.m.s.d.95 ë‹¬ì„±, 2ìœ„ ëŒ€ë¹„ **3ë°° ì •í™•**
- MSA depth < 30ì—ì„œ ì„±ëŠ¥ ì €í•˜, hetero-complex ì˜ˆì¸¡ì€ í–¥í›„ ê³¼ì œ

## Paper Info

| í•­ëª© | ë‚´ìš© |
|---|---|
| **Title** | Highly accurate protein structure prediction with AlphaFold |
| **Authors** | John Jumper, Richard Evans, Alexander Pritzel et al. (DeepMind) |
| **Venue** | Nature, Vol. 596, August 2021 |
| **Submitted** | 2021-07-15 (published 2021-08-02) |
| **Published** | Nature, Vol. 596, August 2021 |
| **Link** | [doi:10.1038/s41586-021-03819-2](https://doi.org/10.1038/s41586-021-03819-2) |
| **Paper** | [Nature](https://www.nature.com/articles/s41586-021-03819-2) |
| **Code** | [GitHub - AlphaFold](https://github.com/deepmind/alphafold) |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
