---
title: "SimpleFold: Folding Proteins is Simpler than You Think"
date: 2026-02-24 12:00:00 +0900
description: "Appleì˜ SimpleFold â€” MSA, pair representation, triangle attention ì—†ì´ í‘œì¤€ Transformer + Flow Matchingë§Œìœ¼ë¡œ protein foldingì„ ë‹¬ì„±í•œ ëª¨ë¸."
categories: [Paper Review, Protein Structure]
tags: [protein-folding, flow-matching, transformer, simplefold, apple, generative-model, esm2]
math: true
mermaid: true
image:
  path: /assets/img/posts/simplefold-folding-proteins-simpler/fig2_architecture.png
  alt: "SimpleFold ì•„í‚¤í…ì²˜ ê°œìš”"
---

ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì—ì„œ AlphaFold2ê°€ ì œì‹œí•œ triangle attention, pair representation, MSA â€” ì´ ëª¨ë“  ê²ƒì´ ì •ë§ "í•„ìˆ˜"ì¼ê¹Œ? Appleì—ì„œ ë‚˜ì˜¨ SimpleFoldëŠ” ì´ ì§ˆë¬¸ì— ì •ë©´ìœ¼ë¡œ ë„ì „í•œë‹¤. ì¼ë°˜ ëª©ì  [Transformer](/posts/attention-is-all-you-need/) + Flow Matchingë§Œìœ¼ë¡œ, ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆì„ ì „ë¶€ ì œê±°í•˜ê³ ë„ competitiveí•œ folding ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤. ì‹¬ì§€ì–´ ensemble predictionì—ì„œëŠ” ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ê¸°ë„ í•œë‹¤.

ì´ ë…¼ë¬¸ì´ í¥ë¯¸ë¡œìš´ ì´ìœ ëŠ” ë‹¨ìˆœí•˜ë‹¤: protein foldingì„ "text-to-image" ìƒì„± ë¬¸ì œì™€ ë™ì¼í•œ í”„ë ˆì„ì›Œí¬ë¡œ í’€ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. ì•„ë¯¸ë…¸ì‚° ì„œì—´ì´ "í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸"ì´ê³ , 3D ì›ì ì¢Œí‘œê°€ "ìƒì„±ëœ ì´ë¯¸ì§€"ì¸ ì…ˆì´ë‹¤.

## Problem: ì™œ SimpleFoldê°€ í•„ìš”í•œê°€

[AlphaFold2](/posts/alphafold2-highly-accurate-protein-structure-prediction/) ì´í›„ protein folding ëª¨ë¸ë“¤ì€ ê³µí†µì ìœ¼ë¡œ ë³µì¡í•œ ë„ë©”ì¸ íŠ¹í™” ì•„í‚¤í…ì²˜ì— ì˜ì¡´í•´ì™”ë‹¤:

- **MSA (Multiple Sequence Alignment)**: ì§„í™”ì  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ë¹„ìš©ì´ í° ê²€ìƒ‰ ê³¼ì •
- **Pair representation**: ì”ê¸° ìŒ ê°„ì˜ ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” $O(N^2)$ í–‰ë ¬
- **Triangle attention/updates**: pair representationì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³ ë¹„ìš© ì—°ì‚°

ì´ ì„¤ê³„ë“¤ì€ "ë‹¨ë°±ì§ˆ êµ¬ì¡° ìƒì„± ê³¼ì •ì— ëŒ€í•œ í˜„ì¬ì˜ ì´í•´"ë¥¼ ëª¨ë¸ì— í•˜ë“œì½”ë”©í•œ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ vision ë¶„ì•¼ì—ì„œ ë²”ìš© Transformerê°€ ë„ë©”ì¸ íŠ¹í™” ì•„í‚¤í…ì²˜ë¥¼ ëŒ€ì²´í•œ ê²ƒì²˜ëŸ¼, protein foldingì—ì„œë„ ê°™ì€ ì¼ì´ ê°€ëŠ¥í•˜ì§€ ì•Šì„ê¹Œ?

ë˜ ë‹¤ë¥¸ ë¬¸ì œëŠ” ensemble generationì´ë‹¤. ê¸°ì¡´ folding ëª¨ë¸ ëŒ€ë¶€ë¶„ì€ deterministic regression objectiveë¡œ í•™ìŠµë˜ì–´, í•˜ë‚˜ì˜ êµ¬ì¡°ë§Œ ì˜ˆì¸¡í•œë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ë‹¨ë°±ì§ˆì€ ì—¬ëŸ¬ conformationì˜ ì•™ìƒë¸”ë¡œ ì¡´ì¬í•˜ë©°, ì´ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì´ drug discoveryì—ì„œ cryptic pocket ë°œê²¬ ë“±ì— í•µì‹¬ì ì´ë‹¤.

SimpleFoldëŠ” ì´ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•œë‹¤: ì•„í‚¤í…ì²˜ ë‹¨ìˆœí™” + ìƒì„±ì  ì ‘ê·¼ì„ í†µí•œ ensemble modeling.

## Key Idea: Text-to-3Dë¡œì„œì˜ Protein Folding

SimpleFoldì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ ì§ê´€ì ì´ë‹¤:

- ì•„ë¯¸ë…¸ì‚° ì„œì—´ â†’ 3D êµ¬ì¡° ë§¤í•‘ì„ **conditional generative model**ë¡œ ìºìŠ¤íŒ…
- **Flow Matching** objectiveë¡œ noiseì—ì„œ all-atom êµ¬ì¡°ë¥¼ ìƒì„±
- ì•„í‚¤í…ì²˜ëŠ” **DiT (Diffusion Transformer)** ìŠ¤íƒ€ì¼ì˜ adaptive layerê°€ ìˆëŠ” í‘œì¤€ Transformer ë¸”ë¡ë§Œ ì‚¬ìš©
- **Pair representation, triangle update, MSA ëª¨ë‘ ì œê±°**

ê¸°ì¡´ flow matching ê¸°ë°˜ protein ëª¨ë¸ë“¤(AlphaFlow, ESMFlow, Proteina)ì´ ì—¬ì „íˆ AlphaFold2ì˜ ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆì„ ìœ ì§€í–ˆë˜ ê²ƒê³¼ ëŒ€ë¹„ëœë‹¤. SimpleFoldëŠ” ì²˜ìŒë¶€í„° ìˆœìˆ˜ ìƒì„± ëª¨ë¸ë¡œ from scratch í•™ìŠµí•œ ìµœì´ˆì˜ flow matching protein folding ëª¨ë¸ì´ë‹¤.

## How it works

### Overview

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ í¬ê²Œ ì„¸ ë‹¨ê³„ë‹¤: (1) noisy ì›ì ì¢Œí‘œë¥¼ atom-levelë¡œ ì¸ì½”ë”©, (2) residue-level trunkì—ì„œ heavy ì—°ì‚° ìˆ˜í–‰, (3) ë‹¤ì‹œ atom-levelë¡œ ë””ì½”ë”©í•˜ì—¬ velocity field ì˜ˆì¸¡.

![Figure 2: SimpleFold ì•„í‚¤í…ì²˜](/assets/img/posts/simplefold-folding-proteins-simpler/fig2_architecture.png)
_Figure 2: SimpleFold ì•„í‚¤í…ì²˜. Atom Encoder â†’ Grouping â†’ Residue Trunk â†’ Ungrouping â†’ Atom Decoder. ëª¨ë“  ëª¨ë“ˆì´ ë™ì¼í•œ adaptive layer Transformer ë¸”ë¡ì„ ê³µìœ í•œë‹¤. ì¶œì²˜: ì› ë…¼ë¬¸_

```mermaid
graph TD
    A["ğŸ§¬ AA Sequence s"] --> PLM["ESM2-3B<br/>(Frozen PLM)"]
    PLM --> E["Sequence Embeddings<br/>e âˆˆ â„<sup>N<sub>r</sub> Ã— d<sub>e</sub></sup>"]

    N["ğŸ² Noisy Coords x<sub>t</sub><br/>+ Atomic Features"] --> AE["Atom Encoder<br/>(Local Attention)"]
    AE --> AT["Atom Tokens<br/>a âˆˆ â„<sup>N<sub>a</sub> Ã— d<sub>a</sub></sup>"]
    AT --> G["Grouping<br/>(Avg Pool per Residue)"]
    G --> RT["Residue Tokens<br/>r âˆˆ â„<sup>N<sub>r</sub> Ã— d<sub>a</sub></sup>"]

    RT --> CAT["Concat"]
    E --> CAT
    CAT --> TRUNK["Residue Trunk<br/>(Heavy Transformer)"]

    T["â± Timestep t"] --> TRUNK

    TRUNK --> UG["Ungrouping<br/>(Broadcast to Atoms)"]
    UG --> SKIP["+ Skip Connection"]
    AT --> SKIP
    SKIP --> AD["Atom Decoder<br/>(Local Attention)"]
    AD --> V["Predicted Velocity vÌ‚<sub>t</sub>"]

    style A fill:#e1f5fe
    style V fill:#e8f5e9
    style TRUNK fill:#fff3e0
```

<details markdown="1">
<summary>ğŸ“ Overall Architecture Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class SimpleFold(nn.Module):
    """SimpleFold: Flow Matching Protein Folding Model"""
    
    def __init__(self, config):
        super().__init__()
        # Frozen PLM for sequence conditioning
        self.plm = ESM2_3B(frozen=True)  # â†’ (Nr, de=2560)
        
        # Atom Encoder: local attention Transformer
        self.atom_encoder = TransformerStack(
            dim=config.atom_dim,       # e.g., 640 for 3B
            heads=config.atom_heads,   # e.g., 10
            blocks=config.atom_blocks, # e.g., 4
            local_attention=True,      # attend only within nearby residues
            rope_mode="4d_axial",      # 3D coords + 1D residue index
        )
        
        # Residue Trunk: heavy global attention Transformer
        self.residue_trunk = TransformerStack(
            dim=config.trunk_dim,       # e.g., 2048 for 3B
            heads=config.trunk_heads,   # e.g., 32
            blocks=config.trunk_blocks, # e.g., 36
            local_attention=False,      # full global attention
            rope_mode="1d",             # residue sequence position
        )
        
        # Atom Decoder: symmetric to encoder
        self.atom_decoder = TransformerStack(
            dim=config.atom_dim,
            heads=config.atom_heads,
            blocks=config.atom_blocks,
            local_attention=True,
            rope_mode="4d_axial",
        )
        
        # Projections
        self.seq_proj = nn.Linear(config.atom_dim + config.de, config.trunk_dim)
        self.ungroup_proj = nn.Linear(config.trunk_dim, config.atom_dim)
        self.output_proj = nn.Linear(config.atom_dim, 3)  # velocity in R^3
    
    def forward(self, x_t: Tensor, seq: Tensor, t: Tensor) -> Tensor:
        """
        x_t: (B, Na, 3) noisy atom coords at timestep t
        seq: (B, Nr) amino acid sequence
        t: (B,) timestep in [0, 1]
        Returns: (B, Na, 3) predicted velocity field vÌ‚_t
        """
        # Step 1: Encode sequence with frozen PLM
        e = self.plm(seq)                               # (B, Nr, de)
        
        # Step 2: Atom Encoder (fine level)
        atom_features = build_atom_features(x_t)        # concat coords + type + charge
        a = self.atom_encoder(atom_features, t)          # (B, Na, da)
        a_skip = a                                       # save for skip connection
        
        # Step 3: Grouping â€” avg pool atoms per residue
        r = grouping(a, residue_map)                     # (B, Nr, da)
        
        # Step 4: Concat sequence embeddings + project
        r = self.seq_proj(torch.cat([r, e], dim=-1))     # (B, Nr, d_trunk)
        
        # Step 5: Residue Trunk (coarse level) â€” heavy computation
        r = self.residue_trunk(r, t)                     # (B, Nr, d_trunk)
        
        # Step 6: Ungrouping â€” broadcast residue tokens to atoms
        a = self.ungroup_proj(ungrouping(r, residue_map)) # (B, Na, da)
        a = a + a_skip                                    # skip connection
        
        # Step 7: Atom Decoder (fine level)
        a = self.atom_decoder(a, t)                      # (B, Na, da)
        
        # Step 8: Output velocity
        v_t = self.output_proj(a)                        # (B, Na, 3)
        return v_t
```

</details>


### Representation: Hierarchical "Fine â†’ Coarse â†’ Fine"

SimpleFoldëŠ” ë‹¨ë°±ì§ˆì˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ "fine â†’ coarse â†’ fine" ìŠ¤í‚´ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤:

- **Fine (Atom level)**: Atom Encoderì—ì„œ ê° ì›ìì˜ noisy ì¢Œí‘œ + ì›ì íŠ¹ì„±(type, charge)ì„ ì²˜ë¦¬. **Local attention mask**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ì”ê¸° ë° ì¸ì ‘ ì”ê¸°ì˜ ì›ìë“¤ë¼ë¦¬ë§Œ attend.
- **Coarse (Residue level)**: Grouping ì—°ì‚°ìœ¼ë¡œ í•œ ì”ê¸°ì— ì†í•œ atom tokenë“¤ì„ í‰ê·  í’€ë§í•˜ì—¬ residue tokenì„ ìƒì„±. ì—¬ê¸°ì„œ ESM2-3Bì˜ sequence embeddingê³¼ concatë˜ì–´ Residue Trunkì— ì…ë ¥.
- **Fine (Atom level)**: **Ungrouping**ìœ¼ë¡œ residue tokenì„ ë‹¤ì‹œ atom ìˆ˜ë§Œí¼ broadcast. Atom Encoderì˜ ì¶œë ¥ê³¼ skip connectionì„ ë”í•œ í›„ Atom Decoderì—ì„œ ìµœì¢… velocity field ì˜ˆì¸¡.

![Figure 5: Grouping/Ungrouping](/assets/img/posts/simplefold-folding-proteins-simpler/fig5_grouping.png)
_Figure 5: Groupingê³¼ Ungrouping ì—°ì‚°. Groupingì€ ê°™ì€ ì”ê¸°ì˜ atom tokenì„ í‰ê·  í’€ë§, Ungroupingì€ residue tokenì„ atom ìˆ˜ë§Œí¼ ë³µì œ. ì¶œì²˜: ì› ë…¼ë¬¸_

### Core Architecture: Adaptive Layer Transformer

SimpleFoldì˜ ëª¨ë“  ëª¨ë“ˆ(Atom Encoder, Residue Trunk, Atom Decoder)ì€ **ë™ì¼í•œ ë¹Œë”© ë¸”ë¡**ì„ ì‚¬ìš©í•œë‹¤: adaptive layer normalizationì´ ìˆëŠ” í‘œì¤€ Transformer ë¸”ë¡.

![Figure 4: AF2 vs SimpleFold](/assets/img/posts/simplefold-folding-proteins-simpler/fig4_af2_comparison.png)
_Figure 4: (a) AlphaFold2ì˜ Evoformer â€” pair representationê³¼ triangle attentionì´ í¬í•¨ëœ ë³µì¡í•œ êµ¬ì¡°. (b) SimpleFoldì˜ Transformer ë¸”ë¡ â€” adaptive layerë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ìˆœí•œ êµ¬ì¡°. ì¶œì²˜: ì› ë…¼ë¬¸_

ê° Transformer ë¸”ë¡ì˜ êµ¬ì„±:

1. **Adaptive LayerNorm (adaLN)**: timestep $t$ë¥¼ conditioningìœ¼ë¡œ ë°›ì•„ scale/shift íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±
2. **Multi-Head Attention**: QK-normalization ì ìš©
3. **SwiGLU FFN**: í‘œì¤€ FFN ëŒ€ì‹  SwiGLU ì‚¬ìš© (í•™ìŠµ ì•ˆì •ì„± + ì„±ëŠ¥)
4. **RoPE**: Residue Trunkì—ì„œëŠ” 1D RoPE, Atom Encoder/Decoderì—ì„œëŠ” **4D Axial RoPE** (3D ì›ì ì¢Œí‘œ + 1D residue index)

<details markdown="1">
<summary>ğŸ“ Adaptive Layer Transformer Block (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class AdaptiveTransformerBlock(nn.Module):
    """SimpleFoldì˜ ê¸°ë³¸ ë¹Œë”© ë¸”ë¡ â€” DiT ìŠ¤íƒ€ì¼ adaptive layer"""
    
    def __init__(self, dim: int, heads: int, local_attention: bool = False):
        super().__init__()
        self.norm1 = nn.LayerNorm(dim)
        self.norm2 = nn.LayerNorm(dim)
        self.attn = MultiHeadAttention(
            dim=dim, heads=heads,
            qk_norm=True,                   # QK-normalization
            local_mask=local_attention,      # local attention for atom level
        )
        self.ffn = SwiGLU(dim)              # SwiGLU replaces standard FFN
        
        # adaLN: timestep t â†’ (scale1, shift1, gate1, scale2, shift2, gate2)
        self.ada_proj = nn.Linear(dim, 6 * dim)
    
    def forward(self, x: Tensor, t_emb: Tensor) -> Tensor:
        """
        x: (B, N, dim) input tokens
        t_emb: (B, dim) timestep embedding
        """
        # Compute adaptive parameters from timestep
        s1, b1, g1, s2, b2, g2 = self.ada_proj(t_emb).chunk(6, dim=-1)
        
        # Attention with adaLN
        h = self.norm1(x) * (1 + s1) + b1           # adaLN modulation
        h = self.attn(h)                              # (B, N, dim)
        x = x + g1 * h                               # gated residual
        
        # FFN with adaLN
        h = self.norm2(x) * (1 + s2) + b2
        h = self.ffn(h)
        x = x + g2 * h
        
        return x
```

</details>


> SimpleFoldëŠ” AlphaFold2ì˜ Evoformerì—ì„œ triangle attentionê³¼ pair representationì´ í•µì‹¬ì´ì—ˆë˜ ê²ƒê³¼ ë‹¬ë¦¬, ì´ ëª¨ë“  ê²ƒì„ í‘œì¤€ Transformerë¡œ ëŒ€ì²´í–ˆë‹¤. Rotational symmetryëŠ” equivariant architecture ëŒ€ì‹  **SO(3) data augmentation**ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤.
{: .prompt-info }

### Key Innovation: ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆ ì—†ì´ í•™ìŠµ ê°€ëŠ¥í•œ ì´ìœ 

SimpleFoldê°€ pair representationê³¼ triangle attention ì—†ì´ë„ ì‘ë™í•˜ëŠ” ì´ìœ ëŠ” ë‘ ê°€ì§€ë‹¤:

1. **ESM2-3Bì˜ ê°•ë ¥í•œ sequence embedding**: ì§„í™”ì  ì •ë³´ë¥¼ MSA ëŒ€ì‹  pretrained PLMì—ì„œ ì¶”ì¶œ. ESM2ëŠ” ì´ë¯¸ ìˆ˜ì–µ ê°œì˜ ë‹¨ë°±ì§ˆ ì„œì—´ë¡œ í•™ìŠµë˜ì–´ í’ë¶€í•œ co-evolutionary ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤.

2. **ìŠ¤ì¼€ì¼ì˜ í˜**: 100Mì—ì„œ 3Bê¹Œì§€ ìŠ¤ì¼€ì¼ì—…í•˜ë©´ì„œ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒëœë‹¤. íŠ¹íˆ ì–´ë ¤ìš´ ë²¤ì¹˜ë§ˆí¬(CASP14)ì—ì„œ ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ê°€ ë” í¬ê²Œ ë‚˜íƒ€ë‚œë‹¤. ì´ëŠ” ì¶©ë¶„í•œ ìš©ëŸ‰ì„ ê°€ì§„ ë²”ìš© ëª¨ë¸ì´ ë„ë©”ì¸ ì§€ì‹ì„ ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.

![Figure 3: Scaling](/assets/img/posts/simplefold-folding-proteins-simpler/fig3_scaling.png)
_Figure 3: SimpleFoldì˜ ëª¨ë¸ í¬ê¸°ë³„ ì„±ëŠ¥ ìŠ¤ì¼€ì¼ë§ê³¼ consumer hardwareì—ì„œì˜ inference ì‹œê°„. ì¶œì²˜: ì› ë…¼ë¬¸_

### Training & Inference

**Training Objective**: Flow Matching loss + LDDT lossì˜ ê°€ì¤‘í•©.

$$\ell = \ell_{\text{FM}} + \alpha(t) \cdot \ell_{\text{LDDT}}$$

Flow Matching lossëŠ” í‘œì¤€ velocity matchingì´ë‹¤:

$$\ell_{\text{FM}} = \mathbb{E}_{x, s, \epsilon, t} \left[ \frac{1}{N_a} \| v_\theta(x_t, s, t) - (x - \epsilon) \|^2 \right]$$

ì—¬ê¸°ì„œ linear interpolant path $x_t = tx + (1-t)\epsilon$ë¥¼ ì‚¬ìš©í•œë‹¤ (rectified flowì™€ ë™ì¼).

LDDT lossëŠ” ìƒì„±ëœ êµ¬ì¡°ì˜ local atomic distance ì •í™•ë„ë¥¼ ì¸¡ì •í•œë‹¤. One-step Eulerë¡œ ì¶”ì •í•œ êµ¬ì¡° $\hat{x} = x_t + (1-t)v_\theta$ì˜ ì›ì ê°„ ê±°ë¦¬ì™€ ground truth ê°„ ê±°ë¦¬ë¥¼ ë¹„êµí•œë‹¤. ì´ auxiliary lossê°€ íŠ¹íˆ side chain ê°™ì€ ì„¸ë°€í•œ êµ¬ì¡°ì˜ ì •í™•ë„ë¥¼ ë†’ì´ëŠ” ë° ê¸°ì—¬í•œë‹¤.

**Timestep resampling**: ì¼ë°˜ì ì¸ image generationì—ì„œëŠ” $t=0.5$ ê·¼ì²˜ë¥¼ oversamplingí•˜ì§€ë§Œ, SimpleFoldëŠ” **$t=1$ (clean data) ê·¼ì²˜ë¥¼ oversampling**í•œë‹¤: $p(t) = 0.98 \cdot \text{LN}(0.8, 1.7) + 0.02 \cdot \mathcal{U}(0, 1)$. ì´ëŠ” ë‹¨ë°±ì§ˆ êµ¬ì¡°ì˜ "secondary structure â†’ backbone â†’ side chain" ê³„ì¸µ ë•Œë¬¸ì´ë‹¤.

<details markdown="1">
<summary>ğŸ“ Training Loop Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Training Loop
for batch in dataloader:
    x = batch["all_atom_coords"]       # (B, Na, 3) ground truth
    s = batch["aa_sequence"]           # (B, Nr) amino acid sequence
    
    # Sample timestep from shifted logistic-normal
    # Biased toward t=1 (clean data) for fine structure learning
    t = 0.98 * logistic_normal(0.8, 1.7) + 0.02 * uniform(0, 1)  # (B,)
    
    # Linear interpolant: noise â†’ data
    eps = torch.randn_like(x)            # (B, Na, 3) â€” Îµ ~ N(0, I)
    x_t = t * x + (1 - t) * eps         # noisy coords â€” Eq. 2.1
    v_target = x - eps                   # target velocity â€” v_t = x - Îµ
    
    # Forward pass
    v_pred = model(x_t, s, t)           # (B, Na, 3) â€” predicted vÌ‚_Î¸
    
    # Flow Matching loss â€” Eq. 2.2
    loss_fm = F.mse_loss(v_pred, v_target)
    
    # One-step Euler estimate for LDDT loss
    x_hat = x_t + (1 - t) * v_pred     # estimated clean structure
    loss_lddt = compute_lddt_loss(x, x_hat, cutoff=C)  # Eq. 2.3
    
    # Combined loss â€” Eq. 2.4
    # Pre-training: Î±(t) = 1
    # Finetuning:   Î±(t) = 1 + 8 * ReLU(t - 0.5)
    loss = loss_fm + alpha(t) * loss_lddt
    loss.backward()
    optimizer.step()
```

</details>


<details markdown="1">
<summary>ğŸ“ Inference (Sampling) Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Inference: Stochastic sampling via Langevin-style SDE â€” Eq. 2.5
def fold_protein(model, sequence, n_steps=200, tau=0.01):
    """
    sequence: amino acid sequence
    tau: stochasticity scale
         0.01 for single structure folding
         0.6  for ensemble generation
    """
    Na = get_num_atoms(sequence)
    x_t = torch.randn(1, Na, 3)   # x_0 ~ N(0, I) â€” start from noise
    dt = 1.0 / n_steps
    
    for i in range(n_steps):
        t = i * dt                 # t: 0 â†’ 1
        
        # Predict velocity field
        v = model(x_t, sequence, t)                    # vÌ‚_Î¸(x_t, s, t)
        
        # Compute score from velocity â€” s_Î¸ = (tÂ·v - x_t) / (1-t)
        score = (t * v - x_t) / (1 - t + 1e-8)
        
        # Diffusion coefficient â€” w(t) = 2(1-t) / (t + Î·)
        eta = 1e-5
        w_t = 2 * (1 - t) / (t + eta)
        
        # Euler-Maruyama step
        drift = v * dt + 0.5 * w_t * score * dt
        diffusion = math.sqrt(tau * w_t * dt) * torch.randn_like(x_t)
        x_t = x_t + drift + diffusion
    
    return x_t  # folded all-atom structure
```

</details>


**Training Data**: ì´ ~9M êµ¬ì¡° (3B ëª¨ë¸ ê¸°ì¤€)

| ë°ì´í„° ì†ŒìŠ¤ | êµ¬ì¡° ìˆ˜ | ì„¤ëª… |
|---|---|---|
| **PDB** | ~160K | ì‹¤í—˜ êµ¬ì¡° (cutoff: May 2020) |
| **SwissProt (AFDB)** | ~270K | pLDDT > 85, std < 15 í•„í„° |
| **AFESM** | ~1.9M | í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ, pLDDT > 0.8 |
| **AFESM-E** (3B only) | ~8.6M | í´ëŸ¬ìŠ¤í„°ë‹¹ ìµœëŒ€ 10ê°œ |

2ë‹¨ê³„ í•™ìŠµ: Pre-training (ì „ì²´ ë°ì´í„°, max length 256) â†’ Finetuning (PDB + SwissProt, max length 512, LDDT loss ê°€ì¤‘ì¹˜ ì¦ê°€).

## Results

í•µì‹¬ ë²¤ì¹˜ë§ˆí¬ì¸ CAMEO22ì™€ CASP14ì—ì„œì˜ ê²°ê³¼ë¥¼ ì •ë¦¬í•œë‹¤. SimpleFoldëŠ” MSAë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ì„œë„ PLM ê¸°ë°˜ baselineë“¤ê³¼ competitiveí•˜ê±°ë‚˜ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

| Model | Type | CAMEO22 TMâ†‘ | CASP14 TMâ†‘ | CASP14 LDDTâ†‘ |
|---|---|---|---|---|
| **AlphaFold2** | MSA | **0.863 / 0.942** | **0.845 / 0.907** | **0.778 / 0.817** |
| RoseTTAFold2 | MSA | 0.864 / 0.947 | 0.802 / 0.881 | 0.638 / 0.669 |
| AlphaFlow | MSA+FM | 0.840 / 0.927 | 0.740 / 0.812 | 0.632 / 0.662 |
| ESMFold | PLM | 0.853 / 0.933 | 0.701 / 0.792 | 0.637 / 0.705 |
| ESMFlow | PLM+FM | 0.818 / 0.893 | 0.627 / 0.679 | 0.525 / 0.539 |
| **SimpleFold-3B** | PLM+FM | 0.837 / 0.916 | 0.720 / 0.792 | 0.666 / 0.709 |
| SimpleFold-100M | PLM+FM | 0.803 / 0.878 | 0.611 / 0.628 | 0.537 / 0.549 |

ìˆ˜ì¹˜ê°€ ë§í•´ì£¼ëŠ” ê²ƒ: SimpleFold-3BëŠ” CASP14ì—ì„œ ESMFold(TM 0.701)ì„ ë„˜ì–´ì„œëŠ” TM-score 0.720ì„ ë‹¬ì„±í–ˆë‹¤. ë‘ ëª¨ë¸ ëª¨ë‘ PLM ê¸°ë°˜ì´ê³  MSAë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì§€ë§Œ, SimpleFoldëŠ” ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆ ì—†ì´ë„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤€ë‹¤. CAMEO22ì—ì„œëŠ” AlphaFold2 ëŒ€ë¹„ 95% ì´ìƒì˜ ì„±ëŠ¥ì„ ìœ ì§€í•œë‹¤.

> AlphaFlow, ESMFlow ê°™ì€ flow matching ëª¨ë¸ë“¤ì€ ì˜¤íˆë ¤ ì›ë˜ì˜ regression ëª¨ë¸(AlphaFold2, ESMFold)ë³´ë‹¤ folding benchmarkì—ì„œ ë‚®ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. ì´ëŠ” ë²¤ì¹˜ë§ˆí¬ê°€ ë‹¨ì¼ ground truthë§Œ í¬í•¨í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ, deterministic regressionì´ ìœ ë¦¬í•œ êµ¬ì¡°ì´ë‹¤. SimpleFoldëŠ” ì´ëŸ° ë¶ˆë¦¬í•œ í‰ê°€ ë°©ì‹ì—ì„œë„ competitiveí•œ ê²°ê³¼ë¥¼ ë‹¬ì„±í–ˆë‹¤ëŠ” ì ì—ì„œ ì£¼ëª©í•  ë§Œí•˜ë‹¤.
{: .prompt-info }

**Ensemble Generation**: SimpleFoldëŠ” stochasticity parameter $\tau$ë¥¼ ì¡°ì ˆí•´ conformation ensembleì„ ìƒì„±í•  ìˆ˜ ìˆë‹¤. MD ensemble í‰ê°€ì—ì„œ ë³„ë„ì˜ finetuning ì—†ì´ë„ Pairwise RMSD correlation 0.44ë¥¼ ë‹¬ì„±í•˜ë©°, AlphaFold2(0.10)ë³´ë‹¤ ensemble diversityë¥¼ ì˜ ëª¨ë¸ë§í•œë‹¤. MD ë°ì´í„°ë¡œ finetuningí•œ SimpleFold-MDëŠ” AlphaFlow-MDì™€ comparableí•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.

**Scaling**: 100M â†’ 3B ìŠ¤ì¼€ì¼ì—…ì— ë”°ë¼ ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ì—ì„œ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒëœë‹¤. íŠ¹íˆ ì–´ë ¤ìš´ CASP14ì—ì„œì˜ ê°œì„ í­ì´ CAMEO22ë³´ë‹¤ í¬ë‹¤ â€” ë” í° ëª¨ë¸ì´ ë³µì¡í•œ folding ë¬¸ì œë¥¼ ë” ì˜ í•´ê²°í•œë‹¤ëŠ” ì¦ê±°ë‹¤.

## Discussion

### í•œê³„

ë…¼ë¬¸ì—ì„œ ë°íŒ í•œê³„ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤:

- SimpleFoldëŠ” [AlphaFold2](/posts/alphafold2-highly-accurate-protein-structure-prediction/) ìˆ˜ì¤€ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•œë‹¤. MSA ê¸°ë°˜ ëª¨ë¸ê³¼ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ í–¥í›„ ê³¼ì œ.
- í˜„ì¬ëŠ” single-chain foldingì— ì§‘ì¤‘í–ˆìœ¼ë©°, protein complex predictionìœ¼ë¡œì˜ í™•ì¥ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ìŒ ë‹¨ê³„.
- ë…¼ë¬¸ì—ì„œëŠ” ë” í° ëª¨ë¸(3B ì´ìƒ)ê³¼ ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ê³„ì† ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆë‹¤.

### ì¬í˜„ì„±

- **ì½”ë“œ ê³µê°œ**: âœ… [apple/ml-simplefold](https://github.com/apple/ml-simplefold) (Jupyter notebook í¬í•¨)
- **í•™ìŠµ ë°ì´í„°**: PDB (ê³µê°œ), AFDB/AFESM (ê³µê°œ) â€” ì ‘ê·¼ ê°€ëŠ¥
- **í•„ìš” GPU**: ë…¼ë¬¸ ë¯¸ëª…ì‹œ, 3B ëª¨ë¸ ê·œëª¨ë¡œ ì¶”ì • ì‹œ ëŒ€ê·œëª¨ GPU í´ëŸ¬ìŠ¤í„° í•„ìš”
- **ì¬í˜„ ë‚œì´ë„**: â­â­â­ (ì¤‘ê°„ â€” í•™ìŠµ ê·œëª¨ëŠ” í¬ì§€ë§Œ ì½”ë“œ ê³µê°œë¨)

> ì´ì „ì— ë¦¬ë·°í•œ [AlphaFold2](/posts/alphafold2-highly-accurate-protein-structure-prediction/)ì™€ ë¹„êµí•˜ë©´, SimpleFoldëŠ” ì•„í‚¤í…ì²˜ ë‹¨ìˆœí™”ì˜ ê·¹ë‹¨ì„ ë³´ì—¬ì¤€ë‹¤. AlphaFold2ì˜ Evoformerì—ì„œ triangle attentionê³¼ pair representationì´ í•µì‹¬ì´ì—ˆë‹¤ë©´, SimpleFoldëŠ” ì´ ëª¨ë“  ê²ƒì„ í‘œì¤€ Transformerë¡œ ëŒ€ì²´í–ˆë‹¤.
{: .prompt-info }

## TL;DR

1. **SimpleFold**: MSA, pair representation, triangle attention ì—†ì´ **í‘œì¤€ Transformer + Flow Matching**ë§Œìœ¼ë¡œ protein foldingì„ ë‹¬ì„±í•œ ìµœì´ˆì˜ ëª¨ë¸.
2. **í•µì‹¬ êµ¬ì¡°**: Atom Encoder â†’ Grouping â†’ Residue Trunk â†’ Ungrouping â†’ Atom Decoder. ESM2-3Bë¡œ sequence conditioning.
3. **ì„±ëŠ¥**: CASP14ì—ì„œ ESMFoldë¥¼ ëŠ¥ê°€í•˜ê³ , AlphaFold2 ëŒ€ë¹„ ~85% ìˆ˜ì¤€. Ensemble generationì—ì„œëŠ” ê¸°ì¡´ ë°©ë²• ëŠ¥ê°€.

## Paper Info

| í•­ëª© | ë‚´ìš© |
|---|---|
| **Title** | Folding Proteins is Simpler than You Think |
| **Authors** | Dingquan Li et al. (Apple) |
| **Venue** | arXiv preprint (December 2025) |
| **Submitted** | 2025-09-25 |
| **Paper** | [arXiv](https://arxiv.org/abs/2509.18480) |
| **Code** | [GitHub](https://github.com/apple/ml-simplefold) |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
