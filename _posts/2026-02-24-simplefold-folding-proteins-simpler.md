---
title: "SimpleFold: Folding Proteins is Simpler than You Think"
date: 2026-02-24 12:00:00 +0900
description: "Appleì˜ SimpleFoldëŠ” ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆ ì—†ì´ ì¼ë°˜ Transformer + Flow Matchingë§Œìœ¼ë¡œ protein foldingì„ ìˆ˜í–‰í•œë‹¤. 3B íŒŒë¼ë¯¸í„°ê¹Œì§€ ìŠ¤ì¼€ì¼í•˜ì—¬ SOTAê¸‰ ì„±ëŠ¥ê³¼ ensemble generationì—ì„œ ê°•ì ì„ ë³´ì¸ë‹¤."
categories: [Paper Review]
tags: [protein-folding, flow-matching, transformer, simplefold, alphafold, esm, generative-model, scaling]
math: true
mermaid: true
image:
  path: /assets/img/posts/simplefold-folding-proteins-simpler/fig1_predictions.png
  alt: "SimpleFoldì˜ protein folding ì˜ˆì¸¡ ê²°ê³¼ì™€ ìŠ¤ì¼€ì¼ë§ ê³¡ì„ "
---

ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì—ì„œ AlphaFold2ê°€ ì œì‹œí•œ triangle attention, pair representation, MSA â€” ì´ ëª¨ë“  ê²ƒì´ ì •ë§ "í•„ìˆ˜"ì¼ê¹Œ? Appleì—ì„œ ë‚˜ì˜¨ SimpleFoldëŠ” ì´ ì§ˆë¬¸ì— ì •ë©´ìœ¼ë¡œ ë„ì „í•œë‹¤. **ì¼ë°˜ ëª©ì  Transformer + Flow Matching**ë§Œìœ¼ë¡œ, ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆì„ ì „ë¶€ ì œê±°í•˜ê³ ë„ competitiveí•œ folding ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤. ì‹¬ì§€ì–´ ensemble predictionì—ì„œëŠ” ê¸°ì¡´ ë°©ë²•ë“¤ì„ ëŠ¥ê°€í•˜ê¸°ë„ í•œë‹¤.

ì´ ë…¼ë¬¸ì´ í¥ë¯¸ë¡œìš´ ì´ìœ ëŠ” ë‹¨ìˆœí•˜ë‹¤: protein foldingì„ "text-to-image" ìƒì„± ë¬¸ì œì™€ ë™ì¼í•œ í”„ë ˆì„ì›Œí¬ë¡œ í’€ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì£¼ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. ì•„ë¯¸ë…¸ì‚° ì„œì—´ì´ "í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸"ì´ê³ , 3D ì›ì ì¢Œí‘œê°€ "ìƒì„±ëœ ì´ë¯¸ì§€"ì¸ ì…ˆì´ë‹¤.

## Problem: ì™œ SimpleFoldê°€ í•„ìš”í•œê°€

AlphaFold2 ì´í›„ protein folding ëª¨ë¸ë“¤ì€ ê³µí†µì ìœ¼ë¡œ ë³µì¡í•œ ë„ë©”ì¸ íŠ¹í™” ì•„í‚¤í…ì²˜ì— ì˜ì¡´í•´ì™”ë‹¤:

- **MSA (Multiple Sequence Alignment)**: ì§„í™”ì  ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•œ ë¹„ìš©ì´ í° ê²€ìƒ‰ ê³¼ì •
- **Pair representation**: ì”ê¸° ìŒ ê°„ì˜ ê´€ê³„ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ëŠ” $O(N^2)$ í–‰ë ¬
- **Triangle attention/updates**: pair representationì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³ ë¹„ìš© ì—°ì‚°

ì´ ì„¤ê³„ë“¤ì€ "ë‹¨ë°±ì§ˆ êµ¬ì¡° ìƒì„± ê³¼ì •ì— ëŒ€í•œ í˜„ì¬ì˜ ì´í•´"ë¥¼ ëª¨ë¸ì— í•˜ë“œì½”ë”©í•œ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ vision ë¶„ì•¼ì—ì„œ ë²”ìš© Transformerê°€ ë„ë©”ì¸ íŠ¹í™” ì•„í‚¤í…ì²˜ë¥¼ ëŒ€ì²´í•œ ê²ƒì²˜ëŸ¼, protein foldingì—ì„œë„ ê°™ì€ ì¼ì´ ê°€ëŠ¥í•˜ì§€ ì•Šì„ê¹Œ?

ë˜ ë‹¤ë¥¸ ë¬¸ì œëŠ” **ensemble generation**ì´ë‹¤. ê¸°ì¡´ folding ëª¨ë¸ ëŒ€ë¶€ë¶„ì€ deterministic regression objectiveë¡œ í•™ìŠµë˜ì–´, í•˜ë‚˜ì˜ êµ¬ì¡°ë§Œ ì˜ˆì¸¡í•œë‹¤. í•˜ì§€ë§Œ ì‹¤ì œ ë‹¨ë°±ì§ˆì€ ì—¬ëŸ¬ conformationì˜ ì•™ìƒë¸”ë¡œ ì¡´ì¬í•˜ë©°, ì´ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ê²ƒì´ drug discoveryì—ì„œ cryptic pocket ë°œê²¬ ë“±ì— í•µì‹¬ì ì´ë‹¤.

> SimpleFoldëŠ” ì´ ë‘ ê°€ì§€ ë¬¸ì œë¥¼ ë™ì‹œì— í•´ê²°í•œë‹¤: ì•„í‚¤í…ì²˜ ë‹¨ìˆœí™” + ìƒì„±ì  ì ‘ê·¼ì„ í†µí•œ ensemble modeling.
{: .prompt-tip }

## Key Idea: Text-to-3Dë¡œì„œì˜ Protein Folding

SimpleFoldì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë†€ë¼ìš¸ ì •ë„ë¡œ ì§ê´€ì ì´ë‹¤:

1. **ì•„ë¯¸ë…¸ì‚° ì„œì—´ â†’ 3D êµ¬ì¡°** ë§¤í•‘ì„ conditional generative modelë¡œ ìºìŠ¤íŒ…
2. **Flow Matching** objectiveë¡œ noiseì—ì„œ all-atom êµ¬ì¡°ë¥¼ ìƒì„±
3. ì•„í‚¤í…ì²˜ëŠ” DiT (Diffusion Transformer) ìŠ¤íƒ€ì¼ì˜ **adaptive layerê°€ ìˆëŠ” í‘œì¤€ Transformer ë¸”ë¡**ë§Œ ì‚¬ìš©
4. Pair representation, triangle update, MSA ëª¨ë‘ ì œê±°

ê¸°ì¡´ flow matching ê¸°ë°˜ protein ëª¨ë¸ë“¤(AlphaFlow, ESMFlow, Proteina)ì´ ì—¬ì „íˆ AlphaFold2ì˜ ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆì„ ìœ ì§€í–ˆë˜ ê²ƒê³¼ ëŒ€ë¹„ëœë‹¤. SimpleFoldëŠ” ì²˜ìŒë¶€í„° ìˆœìˆ˜ ìƒì„± ëª¨ë¸ë¡œ from scratch í•™ìŠµí•œ ìµœì´ˆì˜ flow matching protein folding ëª¨ë¸ì´ë‹¤.

## How it works

### Overview

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ í¬ê²Œ ì„¸ ë‹¨ê³„ë‹¤: (1) noisy ì›ì ì¢Œí‘œë¥¼ atom-levelë¡œ ì¸ì½”ë”©, (2) residue-level trunkì—ì„œ heavy ì—°ì‚° ìˆ˜í–‰, (3) ë‹¤ì‹œ atom-levelë¡œ ë””ì½”ë”©í•˜ì—¬ velocity field ì˜ˆì¸¡.

![SimpleFold Architecture](/assets/img/posts/simplefold-folding-proteins-simpler/fig2_architecture.png)
_Figure 2: SimpleFold ì•„í‚¤í…ì²˜. Atom Encoder â†’ Grouping â†’ Residue Trunk â†’ Ungrouping â†’ Atom Decoder. ëª¨ë“  ëª¨ë“ˆì´ ë™ì¼í•œ adaptive layer Transformer ë¸”ë¡ì„ ê³µìœ í•œë‹¤. ì¶œì²˜: ì› ë…¼ë¬¸_

```mermaid
graph TD
    A["ğŸ§¬ AA Sequence s"] --> PLM["ESM2-3B<br/>(Frozen PLM)"]
    PLM --> E["Sequence Embeddings<br/>e âˆˆ â„^(Nr Ã— de)"]
    
    N["ğŸ² Noisy Coords x_t<br/>+ Atomic Features"] --> AE["Atom Encoder<br/>(Local Attention)"]
    AE --> AT["Atom Tokens<br/>a âˆˆ â„^(Na Ã— da)"]
    AT --> G["Grouping<br/>(Avg Pool per Residue)"]
    G --> RT["Residue Tokens<br/>r âˆˆ â„^(Nr Ã— da)"]
    
    RT --> CAT["Concat"]
    E --> CAT
    CAT --> TRUNK["Residue Trunk<br/>(Heavy Transformer)"]
    
    T["â± Timestep t"] --> TRUNK
    
    TRUNK --> UG["Ungrouping<br/>(Broadcast to Atoms)"]
    UG --> SKIP["+ Skip Connection"]
    AT --> SKIP
    SKIP --> AD["Atom Decoder<br/>(Local Attention)"]
    AD --> V["Predicted Velocity áº‘_t"]
    
    style A fill:#e1f5fe
    style V fill:#e8f5e9
    style TRUNK fill:#fff3e0
```

### Representation: Hierarchical "Fine â†’ Coarse â†’ Fine"

SimpleFoldëŠ” ë‹¨ë°±ì§ˆì˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ "fine â†’ coarse â†’ fine" ìŠ¤í‚´ìœ¼ë¡œ ì²˜ë¦¬í•œë‹¤:

- **Fine (Atom level)**: Atom Encoderì—ì„œ ê° ì›ìì˜ noisy ì¢Œí‘œ + ì›ì íŠ¹ì„±(type, charge)ì„ ì²˜ë¦¬. **Local attention mask**ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°™ì€ ì”ê¸° ë° ì¸ì ‘ ì”ê¸°ì˜ ì›ìë“¤ë¼ë¦¬ë§Œ attend.
- **Coarse (Residue level)**: **Grouping** ì—°ì‚°ìœ¼ë¡œ í•œ ì”ê¸°ì— ì†í•œ atom tokenë“¤ì„ í‰ê·  í’€ë§í•˜ì—¬ residue tokenì„ ìƒì„±. ì—¬ê¸°ì„œ ESM2-3Bì˜ sequence embeddingê³¼ concatë˜ì–´ Residue Trunkì— ì…ë ¥.
- **Fine (Atom level)**: **Ungrouping**ìœ¼ë¡œ residue tokenì„ ë‹¤ì‹œ atom ìˆ˜ë§Œí¼ broadcast. Atom Encoderì˜ ì¶œë ¥ê³¼ skip connectionì„ ë”í•œ í›„ Atom Decoderì—ì„œ ìµœì¢… velocity field ì˜ˆì¸¡.

![Grouping/Ungrouping Operations](/assets/img/posts/simplefold-folding-proteins-simpler/fig5_grouping.png)
_Figure 5: Groupingê³¼ Ungrouping ì—°ì‚°. Groupingì€ ê°™ì€ ì”ê¸°ì˜ atom tokenì„ í‰ê·  í’€ë§, Ungroupingì€ residue tokenì„ atom ìˆ˜ë§Œí¼ ë³µì œ. ì¶œì²˜: ì› ë…¼ë¬¸_

### Core Architecture: Adaptive Layer Transformer

SimpleFoldì˜ ëª¨ë“  ëª¨ë“ˆ(Atom Encoder, Residue Trunk, Atom Decoder)ì€ **ë™ì¼í•œ ë¹Œë”© ë¸”ë¡**ì„ ì‚¬ìš©í•œë‹¤: adaptive layer normalizationì´ ìˆëŠ” í‘œì¤€ Transformer ë¸”ë¡.

![Architecture Comparison](/assets/img/posts/simplefold-folding-proteins-simpler/fig4_af2_comparison.png)
_Figure 4: (a) AlphaFold2ì˜ Evoformer â€” pair representationê³¼ triangle attentionì´ í¬í•¨ëœ ë³µì¡í•œ êµ¬ì¡°. (b) SimpleFoldì˜ Transformer ë¸”ë¡ â€” adaptive layerë§Œìœ¼ë¡œ êµ¬ì„±ëœ ë‹¨ìˆœí•œ êµ¬ì¡°. ì¶œì²˜: ì› ë…¼ë¬¸_

ê° Transformer ë¸”ë¡ì˜ êµ¬ì„±:

1. **Adaptive LayerNorm (adaLN)**: timestep $t$ë¥¼ conditioningìœ¼ë¡œ ë°›ì•„ scale/shift íŒŒë¼ë¯¸í„°ë¥¼ ìƒì„±
2. **Multi-Head Attention (MHA)**: QK-normalization ì ìš©
3. **SwiGLU FFN**: í‘œì¤€ FFN ëŒ€ì‹  SwiGLU í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
4. **RoPE**: Residue Trunkì—ì„œëŠ” 1D RoPE, Atom Encoder/Decoderì—ì„œëŠ” **4D Axial RoPE** (3D ì›ì ì¢Œí‘œ + 1D residue index)

ëª¨ë¸ êµ¬ì„± (ìµœì†Œ â†’ ìµœëŒ€):

| Model | Params | GFLOPs | Trunk Dim | Trunk Heads | Trunk Blocks |
|---|---|---|---|---|---|
| SimpleFold-100M | 94M | 66.5 | 768 | 12 | 8 |
| SimpleFold-360M | 360M | 189.9 | 1024 | 16 | 18 |
| SimpleFold-700M | 687M | 310.4 | 1152 | 16 | 28 |
| SimpleFold-1.1B | 1.11B | 496.0 | 1280 | 20 | 36 |
| SimpleFold-3B | 2.86B | 1382.4 | 2048 | 32 | 36 |

AlphaFold2ëŠ” ~95M íŒŒë¼ë¯¸í„°ì¸ë° forward GFLOPsê°€ **~30,935**ì´ë‹¤. SimpleFold-3BëŠ” 2.86B íŒŒë¼ë¯¸í„°ì„ì—ë„ **~1,382 GFLOPs**. Triangle updateì™€ pair representationì˜ ì œê±°ê°€ ê³„ì‚° íš¨ìœ¨ì„ ê·¹ì ìœ¼ë¡œ ê°œì„ í•œë‹¤.

<details>
<summary>ğŸ“ Overall Architecture Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class SimpleFold(nn.Module):
    """SimpleFold: Flow Matching Protein Folding Model"""
    
    def __init__(self, config):
        super().__init__()
        # Frozen PLM for sequence conditioning
        self.plm = ESM2_3B(frozen=True)  # (Nr, de=2560)
        
        # Atom Encoder: local attention Transformer
        self.atom_encoder = TransformerStack(
            dim=config.atom_dim,      # e.g., 640 for 3B
            heads=config.atom_heads,  # e.g., 10
            blocks=config.atom_blocks,# e.g., 4
            local_attention=True,     # attend only within nearby residues
            rope_mode="4d_axial",     # 3D coords + 1D residue index
        )
        
        # Residue Trunk: heavy global attention Transformer
        self.residue_trunk = TransformerStack(
            dim=config.trunk_dim,     # e.g., 2048 for 3B
            heads=config.trunk_heads, # e.g., 32
            blocks=config.trunk_blocks,# e.g., 36
            local_attention=False,    # full global attention
            rope_mode="1d",           # residue sequence position
        )
        
        # Atom Decoder: symmetric to encoder
        self.atom_decoder = TransformerStack(
            dim=config.atom_dim,
            heads=config.atom_heads,
            blocks=config.atom_blocks,
            local_attention=True,
            rope_mode="4d_axial",
        )
        
        # Projections
        self.seq_proj = nn.Linear(config.atom_dim + config.de, config.trunk_dim)
        self.ungroup_proj = nn.Linear(config.trunk_dim, config.atom_dim)
        self.output_proj = nn.Linear(config.atom_dim, 3)  # velocity in R^3
    
    def forward(self, x_t, seq, t):
        """
        x_t: (B, Na, 3) noisy atom coords at timestep t
        seq: (B, Nr) amino acid sequence
        t: (B,) timestep in [0, 1]
        Returns: (B, Na, 3) predicted velocity field
        """
        # Step 1: Encode sequence with frozen PLM
        e = self.plm(seq)  # (B, Nr, de)
        
        # Step 2: Atom Encoder (fine level)
        atom_features = build_atom_features(x_t)  # concat coords + type + charge
        a = self.atom_encoder(atom_features, t)     # (B, Na, da)
        a_skip = a  # save for skip connection
        
        # Step 3: Grouping â€” avg pool atoms per residue
        r = grouping(a, residue_map)  # (B, Nr, da)
        
        # Step 4: Concat sequence embeddings + project
        r = self.seq_proj(torch.cat([r, e], dim=-1))  # (B, Nr, d_trunk)
        
        # Step 5: Residue Trunk (coarse level) â€” heavy computation
        r = self.residue_trunk(r, t)  # (B, Nr, d_trunk)
        
        # Step 6: Ungrouping â€” broadcast residue tokens to atoms
        a = self.ungroup_proj(ungrouping(r, residue_map))  # (B, Na, da)
        a = a + a_skip  # skip connection
        
        # Step 7: Atom Decoder (fine level)
        a = self.atom_decoder(a, t)  # (B, Na, da)
        
        # Step 8: Output velocity
        v_t = self.output_proj(a)  # (B, Na, 3)
        return v_t
```

</details>

### Key Innovation: ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆ ì—†ì´ í•™ìŠµ ê°€ëŠ¥í•œ ì´ìœ 

SimpleFoldê°€ pair representationê³¼ triangle attention ì—†ì´ë„ ì‘ë™í•˜ëŠ” ì´ìœ ëŠ” ë‘ ê°€ì§€ë‹¤:

1. **ESM2-3Bì˜ ê°•ë ¥í•œ sequence embedding**: ì§„í™”ì  ì •ë³´ë¥¼ MSA ëŒ€ì‹  pretrained PLMì—ì„œ ì¶”ì¶œ. ESM2ëŠ” ì´ë¯¸ ìˆ˜ì–µ ê°œì˜ ë‹¨ë°±ì§ˆ ì„œì—´ë¡œ í•™ìŠµë˜ì–´ í’ë¶€í•œ co-evolutionary ì •ë³´ë¥¼ ë‹´ê³  ìˆë‹¤.

2. **ìŠ¤ì¼€ì¼ì˜ í˜**: 100Mì—ì„œ 3Bê¹Œì§€ ìŠ¤ì¼€ì¼ì—…í•˜ë©´ì„œ ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒëœë‹¤. íŠ¹íˆ ì–´ë ¤ìš´ ë²¤ì¹˜ë§ˆí¬(CASP14)ì—ì„œ ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ê°€ ë” í¬ê²Œ ë‚˜íƒ€ë‚œë‹¤. ì´ëŠ” ì¶©ë¶„í•œ ìš©ëŸ‰ì„ ê°€ì§„ ë²”ìš© ëª¨ë¸ì´ ë„ë©”ì¸ ì§€ì‹ì„ ë°ì´í„°ë¡œë¶€í„° ì§ì ‘ í•™ìŠµí•  ìˆ˜ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.

### Training & Inference

**Training Objective**: Flow Matching loss + LDDT lossì˜ ê°€ì¤‘í•©.

$$\ell = \ell_{\text{FM}} + \alpha(t) \cdot \ell_{\text{LDDT}}$$

Flow Matching lossëŠ” í‘œì¤€ velocity matchingì´ë‹¤:

$$\ell_{\text{FM}} = \mathbb{E}_{x, s, \epsilon, t} \left[ \frac{1}{N_a} \| v_\theta(x_t, s, t) - (x - \epsilon) \|^2 \right]$$

ì—¬ê¸°ì„œ linear interpolant path $x_t = tx + (1-t)\epsilon$ë¥¼ ì‚¬ìš©í•œë‹¤ (rectified flowì™€ ë™ì¼).

LDDT lossëŠ” ìƒì„±ëœ êµ¬ì¡°ì˜ local atomic distance ì •í™•ë„ë¥¼ ì¸¡ì •í•œë‹¤:

$$\ell_{\text{LDDT}} = \mathbb{E} \left[ \frac{\sum_{i \neq j} \mathbf{1}(\delta_{ij} < C) \cdot \sigma(\| \delta_{ij} - \hat{\delta}^t_{ij} \|)}{\sum \mathbf{1}(\delta_{ij} < C)} \right]$$

<details>
<summary>ğŸ“ Training Loop Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Training Loop
for batch in dataloader:
    x = batch["all_atom_coords"]      # (B, Na, 3) ground truth
    s = batch["aa_sequence"]           # (B, Nr) amino acid sequence
    
    # Sample timestep from shifted logistic-normal
    # Biased toward t=1 (clean data) for fine structure learning
    t = 0.98 * logistic_normal(0.8, 1.7) + 0.02 * uniform(0, 1)  # (B,)
    
    # Linear interpolant: noise â†’ data
    eps = torch.randn_like(x)           # (B, Na, 3)
    x_t = t * x + (1 - t) * eps        # noisy coords
    v_target = x - eps                  # target velocity
    
    # Forward
    v_pred = model(x_t, s, t)           # (B, Na, 3)
    
    # Flow Matching loss
    loss_fm = F.mse_loss(v_pred, v_target)
    
    # One-step Euler estimate for LDDT loss
    x_hat = x_t + (1 - t) * v_pred     # estimated clean structure
    loss_lddt = compute_lddt_loss(x, x_hat, cutoff=C)
    
    # Combined loss
    loss = loss_fm + alpha(t) * loss_lddt
    loss.backward()
    optimizer.step()
```

</details>

<details>
<summary>ğŸ“ Inference (Sampling) Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Inference: Stochastic sampling via Langevin-style SDE
def fold_protein(model, sequence, n_steps=200, tau=0.01):
    """
    sequence: amino acid sequence
    tau: stochasticity scale (0.01 for folding, 0.6 for ensemble)
    """
    Na = get_num_atoms(sequence)
    x_t = torch.randn(1, Na, 3)  # x_0 ~ N(0, I)
    dt = 1.0 / n_steps
    
    for i in range(n_steps):
        t = i * dt  # t: 0 â†’ 1
        
        # Predict velocity
        v = model(x_t, sequence, t)
        
        # Compute score from velocity: s_Î¸ = (tÂ·v - x_t) / (1-t)
        score = (t * v - x_t) / (1 - t + 1e-8)
        
        # Diffusion coefficient
        eta = 1e-5  # numerical stability
        w_t = 2 * (1 - t) / (t + eta)
        
        # Euler-Maruyama step
        drift = v * dt + 0.5 * w_t * score * dt
        diffusion = math.sqrt(tau * w_t * dt) * torch.randn_like(x_t)
        x_t = x_t + drift + diffusion
    
    return x_t  # folded all-atom structure
```

</details>

**Training Data**: ì´ ~9M êµ¬ì¡° (3B ëª¨ë¸ ê¸°ì¤€)

| Source | ê·œëª¨ | í•„í„° ì¡°ê±´ |
|---|---|---|
| PDB (experimental) | ~160K | cutoff: May 2020 |
| SwissProt (AFDB distilled) | ~270K | pLDDT > 85, std < 15 |
| AFESM-E (distilled) | ~8.6M | pLDDT > 80, max 10/cluster |

**Timestep resampling**ì´ í¥ë¯¸ë¡œìš´ í¬ì¸íŠ¸ë‹¤: ì´ë¯¸ì§€ ìƒì„±ì—ì„œëŠ” $t \approx 0.5$ ê·¼ì²˜ë¥¼ denseí•˜ê²Œ ìƒ˜í”Œë§í•˜ì§€ë§Œ, ë‹¨ë°±ì§ˆì—ì„œëŠ” **$t \approx 1$ (clean data) ê·¼ì²˜ë¥¼ oversampling**í•œë‹¤. ë‹¨ë°±ì§ˆì˜ "secondary structure â†’ backbone â†’ side chain" ê³„ì¸µì  êµ¬ì¡° ë•Œë¬¸ì—, ë°ì´í„° manifold ê°€ê¹Œì´ì—ì„œ ë” ë§ì´ í•™ìŠµí•´ì•¼ fine-grained side chain ìœ„ì¹˜ë¥¼ ì •í™•í•˜ê²Œ ëª¨ë¸ë§í•  ìˆ˜ ìˆë‹¤.

## Results

### Protein Folding (CAMEO22 & CASP14)

| Model | Type | CAMEO22 TMâ†‘ | CASP14 TMâ†‘ | CASP14 GDTâ†‘ |
|---|---|---|---|---|
| **AlphaFold2** | MSA + Regression | 0.863 / 0.942 | **0.845 / 0.907** | **0.783 / 0.855** |
| RoseTTAFold2 | MSA + Regression | 0.864 / 0.947 | 0.802 / 0.881 | 0.740 / 0.824 |
| ESMFold | PLM + Regression | 0.853 / 0.933 | 0.701 / 0.792 | 0.622 / 0.711 |
| ESMFlow | PLM + Flow Matching | 0.818 / 0.893 | 0.627 / 0.679 | 0.539 / 0.544 |
| **SimpleFold-3B** | PLM + Flow Matching | **0.837 / 0.916** | **0.720 / 0.792** | **0.639 / 0.703** |

(ê° ë©”íŠ¸ë¦­ì€ mean / median. BoldëŠ” PLM ê¸°ë°˜ ëª¨ë¸ ì¤‘ ìµœê³ .)

SimpleFold-3BëŠ” ê°™ì€ PLM + Flow Matching ì¡°í•©ì¸ ESMFlowë¥¼ í¬ê²Œ ëŠ¥ê°€í•˜ë©°, PLM + Regressionì¸ ESMFoldë„ CASP14ì—ì„œ ì•ì„ ë‹¤. MSA ê¸°ë°˜ AlphaFold2ì™€ì˜ ê²©ì°¨ëŠ” ì—¬ì „íˆ ì¡´ì¬í•˜ì§€ë§Œ, MSA ì—†ì´ ì´ ìˆ˜ì¤€ì— ë„ë‹¬í•œ ê²ƒì´ í•µì‹¬ì´ë‹¤.

### Ensemble Generation (ATLAS MD & Two-State)

| Model | Pairwise RMSD râ†‘ | Global RMSF râ†‘ | Exposed Residue Jâ†‘ |
|---|---|---|---|
| AlphaFold2 | 0.10 | 0.21 | 0.32 |
| ESMFlow-MD | 0.19 | 0.31 | 0.49 |
| AlphaFlow-MD | 0.48 | 0.60 | 0.50 |
| **SimpleFold** (no tuning) | **0.44** | **0.45** | **0.39** |
| **SimpleFold-MD** (tuned) | 0.45 | 0.48 | **0.60** |

Ensemble predictionì—ì„œ SimpleFoldëŠ” **ë³„ë„ fine-tuning ì—†ì´ë„** ESMFlow-MDë³´ë‹¤ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤. íŠ¹íˆ exposed residue (drug discoveryì—ì„œ cryptic pocket ë°œê²¬ì— ì¤‘ìš”í•œ ì§€í‘œ)ì—ì„œ fine-tuned SimpleFold-MDê°€ 0.60ìœ¼ë¡œ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆë‹¤.

![Scaling Behavior](/assets/img/posts/simplefold-folding-proteins-simpler/fig3_scaling.png)
_Figure 3: SimpleFoldì˜ ìŠ¤ì¼€ì¼ë§ í–‰ë™. ëª¨ë¸ í¬ê¸°ì™€ ë°ì´í„° ì¦ê°€ì— ë”°ë¼ folding ì„±ëŠ¥ì´ ì§€ì†ì ìœ¼ë¡œ í–¥ìƒëœë‹¤. ì¶œì²˜: ì› ë…¼ë¬¸_

### Scaling Behavior

ìœ„ Figure 3ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆë“¯ì´, SimpleFoldëŠ” model sizeì™€ data size ì–‘ìª½ ëª¨ë‘ì—ì„œ **ëª…í™•í•œ scaling í–‰ë™**ì„ ë³´ì¸ë‹¤. íŠ¹íˆ CASP14 ê°™ì€ ì–´ë ¤ìš´ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ìŠ¤ì¼€ì¼ë§ íš¨ê³¼ê°€ ë” ë‘ë“œëŸ¬ì§„ë‹¤. ë…¼ë¬¸ì— ë”°ë¥´ë©´, protein foldingì—ì„œ ì´ëŸ¬í•œ ìŠ¤ì¼€ì¼ë§ í–‰ë™ì„ ì²´ê³„ì ìœ¼ë¡œ ë³´ì¸ ê²ƒì€ SimpleFoldê°€ ì²˜ìŒì´ë‹¤.

## Discussion

### ì €ìê°€ ë°íŒ í•œê³„ì™€ í–¥í›„ ë°©í–¥

- SimpleFoldëŠ” AlphaFold2 ìˆ˜ì¤€ì—ëŠ” ë¯¸ì¹˜ì§€ ëª»í•œë‹¤. MSA ê¸°ë°˜ ëª¨ë¸ê³¼ì˜ ê²©ì°¨ë¥¼ ì¤„ì´ëŠ” ê²ƒì´ í–¥í›„ ê³¼ì œ.
- í˜„ì¬ëŠ” single-chain foldingì— ì§‘ì¤‘í–ˆìœ¼ë©°, protein complex predictionìœ¼ë¡œì˜ í™•ì¥ì´ ìì—°ìŠ¤ëŸ¬ìš´ ë‹¤ìŒ ë‹¨ê³„.
- ë…¼ë¬¸ì—ì„œëŠ” ë” í° ëª¨ë¸(3B ì´ìƒ)ê³¼ ë” ë§ì€ í•™ìŠµ ë°ì´í„°ë¡œ ê³„ì† ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒìœ¼ë¡œ ê¸°ëŒ€í•˜ê³  ìˆë‹¤.

### ì¬í˜„ì„±

- **ì½”ë“œ ê³µê°œ**: âœ… [apple/ml-simplefold](https://github.com/apple/ml-simplefold) (Jupyter notebook í¬í•¨)
- **í•™ìŠµ ë°ì´í„°**: PDB (ê³µê°œ), AFDB/AFESM (ê³µê°œ) â€” ì ‘ê·¼ ê°€ëŠ¥
- **í•„ìš” GPU**: ë…¼ë¬¸ ë¯¸ëª…ì‹œ, 3B ëª¨ë¸ ê·œëª¨ë¡œ ì¶”ì • ì‹œ ëŒ€ê·œëª¨ GPU í´ëŸ¬ìŠ¤í„° í•„ìš”
- **ì¬í˜„ ë‚œì´ë„**: â­â­â­ (ì¤‘ê°„ â€” í•™ìŠµ ê·œëª¨ëŠ” í¬ì§€ë§Œ ì½”ë“œ ê³µê°œë¨)

> ì´ì „ì— ë¦¬ë·°í•œ [AlphaFold2](/posts/alphafold2-highly-accurate-protein-structure-prediction/)ì™€ ë¹„êµí•˜ë©´, SimpleFoldëŠ” ì•„í‚¤í…ì²˜ ë‹¨ìˆœí™”ì˜ ê·¹ë‹¨ì„ ë³´ì—¬ì¤€ë‹¤. AlphaFold2ì˜ Evoformerì—ì„œ triangle attentionê³¼ pair representationì´ í•µì‹¬ì´ì—ˆë‹¤ë©´, SimpleFoldëŠ” ì´ ëª¨ë“  ê²ƒì„ í‘œì¤€ Transformerë¡œ ëŒ€ì²´í–ˆë‹¤.
{: .prompt-info }

## TL;DR

- **SimpleFoldëŠ” ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆ(MSA, pair representation, triangle attention)ì„ ì „ë¶€ ì œê±°í•˜ê³ **, í‘œì¤€ Transformer + Flow Matchingë§Œìœ¼ë¡œ protein foldingì„ ìˆ˜í–‰í•˜ëŠ” ìµœì´ˆì˜ ëª¨ë¸ì´ë‹¤.
- 3B íŒŒë¼ë¯¸í„°ê¹Œì§€ ìŠ¤ì¼€ì¼í•˜ì—¬ PLM ê¸°ë°˜ ëª¨ë¸ ì¤‘ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ê³ , **ensemble predictionì—ì„œëŠ” MSA ê¸°ë°˜ ëª¨ë¸ë„ ëŠ¥ê°€**í•œë‹¤.
- Protein foldingì—ì„œì˜ **scaling law**ë¥¼ ì²˜ìŒìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ë³´ì—¬, ë²”ìš© ì•„í‚¤í…ì²˜ + ëŒ€ê·œëª¨ í•™ìŠµì´ ë„ë©”ì¸ ì§€ì‹ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ” ë°©í–¥ì„ ì œì‹œí–ˆë‹¤.

## Paper Info

| í•­ëª© | ë‚´ìš© |
|---|---|
| **Title** | SimpleFold: Folding Proteins is Simpler than You Think |
| **Authors** | Yuyang Wang, Jiarui Lu, Navdeep Jaitly, Josh Susskind, Miguel Angel Bautista (Apple) |
| **Venue** | ICLR 2026 (Poster) |
| **Paper** | [OpenReview](https://openreview.net/forum?id=0j0MmK7EMA) Â· [arXiv](https://arxiv.org/abs/2509.18480) |
| **Code** | ë¯¸ê³µê°œ |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
