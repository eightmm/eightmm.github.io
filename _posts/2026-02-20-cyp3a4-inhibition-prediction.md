---
title: "CYP3A4 Inhibition Prediction: Multi-Modal GNN with Molecular Fingerprints"
date: 2026-02-20 10:02:00 +0900
description: "GatedGCN-LSPEì™€ 9ì¢… molecular fingerprintë¥¼ cross-modal attentionìœ¼ë¡œ ê²°í•©í•˜ì—¬ CYP3A4 ì–µì œìœ¨ì„ ì˜ˆì¸¡í•˜ëŠ” multi-modal GNN ëª¨ë¸. Dacon ì‹ ì•½ê°œë°œ ê²½ì§„ëŒ€íšŒ Private 4ìœ„."
categories: [Projects, ADMET]
tags: [gnn, cyp3a4, molecular-fingerprint, drug-metabolism, multi-modal, dacon, admet]
math: true
mermaid: true
image:
  path: /assets/img/posts/cyp3a4-inhibition-prediction/cv_summary.png
  alt: "CYP3A4 Inhibition Prediction â€” 5-Fold Cross-Validation Training Curves"
---

## Hook

ì‹ ì•½ í›„ë³´ë¬¼ì§ˆ í•˜ë‚˜ê°€ CYP3A4ë¥¼ ê°•í•˜ê²Œ ì–µì œí•˜ë©´ ì–´ë–»ê²Œ ë ê¹Œ? ë³‘ìš© ì•½ë¬¼ì˜ í˜ˆì¤‘ ë†ë„ê°€ ì¹˜ì†Ÿê³ , ë…ì„±ì´ ì˜¬ë¼ê°€ê³ , ê²°êµ­ ì„ìƒì—ì„œ íƒˆë½í•œë‹¤. ì¸ì²´ ê°„ì—ì„œ ì „ì²´ ì˜ì•½í’ˆì˜ ì•½ 50%ë¥¼ ëŒ€ì‚¬í•˜ëŠ” ì´ íš¨ì†Œì˜ ì–µì œ ì •ë„ë¥¼ SMILES í•˜ë‚˜ë¡œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤ë©´ â€” ì‹ ì•½ ê°œë°œ ì´ˆê¸° ìŠ¤í¬ë¦¬ë‹ì´ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¬ë¼ì§„ë‹¤. ì´ í”„ë¡œì íŠ¸ëŠ” **ë¶„ì ê·¸ë˜í”„ì™€ 9ì¢…ì˜ molecular fingerprintë¥¼ cross-modal attentionìœ¼ë¡œ ìœµí•©**í•˜ì—¬ CYP3A4 Inhibition %ë¥¼ ì˜ˆì¸¡í•˜ê³ , Dacon ê²½ì§„ëŒ€íšŒì—ì„œ **Private 4ìœ„**ë¥¼ ë‹¬ì„±í•œ ëª¨ë¸ì´ë‹¤.

> Dacon [Boost up AI 2025: ì‹ ì•½ ê°œë°œ ê²½ì§„ëŒ€íšŒ](https://dacon.io/competitions/official/236518/overview/description) **Private 4ìœ„ (íŒ€ íŒ”ë¯¸ë¦¬)**  
> GitHub: [eightmm/CYP3A4](https://github.com/eightmm/CYP3A4)
{: .prompt-info }

## Problem

### CYP3A4 ì–µì œ ì˜ˆì¸¡ì˜ ì–´ë ¤ì›€

**CYP3A4**(Cytochrome P450 3A4)ëŠ” ì¸ì²´ ê°„ì—ì„œ ì „ì²´ ì˜ì•½í’ˆì˜ ì•½ 50%ë¥¼ ëŒ€ì‚¬í•˜ëŠ” í•µì‹¬ íš¨ì†Œë‹¤. ì‹ ì•½ í›„ë³´ë¬¼ì§ˆì´ CYP3A4ë¥¼ ê°•í•˜ê²Œ ì–µì œí•˜ë©´:

- **ì•½ë¬¼-ì•½ë¬¼ ìƒí˜¸ì‘ìš©(DDI)**: ë³‘ìš© ì•½ë¬¼ì˜ í˜ˆì¤‘ ë†ë„ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ìƒìŠ¹
- **ë…ì„± ìœ„í—˜ ì¦ê°€**: ëŒ€ì‚¬ë˜ì§€ ì•Šì€ ì•½ë¬¼ì´ ì²´ë‚´ ì¶•ì 
- **ì„ìƒ ì‹¤íŒ¨**: DDI ë¬¸ì œë¡œ ê°œë°œ ì¤‘ë‹¨

ë”°ë¼ì„œ ì‹ ì•½ ê°œë°œ ì´ˆê¸° ë‹¨ê³„ì—ì„œ CYP3A4 ì–µì œ ì •ë„ë¥¼ ë¹ ë¥´ê²Œ ìŠ¤í¬ë¦¬ë‹í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.

### ëŒ€íšŒ ì„¤ì •

| í•­ëª© | ë‚´ìš© |
|------|------|
| **í”Œë«í¼** | [Dacon](https://dacon.io/competitions/official/236518/overview/description) |
| **Task** | Regression â€” CYP3A4 Inhibition (%) ì˜ˆì¸¡ |
| **Input** | Canonical SMILES |
| **Target** | Inhibition ê°’ (range: 0.0 ~ 99.38) |
| **Train** | 1,681 samples |
| **Test** | 100 samples |

ì´ ëŒ€íšŒì—ì„œì˜ ë„ì „ì€ ë‹¨ìˆœí•œ regressionì´ ì•„ë‹ˆë‹¤.

**ë°ì´í„° íŠ¹ì„±**:
- **Tanimoto Similarity**: Train-Test ê°„ ìµœëŒ€ ìœ ì‚¬ë„ê°€ 0.6 ë¯¸ë§Œ â€” êµ¬ì¡°ì ìœ¼ë¡œ ë…ë¦½ì ì¸ ë¶„ìë“¤ì´ í¬í•¨
- **Murcko Scaffold Split**: 95.7%ê°€ singleton scaffolds â€” ë§¤ìš° ë†’ì€ êµ¬ì¡°ì  ë‹¤ì–‘ì„±
- ì´ëŠ” ëª¨ë¸ì´ ë‹¨ìˆœ ì•”ê¸°ê°€ ì•„ë‹Œ **ì¼ë°˜í™”ëœ ë¶„ì-ì–µì œ ê´€ê³„**ë¥¼ í•™ìŠµí•´ì•¼ í•¨ì„ ì˜ë¯¸

ì „í†µì ì¸ ì•½ë¬¼ ëŒ€ì‚¬ ì˜ˆì¸¡ ëª¨ë¸ì€ ë„ë©”ì¸ ì „ë¬¸ê°€ê°€ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì„¤ê³„í•œ molecular descriptorsì— ì˜ì¡´í•œë‹¤. CYP3A4 ê°™ì€ íŠ¹ì • íš¨ì†Œì— ëŒ€í•œ ê¹Šì€ ìƒí™”í•™ì  ì§€ì‹ì´ ì—†ìœ¼ë©´, ì–µì œ ë©”ì»¤ë‹ˆì¦˜ê³¼ ì§ì ‘ ì—°ê´€ëœ êµ¬ì¡°ì  íŒ¨í„´ì„ í¬ì°©í•˜ê¸° ì–´ë µë‹¤.

## Key Idea

### Dual-Stream + LLM Feature Engineering

ì´ ëª¨ë¸ì˜ í•µì‹¬ì€ ë‘ ê°€ì§€ë‹¤.

**ì²«ì§¸, ë¶„ìë¥¼ ë‘ ê°€ì§€ ê´€ì ì—ì„œ ë³¸ë‹¤.** í•˜ë‚˜ì˜ SMILESì—ì„œ molecular graph(ì›ì ê°„ ì—°ê²° êµ¬ì¡°)ì™€ 9ì¢…ì˜ molecular fingerprint(ì „ì—­ì  í™”í•™ ê¸°ìˆ ì)ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì¶”ì¶œí•œë‹¤. ì „ìëŠ” local substructureë¥¼ ì •ë°€í•˜ê²Œ ì¸ì½”ë”©í•˜ê³ , í›„ìëŠ” ì „ì—­ì  í™”í•™ì  ì„±ì§ˆì„ í¬ì°©í•œë‹¤. ë‘ streamì„ cross-modal attentionìœ¼ë¡œ í†µí•©í•˜ë©´, ì–´ëŠ í•œìª½ë§Œìœ¼ë¡œëŠ” ì¡ì„ ìˆ˜ ì—†ëŠ” ìƒë³´ì  ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ìˆë‹¤.

**ë‘˜ì§¸, LLMì„ "Virtual Biochemist"ë¡œ í™œìš©í•œë‹¤.** Gemini 2.5 Proì™€ Claude Sonnet 4ì„ ë°˜ë³µì ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ CYP3A4 inhibitor/substrateì˜ êµ¬ì¡°ì  íŠ¹ì§•ì„ SMARTS íŒ¨í„´ìœ¼ë¡œ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ molecular graphì˜ node featureë¡œ ì§ì ‘ ì£¼ì…í•œë‹¤. ë„ë©”ì¸ ì „ë¬¸ê°€ ì—†ì´ë„ ê³ í’ˆì§ˆì˜ íš¨ì†Œ íŠ¹í™” featureë¥¼ ìƒì„±í•˜ëŠ” ì ‘ê·¼ì´ë‹¤.

## How it works

### 4.1 Overview

ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ **ë‘ ê°œì˜ ë…ë¦½ì ì¸ feature extraction stream**ê³¼ **cross-modal fusion**ìœ¼ë¡œ êµ¬ì„±ëœë‹¤.

```mermaid
graph TD
    A["SMILES Input"] --> B["Stream 1: Molecular Graph"]
    A --> C["Stream 2: 9 Fingerprints"]
    
    B --> D["GraphEmbedding<br/>(Node 158D + Edge 44D + RWPE 20D)"]
    D --> E["GatedGCN-LSPE Ã— 8 layers<br/>+ AdaLN + ConditionalTransition"]
    E --> F["SumPooling"]
    F --> G["Graph Features (512D)"]
    
    C --> H["9Ã— MLP Encoders"]
    H --> I["8-head Self-Attention"]
    I --> J["Mean + Max Pooling<br/>â†’ Fusion Network"]
    J --> K["FP Features (512D)"]
    
    G --> L["Cross-Modal Attention<br/>(8-head MHA)"]
    K --> L
    L --> M["Fusion Gate<br/>Î± Â· h_fused + (1-Î±) Â· h_graph"]
    M --> N["Regression Head (MLP)"]
    N --> O["Inhibition %"]

    style A fill:#e1f5fe
    style O fill:#e8f5e9
    style E fill:#fff3e0
    style I fill:#fff3e0
    style L fill:#fce4ec
```

![5-Fold CV Training Curves](/assets/img/posts/cyp3a4-inhibition-prediction/cv_summary.png)
_Figure 1: 5-Fold Cross-Validation í•™ìŠµ ê³¡ì„ . ê° foldì˜ training/validation lossì™€ competition score ì¶”ì´. ì¶œì²˜: eightmm/CYP3A4_

<details markdown="1">
<summary>ğŸ“ Overall Architecture Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class PropertyPredictor(nn.Module):
    """CYP3A4 Inhibition Prediction â€” Dual-Stream Architecture"""
    def __init__(self, node_dim=158, edge_dim=44, hidden_dim=512, num_layers=8):
        # Stream 1: Graph
        self.graph_extractor = GraphFeatureExtractor(
            node_dim, edge_dim, hidden_dim, num_layers
        )
        # Stream 2: Fingerprints
        self.molecular_extractor = MolecularFeatureExtractor(hidden_dim)
        # Cross-modal fusion
        self.cross_modal_attention = MultiheadAttention(hidden_dim, num_heads=8)
        self.fusion_gate = Sequential(Linear(hidden_dim * 2, hidden_dim), Sigmoid())
        self.regression_head = MLP(hidden_dim â†’ 1)

    def forward(self, graph, fingerprints):
        h_graph = self.graph_extractor(graph)           # (B, 512)
        h_fp = self.molecular_extractor(fingerprints)   # (B, 512)
        stacked = stack([h_graph, h_fp], dim=1)         # (B, 2, 512)
        attended = self.cross_modal_attention(stacked)   # (B, 2, 512)
        gate = self.fusion_gate(concat(attended))        # (B, 512)
        h_final = gate * fused + (1 - gate) * h_graph   # gated residual
        return self.regression_head(h_final)             # (B, 1)
```

</details>

### 4.2 Representation

**Node features (158D)**: ê¸°ë³¸ ì›ì ì†ì„±(atomic number, period, group, ì „ê¸°ìŒì„±ë„, degree, valence) + í™”í•™ì  ì†ì„±(í˜¼ì„± ê¶¤ë„, formal charge, ë°©í–¥ì¡±ì„±, í‚¤ë„ì„±) + **Random Walk Positional Encoding (RWPE, 20D)** + **CYP3A4-specific SMARTS features (29D)**

**Edge features (44D)**: ê²°í•© ìœ í˜•, ì…ì²´í™”í•™, ê³µì•¡/ê³ ë¦¬ ì—¬ë¶€, topological distance

**9ì¢… Fingerprints**:

| Fingerprint | Dimension | íŠ¹ì„± |
|-------------|-----------|------|
| Descriptor | 27D | ë¬¼ë¦¬í™”í•™ì  ì„±ì§ˆ (MW, LogP, TPSA ë“±) |
| MACCS | 167D | êµ¬ì¡° í‚¤ (ì‚¬ì „ ì •ì˜ëœ substructure íŒ¨í„´) |
| Morgan | 2048D | Circular fingerprint (ì›í˜• í™˜ê²½ ì¸ì½”ë”©) |
| Morgan Count | 2048D | ë¹ˆë„ ê¸°ë°˜ Morgan variant |
| Feature Morgan | 2048D | Feature ê¸°ë°˜ Morgan variant |
| RDKit | 2048D | Topological fingerprint |
| Atom Pair | 2048D | ì›ììŒ ê¸°ìˆ ì |
| Topological Torsion | 2048D | Torsion ê¸°ë°˜ êµ¬ì¡° ì •ë³´ |
| Pharmacophore2D | 1024D | 2D ì•½ë¦¬ì‘ìš©ë‹¨ íŒ¨í„´ |

ê° fingerprint íƒ€ì…ì€ ë¶„ìì˜ ì„œë¡œ ë‹¤ë¥¸ í™”í•™ì  ì¸¡ë©´ì„ í¬ì°©í•˜ë©°, attentionì„ í†µí•´ ì˜ˆì¸¡ì— ìœ ìš©í•œ í‘œí˜„ì— ë™ì ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤.

### 4.3 Core Architecture

#### Stream 1: GatedGCN-LSPE + AlphaFold3-Inspired Layers

Graph streamì€ **8ê°œì˜ GatedGCN-LSPE ë ˆì´ì–´**ë¡œ ë¶„ì ê·¸ë˜í”„ë¥¼ ì²˜ë¦¬í•œë‹¤. ê° ë ˆì´ì–´ëŠ” gated message passingìœ¼ë¡œ ì´ì›ƒ ì›ìì˜ ê¸°ì—¬ë„ë¥¼ ì ì‘ì ìœ¼ë¡œ ì¡°ì ˆí•˜ê³ , positional encodingì„ í•¨ê»˜ ì—…ë°ì´íŠ¸í•œë‹¤.

í•µì‹¬ ìˆ˜ì‹ â€” edge gating:

$$\hat{\eta}_{ij} = B_1 h_i + B_2 h_j + B_3 e_{ij}$$

$$\eta_{ij} = \frac{\sigma(\hat{\eta}_{ij})}{\sum_{k \in \mathcal{N}(j)} \sigma(\hat{\eta}_{kj})}$$

$\sigma$ëŠ” sigmoid functionì´ë‹¤. Edgeì˜ ì¤‘ìš”ë„ë¥¼ ì›ì ìŒ + edge feature ì •ë³´ë¡œ ê³„ì‚°í•˜ê³ , ì´ì›ƒ ì „ì²´ì— ëŒ€í•´ normalizeí•˜ì—¬ soft attention weightë¡œ ì‚¬ìš©í•œë‹¤.

ê° GatedGCN ë ˆì´ì–´ ë’¤ì—ëŠ” **AlphaFold3ì—ì„œ ì‚¬ìš©ëœ ë‘ ê°€ì§€ ì»´í¬ë„ŒíŠ¸**ê°€ ì¶”ê°€ëœë‹¤:

- **Adaptive Layer Normalization (AdaLN)**: $\sigma(W_1 \cdot \text{LN}(c)) \odot \text{LN}(x) + W_2 \cdot \text{LN}(c)$ â€” context-dependent normalizationìœ¼ë¡œ ë¶„ìë§ˆë‹¤ ë‹¤ë¥¸ ì •ê·œí™” ì ìš©
- **Conditional Transition Block**: SwiGLU activation ê¸°ë°˜ gating â€” $\sigma(W_c \cdot c) \odot W_o(\text{SiLU}(W_1 a) \odot W_2 a)$

ì´ˆê¸° embeddingì„ contextë¡œ ìœ ì§€í•˜ë©´ì„œ ê° ë ˆì´ì–´ì˜ ì¶œë ¥ì„ conditioningí•˜ëŠ” êµ¬ì¡°ë‹¤.

<details markdown="1">
<summary>ğŸ“ GatedGCN-LSPE Layer êµ¬í˜„ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class GatedGCNLSPELayer(nn.Module):
    """GatedGCN with Learnable Spectral Positional Encoding"""
    def __init__(self, input_dim: int, output_dim: int, dropout: float, batch_norm: bool):
        super().__init__()
        # Node: h_new = A1[h||p] + Î£ Î·_ij Â· A2[h_i||p_i]
        self.A1 = nn.Linear(input_dim * 2, output_dim)   # self-loop
        self.A2 = nn.Linear(input_dim * 2, output_dim)   # neighbor
        # Gating: Î·Ì‚_ij = B1Â·h_src + B2Â·h_dst + B3Â·e_ij
        self.B1 = nn.Linear(input_dim, output_dim)
        self.B2 = nn.Linear(input_dim, output_dim)
        self.B3 = nn.Linear(input_dim, output_dim)
        # Positional encoding: p_new = C1Â·p_j + Î£ Î·_ij Â· C2Â·p_i
        self.C1 = nn.Linear(input_dim, output_dim)
        self.C2 = nn.Linear(input_dim, output_dim)
        self.bn_node_h = nn.BatchNorm1d(output_dim)
        self.bn_node_e = nn.BatchNorm1d(output_dim)

    def forward(self, g, h, p, e):
        h_in, p_in, e_in = h, p, e  # for residual

        # Edge gating: Î·Ì‚_ij = B1Â·h_src + B2Â·h_dst + B3Â·e_ij
        hat_eta = g.edata['B1_B2_h'] + self.B3(e)           # (E, D)
        sigma = torch.sigmoid(hat_eta)
        eta = sigma / (sum_of_neighbors(sigma) + 1e-6)      # normalized

        # Node update: A1[h||p] + Î£ Î·_ij Â· A2[h_neighbor||p_neighbor]
        h = self.A1(cat([h, p], -1)) + aggregate(eta, self.A2(cat([h_nb, p_nb], -1)))
        p = self.C1(p) + aggregate(eta, self.C2(p_neighbor)) # PE update

        h = F.relu(self.bn_node_h(h)) + h_in                # BN â†’ ReLU â†’ residual
        p = torch.tanh(p) + p_in
        e = F.relu(self.bn_node_e(hat_eta)) + e_in
        return F.dropout(h), F.dropout(p), F.dropout(e)
```

</details>

<details markdown="1">
<summary>ğŸ“ AdaLN + ConditionalTransition êµ¬í˜„ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class AdaLN(nn.Module):
    """Adaptive Layer Normalization (AlphaFold3-style)
    output = sigmoid(W1 Â· LN(context)) * LN(feat) + W2 Â· LN(context)
    """
    def __init__(self, dim: int):
        super().__init__()
        self.ln_feat = nn.LayerNorm(dim)
        self.ln_ctx = nn.LayerNorm(dim)
        self.gate = nn.Linear(dim, dim)          # sigmoid gate
        self.bias = nn.Linear(dim, dim, bias=False)  # additive bias

    def forward(self, feat, context):
        # feat, context: (N, D)
        return torch.sigmoid(self.gate(self.ln_ctx(context))) * self.ln_feat(feat) \
               + self.bias(self.ln_ctx(context))


class ConditionalTransitionBlock(nn.Module):
    """SwiGLU-gated transition conditioned on initial embeddings
    output = sigmoid(W_ctx Â· context) * W_out(SiLU(W1 Â· a) âŠ™ W2 Â· a)
    """
    def __init__(self, dim: int, expansion: int = 4):
        super().__init__()
        self.ada_ln = AdaLN(dim)
        self.w1 = nn.Linear(dim, dim * expansion, bias=False)   # SiLU branch
        self.w2 = nn.Linear(dim, dim * expansion, bias=False)   # gate branch
        self.w_out = nn.Linear(dim * expansion, dim, bias=False)
        self.w_ctx = nn.Linear(dim, dim)                         # output gate

    def forward(self, feat, context):
        a = self.ada_ln(feat, context)                    # (N, D)
        b = F.silu(self.w1(a)) * self.w2(a)              # SwiGLU: (N, D*4)
        return torch.sigmoid(self.w_ctx(context)) * self.w_out(b)  # (N, D)
```

</details>

<details markdown="1">
<summary>ğŸ“ GraphFeatureExtractor ì „ì²´ Forward (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class GraphFeatureExtractor(nn.Module):
    """Stream 1: SMILES â†’ Graph â†’ GatedGCN-LSPE(8L) â†’ 512D"""
    def __init__(self, node_dim=158, edge_dim=44, hidden_dim=512, num_layers=8):
        self.embedding = GraphEmbedding(node_dim, edge_dim, hidden_dim)
        self.node_adaln = AdaLN(hidden_dim)
        self.edge_adaln = AdaLN(hidden_dim)
        self.gnn_layers = ModuleList([GatedGCNLSPELayer(hidden_dim, hidden_dim) for _ in range(8)])
        self.node_transitions = ModuleList([ConditionalTransitionBlock(hidden_dim) for _ in range(8)])
        self.edge_transitions = ModuleList([ConditionalTransitionBlock(hidden_dim) for _ in range(8)])
        self.pool = SumPooling()

    def forward(self, graph):
        h, p, e = self.embedding(graph.ndata['feat'], graph.ndata['rwpe'], graph.edata['feat'])
        h = self.node_adaln(h, h)        # initial self-conditioning
        e = self.edge_adaln(e, e)
        h_raw, e_raw = h, e              # save as context for transitions

        for gnn, n_trans, e_trans in zip(self.gnn_layers, self.node_transitions, self.edge_transitions):
            h, p, e = gnn(graph, h, p, e)    # GatedGCN-LSPE
            h = n_trans(h, h_raw)             # condition on initial embedding
            e = e_trans(e, e_raw)

        return self.pool(graph, h)            # (B, 512)
```

</details>

#### Stream 2: Molecular Fingerprint Extractor

9ì¢…ì˜ fingerprint ê°ê°ì„ ë…ë¦½ì ì¸ MLP encoder(Linear â†’ ReLU â†’ Dropout â†’ Linear)ë¡œ 512Dë¡œ ì‚¬ì˜í•œ ë’¤, **8-head Multi-Head Self-Attention**ìœ¼ë¡œ fingerprint ê°„ ìƒí˜¸ì‘ìš©ì„ í•™ìŠµí•œë‹¤. Attentionì„ í†µí•´ í˜„ì¬ ë¶„ìì˜ ì˜ˆì¸¡ì— ìœ ìš©í•œ fingerprint íƒ€ì…ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œë‹¤.

Attention í›„ mean pooling + max poolingì„ concatenateí•˜ì—¬ fusion network(LayerNorm â†’ MLP)ë¡œ í†µí•©í•œë‹¤.

<details markdown="1">
<summary>ğŸ“ MolecularFeatureExtractor êµ¬í˜„ (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class MolecularFeatureExtractor(nn.Module):
    """Stream 2: SMILES â†’ 9 Fingerprints â†’ Attention â†’ 512D"""
    def __init__(self, hidden_dim=512, dropout=0.15):
        # 9ì¢… fingerprint â†’ ê°ê° MLP encoder
        self.fp_encoders = ModuleDict({
            'descriptor': MLP(27 â†’ hidden_dim),
            'maccs': MLP(167 â†’ hidden_dim),
            'morgan': MLP(2048 â†’ hidden_dim),
            'morgan_count': MLP(2048 â†’ hidden_dim),
            'feature_morgan': MLP(2048 â†’ hidden_dim),
            'rdkit_fp': MLP(2048 â†’ hidden_dim),
            'atom_pair': MLP(2048 â†’ hidden_dim),
            'topo_torsion': MLP(2048 â†’ hidden_dim),
            'pharmacophore': MLP(1024 â†’ hidden_dim),
        })
        self.attention = MultiheadAttention(hidden_dim, num_heads=8)
        self.fusion = Sequential(LayerNorm(hidden_dim*2), MLP(hidden_dim*2 â†’ hidden_dim))

    def forward(self, fingerprints: dict):
        encoded = [self.fp_encoders[k](v) for k, v in fingerprints.items()]
        stacked = torch.stack(encoded, dim=1)               # (B, 9, 512)
        attended, _ = self.attention(stacked, stacked, stacked)
        mean_pool = attended.mean(dim=1)                     # (B, 512)
        max_pool = attended.max(dim=1).values                # (B, 512)
        return self.fusion(cat([mean_pool, max_pool], -1))   # (B, 512)
```

</details>

### 4.4 Key Innovation

#### LLM-Based CYP3A4 Feature Engineering

**Gemini 2.5 Pro**ì™€ **Claude Sonnet 4**ì„ ë°˜ë³µì ìœ¼ë¡œ í™œìš©í•˜ì—¬ CYP3A4 inhibitor/substrateì˜ êµ¬ì¡°ì  íŠ¹ì§•ì„ SMARTS íŒ¨í„´ìœ¼ë¡œ ìë™ ìƒì„±í–ˆë‹¤.

```mermaid
graph TD
    A["Step 1: Gemini 2.5 Pro<br/>CYP3A4 substrate/inhibitor<br/>êµ¬ì¡°ì  íŠ¹ì§• íƒìƒ‰"] --> B["ì´ˆê¸° SMARTS íŒ¨í„´ í›„ë³´êµ°"]
    B --> C["Step 2: Claude Sonnet 4<br/>í™”í•™ì  ìœ íš¨ì„± ê²€ì¦<br/>ì¤‘ë³µ/ê³¼ë„í•œ ì¼ë°˜í™” ì œê±°"]
    C --> D["ì •ì œëœ íŒ¨í„´"]
    D --> E["Step 3: Gemini 2.5 Pro<br/>Substructure variants ìƒì„±"]
    E --> F["ìµœì¢… 29ì¢… SMARTS íŒ¨í„´"]

    style A fill:#e1f5fe
    style C fill:#fce4ec
    style F fill:#e8f5e9
```

ìµœì¢… ê²°ê³¼:
- **Inhibitor íŒ¨í„´ (13ì¢…)**: azole í•­ì§„ê· ì œ(imidazole, triazole), macrolide, HIV protease inhibitor êµ¬ì¡° ë“±
- **Substrate íŒ¨í„´ (16ì¢…)**: N-dealkylation, O-dealkylation, hydroxylation site ë“±

ì´ íŒ¨í„´ë“¤ì€ ë¶„ì ê·¸ë˜í”„ì˜ **node featuresë¡œ ì§ì ‘ ì£¼ì…** â€” ê° ì›ìê°€ CYP3A4 ì–µì œ/ê¸°ì§ˆ íŒ¨í„´ì— í•´ë‹¹í•˜ëŠ”ì§€ë¥¼ binary feature(29D)ë¡œ í‘œí˜„í•œë‹¤. ë¹„ì „ë¬¸ê°€ë„ LLMì„ í™œìš©í•´ ë„ë©”ì¸ íŠ¹í™” featureë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì—ì„œ, í–¥í›„ AI-driven drug discoveryì— ì ìš© ê°€ëŠ¥í•œ ì ‘ê·¼ì´ë‹¤.

### 4.5 Training & Inference

#### Loss Function

ëŒ€íšŒ í‰ê°€ ì§€í‘œë¥¼ ì§ì ‘ lossë¡œ ì‚¬ìš©í•˜ì—¬, í•™ìŠµ ëª©í‘œì™€ í‰ê°€ ëª©í‘œë¥¼ ì¼ì¹˜ì‹œí‚¨ë‹¤:

$$\mathcal{L} = -\left[0.5 \times \left(1 - \min\left(\frac{\text{RMSE}}{\max(y) - \min(y)},\ 1\right)\right) + 0.5 \times r_{Pearson}\right]$$

Normalized RMSEì™€ Pearson Correlationì˜ ê°€ì¤‘í•©ì„ ì§ì ‘ ìµœì í™”í•œë‹¤. ì¶”ê°€ë¡œ ì˜ˆì¸¡ê°’ì´ ìœ íš¨ ë²”ìœ„([0, 99.38])ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° range penaltyë¥¼ ì ìš©í•œë‹¤.

#### Training Configuration

| í•­ëª© | ê°’ |
|------|-----|
| Hidden Dimension | 512 |
| GNN Layers | 8 |
| MHA Heads | 8 |
| Optimizer | AdamW (lr=1e-5, weight_decay=5e-5) |
| Scheduler | CosineAnnealingWarmUpRestarts (T_0=20, T_up=5) |
| Batch Size | 32 |
| Epochs | 500 (Early Stopping patience=30) |
| Gradient Clipping | max_norm=1.0 |
| Dropout | 0.2 (GNN) / 0.15 (MLP) |
| Validation | 5-Fold CV |

<details markdown="1">
<summary>ğŸ“ Training Loop (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# 5-Fold Cross-Validation Training
for fold in range(5):
    model = PropertyPredictor(node_dim=158, edge_dim=44, hidden_dim=512, num_layers=8)
    optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=5e-5)
    scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=20, T_up=5)

    for epoch in range(500):
        model.train()
        for graph_batch, fp_batch, target in train_loader:
            pred = model(graph_batch, fp_batch).squeeze(-1)  # (B,)

            # Competition-aligned loss
            rmse = torch.sqrt(F.mse_loss(pred, target))
            nrmse = rmse / (y_max - y_min)                   # normalized by range
            pearson = pearson_corrcoef(pred, target)
            loss = -(0.5 * (1 - clamp(nrmse, max=1.0)) + 0.5 * pearson)

            # Range penalty for out-of-bound predictions
            penalty = mean(relu(pred - y_max) + relu(y_min - pred))
            loss = loss + 0.1 * penalty

            loss.backward()
            clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

        # Early stopping on validation score
        if no_improvement_for(patience=30):
            break
```

</details>

<details markdown="1">
<summary>ğŸ“ Inference (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
# Ensemble inference: average predictions from 5 fold models
fold_models = [load_model(f"fold_{i}/best.pt") for i in range(5)]

predictions = []
for model in fold_models:
    model.eval()
    with torch.no_grad():
        pred = model(test_graph, test_fingerprints)  # (N_test, 1)
        predictions.append(pred)

# Final prediction: mean of 5 folds
final = torch.stack(predictions).mean(dim=0)  # (N_test, 1)
```

</details>

## Results

### ìµœì¢… ìˆœìœ„

| Private Ranking | Public Score | Private Score | Average Score |
|----------------|--------------|---------------|---------------|
| 1 | 0.81950 | 0.79153 | 0.80551 |
| 2 | 0.72054 | 0.74504 | 0.73279 |
| 3 | 0.76853 | 0.74396 | 0.75624 |
| **4 (íŒ”ë¯¸ë¦¬)** | **0.79511** | **0.73914** | **0.76711** |
| 5 | <0.7800 | 0.73894 | 0.75947 |

Public â†’ Privateë¡œ ê°ˆ ë•Œ ìƒìœ„ íŒ€ë“¤ì˜ ì ìˆ˜ê°€ ì „ë°˜ì ìœ¼ë¡œ í•˜ë½í–ˆë‹¤. íŒ”ë¯¸ë¦¬ íŒ€ì€ Public 2ìœ„ê¶Œì—ì„œ Private 4ìœ„ë¡œ ì´ë™í–ˆìœ¼ë©°, 1ìœ„ íŒ€ì€ ì–‘ìª½ ëª¨ë‘ ì•ˆì •ì ì¸ ì„±ì ì„ ìœ ì§€í–ˆë‹¤.

### Seed Retraining

ëŒ€íšŒ ì¢…ë£Œ í›„, Seedë§Œ 42 â†’ 123ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ retrainingí•œ ê²°ê³¼:
- **Private Score**: 0.73914 â†’ **0.75067** (+0.01153)

Seed sensitivityê°€ ì¡´ì¬í•˜ë©°, ì—¬ëŸ¬ seedì˜ ensembleë¡œ ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥ì„±ì´ ìˆë‹¤. ì´ëŠ” 1,681 samplesì´ë¼ëŠ” ì‘ì€ ë°ì´í„°ì…‹ì—ì„œì˜ varianceê°€ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŒì„ ì‹œì‚¬í•œë‹¤.

## Discussion

### í•œê³„ì™€ ê°œì„  ë°©í–¥

- **Small dataset**: 1,681 samplesë¡œëŠ” deep GNNì˜ ì „ì²´ ìš©ëŸ‰ì„ í™œìš©í•˜ê¸° ì–´ë µë‹¤. External data(ChEMBL ë“±)ì˜ ì‹¤í—˜ ì¡°ê±´ ì°¨ì´ê°€ ì—†ì—ˆë‹¤ë©´ pre-trainingì— í™œìš©í•  ìˆ˜ ìˆì—ˆì„ ê²ƒì´ë‹¤.
- **Seed sensitivity**: ë‹¨ì¼ seed ê²°ê³¼ì˜ varianceê°€ í¬ë‹¤. Multi-seed ensembleì´ í•„ìˆ˜ì ì´ë‚˜, ëŒ€íšŒ ê¸°ê°„ ë‚´ ì‹œê°„ ì œì•½ì´ ìˆì—ˆë‹¤.
- **Public-Private gap**: Publicì—ì„œ 2ìœ„ê¶Œì´ì—ˆìœ¼ë‚˜ Privateì—ì„œ 4ìœ„ë¡œ í•˜ë½í•œ ê²ƒì€, 100ê°œ test sampleì˜ ë¶„í¬ ì°¨ì´ì— ëª¨ë¸ì´ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í–ˆì„ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•œë‹¤.
- **LLM featureì˜ í•´ì„**: LLMì´ ìƒì„±í•œ 29ì¢… SMARTS íŒ¨í„´ì´ ì‹¤ì œ ì–´ë–¤ í™”í•™ì  ë©”ì»¤ë‹ˆì¦˜ì„ í¬ì°©í•˜ëŠ”ì§€ì— ëŒ€í•œ ì²´ê³„ì ì¸ ablationì€ ì•„ì§ ë¶€ì¡±í•˜ë‹¤.

> **ì™¸ë¶€ ë°ì´í„°**: ChEMBL ë“± ê³µê°œ ë°ì´í„°ë² ì´ìŠ¤ì— CYP3A4 ê´€ë ¨ ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ, ì‹¤í—˜ ì¡°ê±´(assay type, concentration, cell line)ì´ ëŒ€íšŒ ë°ì´í„°ì™€ ì •í™•íˆ ë™ì¼í•˜ì§€ ì•Šì•„ ì§ì ‘ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤.
{: .prompt-info }

## TL;DR

- **ë¶„ì ê·¸ë˜í”„(GatedGCN-LSPE 8L) + 9ì¢… fingerprintë¥¼ cross-modal attentionìœ¼ë¡œ ìœµí•©**í•˜ëŠ” dual-stream ì•„í‚¤í…ì²˜ë¡œ CYP3A4 ì–µì œìœ¨ ì˜ˆì¸¡
- **LLM(Gemini + Sonnet)ì„ Virtual Biochemistë¡œ í™œìš©**í•˜ì—¬ CYP3A4 íŠ¹í™” SMARTS íŒ¨í„´ 29ì¢…ì„ ìë™ ìƒì„±, node featureë¡œ ì§ì ‘ ì£¼ì…
- Dacon ì‹ ì•½ê°œë°œ ê²½ì§„ëŒ€íšŒ **Private 4ìœ„** (Score: 0.73914), seed retrainingìœ¼ë¡œ 0.75067ê¹Œì§€ í–¥ìƒ ê°€ëŠ¥

## Project Info

| í•­ëª© | ë‚´ìš© |
|---|---|
| **Project** | CYP3A4 Inhibition Prediction |
| **Team** | íŒ”ë¯¸ë¦¬ |
| **Competition** | Dacon Boost up AI 2025: ì‹ ì•½ ê°œë°œ ê²½ì§„ëŒ€íšŒ |
| **Result** | Private 4ìœ„ (Score: 0.73914) |
| **Code** | [GitHub](https://github.com/eightmm/CYP3A4) |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
