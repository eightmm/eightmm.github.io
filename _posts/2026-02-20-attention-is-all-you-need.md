---
title: "Attention Is All You Need: Transformerì˜ íƒ„ìƒ"
date: 2026-02-20 15:00:00 +0900
categories: [AI, Fundamentals]
tags: [transformer, attention, self-attention, NLP, deep-learning, foundational]
math: true
mermaid: true
---

## RNNê³¼ CNNì„ ë²„ë¦¬ê³  Attentionë§Œìœ¼ë¡œ

2017ë…„, Google BrainíŒ€ì´ ë°œí‘œí•œ "Attention Is All You Need"ëŠ” í˜„ëŒ€ AIì˜ íŒë„ë¥¼ ë°”ê¾¼ ë…¼ë¬¸ì´ë‹¤. 

**í•µì‹¬ ì£¼ì¥:**
- Recurrence (RNN) í•„ìš” ì—†ë‹¤
- Convolution (CNN) í•„ìš” ì—†ë‹¤
- **Attention mechanismë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤**

ì´ ë…¼ë¬¸ì´ ì œì•ˆí•œ **Transformer** ì•„í‚¤í…ì²˜ëŠ”:
- Machine translationì—ì„œ state-of-the-art ë‹¬ì„±
- Training ì‹œê°„ íšê¸°ì  ë‹¨ì¶• (8 GPUs, 12ì‹œê°„)
- ì´í›„ BERT, GPT, LLM ì‹œëŒ€ì˜ foundationì´ ë¨

> ğŸ“„ [Paper (arXiv)](https://arxiv.org/abs/1706.03762) | Google Brain, Google Research, University of Toronto | NIPS 2017

---

## ë‹¹ì‹œ ë°°ê²½: RNNì˜ ì§€ë°°ì™€ í•œê³„

### Sequence Transductionì˜ í‘œì¤€: RNN

2017ë…„ ë‹¹ì‹œ, sequence-to-sequence ëª¨ë¸ì˜ í‘œì¤€ì€:

**Encoder-Decoder + Attention**

```mermaid
graph LR
    X["Input<br/>Sequence"] --> ENC["RNN<br/>Encoder"]
    ENC --> H["Hidden<br/>States"]
    H --> ATT["Attention<br/>Mechanism"]
    ATT --> DEC["RNN<br/>Decoder"]
    DEC --> Y["Output<br/>Sequence"]
    
    style ENC fill:#e1f5fe
    style DEC fill:#e1f5fe
    style ATT fill:#fff3e0
```

**ëŒ€í‘œì  ëª¨ë¸:**
- **LSTM/GRU:** Long short-term memory, Gated recurrent units
- **Seq2Seq with Attention:** Bahdanau et al. (2015)

### RNNì˜ ê·¼ë³¸ì  í•œê³„

**1. Sequential Computation**

RNNì€ position $t$ì˜ hidden state $h_t$ë¥¼ ê³„ì‚°í•˜ë ¤ë©´:

$$
h_t = f(h_{t-1}, x_t)
$$

- $h_{t-1}$ì´ ë¨¼ì € ê³„ì‚°ë˜ì–´ì•¼ í•¨
- **Parallelization ë¶ˆê°€ëŠ¥**
- Long sequenceì—ì„œ **memory constraint** (batch size ì œí•œ)

**2. Long-Range Dependencies**

- Position 1ê³¼ position 100 ì‚¬ì´ì˜ dependencyë¥¼ í•™ìŠµí•˜ë ¤ë©´ 100 steps ê±°ì³ì•¼ í•¨
- **Gradient vanishing/exploding** ë¬¸ì œ
- LSTM/GRUê°€ ì™„í™”í–ˆì§€ë§Œ ê·¼ë³¸ì  í•´ê²°ì€ ëª»í•¨

**3. Computational Complexity**

Sequence length $n$, representation dimension $d$ì— ëŒ€í•´:

| Layer Type | Operations | Sequential |
|---|---|---|
| **Recurrent** | $O(n \cdot d^2)$ | $O(n)$ |
| **Convolutional** | $O(k \cdot n \cdot d^2)$ | $O(1)$ |
| **Self-Attention** | $O(n^2 \cdot d)$ | $O(1)$ |

---

## í•µì‹¬ ì•„ì´ë””ì–´: Self-Attentionìœ¼ë¡œ ëª¨ë“  ê²ƒì„

### Self-Attentionì´ë€?

**ì •ì˜:** í•˜ë‚˜ì˜ sequence ë‚´ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ positionë“¤ ê°„ì˜ relationì„ ê³„ì‚°í•˜ì—¬ sequence representationì„ ë§Œë“œëŠ” ë©”ì»¤ë‹ˆì¦˜.

**ì§ê´€:**
- "The animal didn't cross the street because **it** was too tired."
- "it"ì´ "animal"ì„ ê°€ë¦¬í‚¤ëŠ”ì§€ "street"ë¥¼ ê°€ë¦¬í‚¤ëŠ”ì§€ â†’ **attention**ìœ¼ë¡œ í•™ìŠµ

**ê¸°ì¡´ attention:**
- Decoderê°€ encoderì˜ ëª¨ë“  positionì„ attend
- QueryëŠ” decoder, Key/ValueëŠ” encoder

**Self-attention:**
- Query, Key, Valueê°€ **ëª¨ë‘ ê°™ì€ sequence**ì—ì„œ ì˜´
- Encoder/Decoder ê°ê° ë‚´ë¶€ì—ì„œ self-attention ìˆ˜í–‰

### Transformerì˜ ëŒ€ë‹´í•œ ì„ íƒ

**"Recurrenceì™€ convolutionì„ ì™„ì „íˆ ì œê±°í•˜ê³  attentionë§Œ ì‚¬ìš©í•˜ì"**

**ì¥ì :**
1. **Parallelization:** ëª¨ë“  positionì„ ë™ì‹œì— ê³„ì‚° ê°€ëŠ¥
2. **Constant path length:** ëª¨ë“  position pairê°€ $O(1)$ stepìœ¼ë¡œ ì—°ê²°
3. **Interpretability:** Attention weightë¥¼ ì‹œê°í™”í•˜ì—¬ modelì´ ë¬´ì—‡ì„ ë³´ëŠ”ì§€ íŒŒì•… ê°€ëŠ¥

---

## Transformer Architecture

### Overall Structure

```mermaid
graph TB
    INPUT["Input<br/>Embedding"] --> POSENC["+ Positional<br/>Encoding"]
    POSENC --> ENC1["Encoder Layer 1"]
    ENC1 --> ENC2["..."]
    ENC2 --> ENC6["Encoder Layer 6"]
    
    OUTPUT["Output<br/>Embedding"] --> POSENC2["+ Positional<br/>Encoding"]
    POSENC2 --> DEC1["Decoder Layer 1"]
    DEC1 --> DEC2["..."]
    DEC2 --> DEC6["Decoder Layer 6"]
    
    ENC6 -.->|Keys, Values| DEC6
    
    DEC6 --> LINEAR["Linear"]
    LINEAR --> SOFT["Softmax"]
    SOFT --> PROB["Output<br/>Probabilities"]
    
    style ENC6 fill:#e1f5fe
    style DEC6 fill:#ffe0b2
    style SOFT fill:#c8e6c9
```

**Encoder-Decoder êµ¬ì¡°:**
- **Encoder:** $N=6$ layers (input sequence â†’ continuous representations)
- **Decoder:** $N=6$ layers (representations â†’ output sequence, auto-regressive)

### Encoder Layer

ê° encoder layerëŠ” **2ê°œ sub-layer**:

1. **Multi-Head Self-Attention**
2. **Position-wise Feed-Forward Network**

ê° sub-layer í›„:

$$
\text{LayerNorm}(x + \text{Sublayer}(x))
$$

- **Residual connection** + **Layer normalization**

### Decoder Layer

ê° decoder layerëŠ” **3ê°œ sub-layer**:

1. **Masked Multi-Head Self-Attention** (prevent looking ahead)
2. **Multi-Head Attention** over encoder output (encoder-decoder attention)
3. **Position-wise Feed-Forward Network**

ì—­ì‹œ ê° sub-layer í›„ residual connection + layer normalization.

---

## Scaled Dot-Product Attention

### ìˆ˜ì‹

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**êµ¬ì„± ìš”ì†Œ:**

- **Query $Q$:** "ë¬´ì—‡ì„ ì°¾ê³  ìˆëŠ”ê°€"
- **Key $K$:** "ë‚˜ëŠ” ë¬´ì—‡ì— ê´€í•œ ê²ƒì¸ê°€"
- **Value $V$:** "ë‚´ê°€ ì „ë‹¬í•  ì •ë³´"
- **Scaling factor $1/\sqrt{d_k}$:** Dot product magnitudeë¥¼ normalize

### ì™œ Scalingì´ í•„ìš”í•œê°€?

**ë¬¸ì œ:** $d_k$ê°€ í¬ë©´ dot product $q \cdot k$ì˜ magnitudeê°€ ì»¤ì§„ë‹¤.

**ê°€ì •:** $q$ì™€ $k$ì˜ ê° componentê°€ independent, mean 0, variance 1ì´ë©´:

$$
q \cdot k = \sum_{i=1}^{d_k} q_i k_i \sim N(0, d_k)
$$

- Varianceê°€ $d_k$ì— ë¹„ë¡€
- Large magnitude â†’ softmaxê°€ **extremely small gradient** ì˜ì—­ìœ¼ë¡œ ì´ë™

**í•´ë²•:** $1/\sqrt{d_k}$ë¡œ scalingí•˜ì—¬ varianceë¥¼ 1ë¡œ ìœ ì§€.

### Computational Efficiency

**Additive attention (Bahdanau et al.):**

$$
\text{score}(h_i, s_j) = v^T \tanh(W_1 h_i + W_2 s_j)
$$

- Feed-forward network í•„ìš”
- Theoretically similar complexity

**Dot-product attention:**

$$
\text{score}(Q, K) = QK^T
$$

- **Highly optimized matrix multiplication** (BLAS)
- í›¨ì”¬ ë¹ ë¥´ê³  memory-efficient

---

## Multi-Head Attention

### ë™ê¸°

**ë¬¸ì œ:** Single attention headëŠ” í•˜ë‚˜ì˜ representation subspaceë§Œ capture.

**í•´ë²•:** $h$ê°œì˜ parallel attention headë¥¼ ì‚¬ìš©í•˜ì—¬ **ë‹¤ì–‘í•œ representation subspace**ë¥¼ í•™ìŠµ.

### ìˆ˜ì‹

$$
\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O
$$

ì—¬ê¸°ì„œ:

$$
\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)
$$

**Parameter matrices:**

- $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$
- $W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}}$

**Transformer ì„¤ì •:**

- $h = 8$ heads
- $d_k = d_v = d_{\text{model}}/h = 64$
- **Total computational cost â‰ˆ single-head full dimensionality** (dimensionì´ ì¤„ì–´ë“  ë§Œí¼ head ìˆ˜ê°€ ëŠ˜ì–´ë‚¨)

### 3ê°€ì§€ Attention í™œìš© ë°©ì‹

**1. Encoder Self-Attention**

- All queries, keys, values from **same encoder layer**
- ê° positionì´ encoderì˜ **ëª¨ë“  positionì„ attend**

**2. Decoder Self-Attention**

- All queries, keys, values from **same decoder layer**
- **Masking:** Position $i$ëŠ” position $< i$ë§Œ attend (prevent looking ahead)

**3. Encoder-Decoder Attention**

- **Queries:** Decoder layer
- **Keys, Values:** Encoder output
- Decoderì˜ ê° positionì´ **input sequenceì˜ ëª¨ë“  positionì„ attend**

---

## Position-wise Feed-Forward Networks

$$
\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2
$$

**íŠ¹ì§•:**

- **Position-wise:** ê° positionì— **independently and identically** ì ìš©
- 2ê°œ linear transformation + ReLU
- **$d_{\text{model}} = 512 \to d_{ff} = 2048 \to 512$**
- Kernel size 1ì¸ 2ê°œ convolutionìœ¼ë¡œ ë³¼ ìˆ˜ë„ ìˆìŒ

---

## Positional Encoding

### ì™œ í•„ìš”í•œê°€?

**ë¬¸ì œ:** Attentionì€ **permutation-invariant** (ìˆœì„œ ì •ë³´ ì—†ìŒ)

**í•´ë²•:** Input embeddingì— **positional encoding ì¶”ê°€**.

### Sinusoidal Positional Encoding

$$
\begin{aligned}
PE_{(pos, 2i)} &= \sin(pos / 10000^{2i/d_{\text{model}}}) \\
PE_{(pos, 2i+1)} &= \cos(pos / 10000^{2i/d_{\text{model}}})
\end{aligned}
$$

**íŠ¹ì§•:**

- $pos$: Position index
- $i$: Dimension index
- **Sinusoidì˜ wavelength:** $2\pi \to 10000 \cdot 2\pi$

**ì¥ì :**

1. **Relative positionì„ í•™ìŠµí•˜ê¸° ì‰¬ì›€:**

$$
PE_{pos+k} = f(PE_{pos})
$$

(Linear function)

2. **Extrapolation:** Training ì‹œë³´ë‹¤ ê¸´ sequenceì—ë„ ì ìš© ê°€ëŠ¥

**Learned positional embeddings vs Sinusoidal:**

- ê±°ì˜ ë™ì¼í•œ ì„±ëŠ¥ (Table 3, row E)
- Sinusoidal ì„ íƒ ì´ìœ : Extrapolation potential

---

## Training Details

### Datasets

**WMT 2014 English-German:**
- 4.5M sentence pairs
- Byte-pair encoding, shared vocab ~37K tokens

**WMT 2014 English-French:**
- 36M sentence pairs
- Word-piece vocab ~32K tokens

### Hardware & Schedule

**Base model:**
- **Hardware:** 1 machine, 8 NVIDIA P100 GPUs
- **Step time:** 0.4 seconds
- **Total:** 100K steps = **12 hours**

**Big model:**
- **Step time:** 1.0 seconds
- **Total:** 300K steps = **3.5 days**

### Optimizer: Adam

**Learning rate schedule:**

$$
\text{lrate} = d_{\text{model}}^{-0.5} \cdot \min(\text{step\_num}^{-0.5}, \text{step\_num} \cdot \text{warmup\_steps}^{-1.5})
$$

- **Warmup:** ì²˜ìŒ 4000 steps ë™ì•ˆ linearly ì¦ê°€
- ì´í›„: Step numberì˜ inverse square rootì— ë¹„ë¡€í•˜ì—¬ ê°ì†Œ

### Regularization

**1. Residual Dropout:** $P_{drop} = 0.1$

**2. Label Smoothing:** $\epsilon_{ls} = 0.1$

- PerplexityëŠ” ì•½ê°„ ì˜¬ë¼ê°€ì§€ë§Œ
- **Accuracyì™€ BLEU score í–¥ìƒ**

---

## Results

### Machine Translation

**WMT 2014 English-to-German:**

| Model | BLEU | Training Cost |
|---|---|---|
| Previous SOTA (ensemble) | 26.4 | - |
| **Transformer (big)** | **28.4** | **3.5 days, 8 P100 GPUs** |
| Transformer (base) | 27.3 | 12 hours, 8 P100 GPUs |

**+2.0 BLEU over previous best (including ensembles)**

**WMT 2014 English-to-French:**

| Model | BLEU | Training Cost |
|---|---|---|
| Previous SOTA | 40.4 | - |
| **Transformer (big)** | **41.0** | **3.5 days, 8 P100 GPUs** |

**1/4 training cost of previous SOTA**

### Model Variations (Ablation Study)

**Key findings (Table 3):**

**Number of heads ($h$):**
- 1 head: 25.8 BLEU (worse)
- 4 heads: 25.5 BLEU
- **8 heads: 25.8 BLEU (best)**
- 16 heads: 25.8 BLEU
- 32 heads: 25.4 BLEU (too many heads hurts)

**Key dimension ($d_k$):**
- $d_k = 16$: 25.4 BLEU (too small)
- **$d_k = 64$: 25.8 BLEU (best)**
- $d_k = 128$: 25.5 BLEU

**Model size ($d_{\text{model}}$):**
- 256: 24.5 BLEU
- **512: 25.8 BLEU (base)**
- **1024: 26.0 BLEU (big, best)**

**Dropout:**
- 0.0: 24.6 BLEU (overfitting)
- **0.1: 25.8 BLEU (best)**
- 0.2: 25.5 BLEU

### English Constituency Parsing

**Penn Treebank:**

- **40K training sentences:** 91.3% F1 (outperforms all previous models)
- **Only 16K training sentences:** 88.4% F1 (competitive)

**Transformer generalizes well to other tasks beyond translation**

---

## Why Self-Attention?

ë…¼ë¬¸ì€ self-attentionì„ ì„ íƒí•œ ì´ìœ ë¥¼ **3ê°€ì§€ desiderata**ë¡œ ì„¤ëª…:

### 1. Computational Complexity per Layer

| Layer Type | Complexity | Sequential |
|---|---|---|
| Self-Attention | $O(n^2 \cdot d)$ | $O(1)$ |
| Recurrent | $O(n \cdot d^2)$ | $O(n)$ |
| Convolutional | $O(k \cdot n \cdot d^2)$ | $O(1)$ |

**ëŒ€ë¶€ë¶„ì˜ ê²½ìš° $n < d$** (sentence length < representation dim)
â†’ **Self-attentionì´ recurrentë³´ë‹¤ ë¹ ë¦„**

### 2. Parallelization

**Minimum sequential operations:**

- **Self-Attention:** $O(1)$ (ëª¨ë“  position ë™ì‹œ ê³„ì‚°)
- **Recurrent:** $O(n)$ (ìˆœì°¨ì )
- **Convolutional:** $O(1)$

### 3. Path Length Between Long-Range Dependencies

**Maximum path length:**

- **Self-Attention:** $O(1)$ (ì§ì ‘ ì—°ê²°)
- **Recurrent:** $O(n)$ (ëª¨ë“  step ê±°ì³ì•¼ í•¨)
- **Convolutional:** $O(\log_k(n))$ (dilated convolution)

**ì§§ì€ path â†’ ì‰½ê²Œ long-range dependency í•™ìŠµ**

---

## Impact & Legacy

### ì¦‰ê°ì  ì˜í–¥

1. **Machine translation SOTA** (2017)
2. **Training efficiency íšê¸°ì  ê°œì„ ** (12 hours for competitive model)
3. **Attention mechanismì˜ ì¤‘ìš”ì„± ì…ì¦**

### ì¥ê¸°ì  ì˜í–¥

**TransformerëŠ” í˜„ëŒ€ AIì˜ foundationì´ ë˜ì—ˆë‹¤:**

**NLP:**
- **BERT** (2018): Bidirectional transformer for pre-training
- **GPT series** (2018-): Autoregressive transformer for language generation
- **T5, BART, etc.:** Various transformer variants

**Vision:**
- **Vision Transformer (ViT)** (2020): Image classification
- **DETR** (2020): Object detection
- **Swin Transformer** (2021): Hierarchical vision transformer

**Multimodal:**
- **CLIP** (2021): Vision-language pre-training
- **Flamingo, DALL-E, etc.:** Text-to-image generation

**Beyond:**
- **AlphaFold2** (2020): Protein structure prediction (attention-based)
- **MolCrystalFlow, SpecLig:** Molecular design (attention-based)

---

## Discussion: ì˜ì˜ì™€ í•œê³„

### í˜ëª…ì  ì˜ì˜

**1. Paradigm shift**

- RNN/CNN ì¤‘ì‹¬ â†’ **Attention ì¤‘ì‹¬**
- Sequential processing â†’ **Parallel processing**

**2. Scalability**

- Large model í•™ìŠµ ê°€ëŠ¥ (parallelization)
- Pre-training + fine-tuning paradigmì˜ ê¸°ì´ˆ

**3. Interpretability**

- Attention weight visualization
- Modelì´ "ë¬´ì—‡ì„ ë³´ëŠ”ì§€" íŒŒì•… ê°€ëŠ¥

### í•œê³„

**1. Quadratic complexity in sequence length**

$$
O(n^2 \cdot d)
$$

- Long sequence (document-level)ì—ì„œ ë¬¸ì œ
- í–¥í›„: Sparse attention, linear attention (Performer, Linformer, etc.)

**2. Lack of inductive bias**

- RNN: Sequential bias
- CNN: Locality bias
- **Transformer: No bias** â†’ ë§ì€ data í•„ìš”

**3. Positional encoding**

- Sinusoidal encodingì´ optimalì¸ì§€ ë¶ˆëª…í™•
- í–¥í›„: Relative positional encoding (T5, DeBERTa)

---

## TL;DR

1. **TransformerëŠ” recurrenceì™€ convolutionì„ ì œê±°í•˜ê³  attention mechanismë§Œìœ¼ë¡œ sequence transductionì„ ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì´ë‹¤.**
2. **Scaled dot-product attentionê³¼ multi-head attentionìœ¼ë¡œ ë‹¤ì–‘í•œ representation subspaceë¥¼ í•™ìŠµí•œë‹¤.**
3. **WMT 2014 En-Deì—ì„œ BLEU 28.4ë¡œ +2.0 BLEU ê°œì„ , 1/4 training costë¡œ ë‹¬ì„±í–ˆë‹¤.**
4. **ì´í›„ BERT, GPT, ViT ë“± í˜„ëŒ€ AIì˜ foundationì´ ë˜ì—ˆë‹¤.**

---

## References

- [Paper (arXiv)](https://arxiv.org/abs/1706.03762)
- Vaswani, Ashish, et al. "Attention is all you need." NIPS 2017.
- Google Brain, Google Research, University of Toronto
- Implementation: [tensor2tensor](https://github.com/tensorflow/tensor2tensor)

---

> ì´ ê¸€ì€ LLMì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
