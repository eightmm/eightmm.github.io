---
title: "Accurate Structure Prediction of Biomolecular Interactions with AlphaFold 3"
date: 2026-02-20 13:00:00 +0900
description: "AlphaFold 3ëŠ” diffusion ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¡œ ë‹¨ë°±ì§ˆ, í•µì‚°, ë¦¬ê°„ë“œ, ì´ì˜¨ ë“± ëª¨ë“  ìƒì²´ë¶„ìì˜ ë³µí•©ì²´ êµ¬ì¡°ë¥¼ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ì—ì„œ ë†’ì€ ì •í™•ë„ë¡œ ì˜ˆì¸¡í•œë‹¤."
categories: [AI, Protein Structure]
tags: [protein-structure, AlphaFold3, diffusion, biomolecular-interactions, drug-discovery, structure-prediction]
math: true
mermaid: true
image:
  path: /assets/img/posts/alphafold3-accurate-biomolecular-interactions/fig1.png
  alt: "AlphaFold 3 architecture and performance across biomolecular complexes"
---

## Hook

AlphaFold 2ê°€ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ì˜ í˜ëª…ì„ ì¼ìœ¼í‚¨ ì§€ 3ë…„. ì´ì œ ë‹¨ë°±ì§ˆë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•˜ë‹¤. ì•½ë¬¼ ê°œë°œì„ ìœ„í•´ì„œëŠ” ë‹¨ë°±ì§ˆê³¼ ë¦¬ê°„ë“œê°€ ì–´ë–»ê²Œ ê²°í•©í•˜ëŠ”ì§€ ì•Œì•„ì•¼ í•˜ê³ , ìœ ì „ì ì¡°ì ˆì„ ì´í•´í•˜ë ¤ë©´ ë‹¨ë°±ì§ˆê³¼ DNAì˜ ìƒí˜¸ì‘ìš©ì´ í•„ìš”í•˜ë©°, í•­ì²´ ì¹˜ë£Œì œ ì„¤ê³„ì—ëŠ” í•­ì²´-í•­ì› ë³µí•©ì²´ì˜ êµ¬ì¡°ê°€ í•µì‹¬ì´ë‹¤. ê¸°ì¡´ì—ëŠ” ê° ìƒí˜¸ì‘ìš© ìœ í˜•ë§ˆë‹¤ íŠ¹í™”ëœ ë„êµ¬ê°€ ë”°ë¡œ í•„ìš”í–ˆë‹¤.

AlphaFold 3ëŠ” ì´ ëª¨ë“  ê²ƒì„ í•˜ë‚˜ì˜ ëª¨ë¸ë¡œ í•´ê²°í•œë‹¤. ë‹¨ë°±ì§ˆ, í•µì‚°, ì‘ì€ ë¶„ì, ì´ì˜¨, ìˆ˜ì •ëœ ì”ê¸°ë¥¼ í¬í•¨í•˜ëŠ” ë³µí•©ì²´ì˜ êµ¬ì¡°ë¥¼ ë‹¨ì¼ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œ ì˜ˆì¸¡í•˜ë©°, ê° ë¶„ì•¼ì˜ íŠ¹í™” ë„êµ¬ë“¤ë³´ë‹¤ í›¨ì”¬ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•œë‹¤. ì´ ê¸€ì€ AlphaFold ì‹œë¦¬ì¦ˆì˜ ì„¸ ë²ˆì§¸ ê¸€ë¡œ, AF2ì˜ IPA ê¸°ë°˜ êµ¬ì¡° ëª¨ë“ˆì—ì„œ diffusion ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¡œì˜ ì „í™˜, ê·¸ë¦¬ê³  ë‹¨ë°±ì§ˆ ì „ìš© ëª¨ë¸ì—ì„œ ì „ì²´ ìƒì²´ë¶„ì ê³µê°„ì„ ë‹¤ë£¨ëŠ” ì¼ë°˜í™”ëœ ëª¨ë¸ë¡œì˜ ì§„í™”ë¥¼ ë‹¤ë£¬ë‹¤.

## Problem

AlphaFold 2ê°€ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ ë¬¸ì œë¥¼ ê±°ì˜ í•´ê²°í•œ ê²ƒì²˜ëŸ¼ ë³´ì˜€ì§€ë§Œ, ì‹¤ì œ ìƒë¬¼í•™ì  ê³¼ì •ì€ ë‹¨ë°±ì§ˆ ë‹¨ë…ìœ¼ë¡œ ì¼ì–´ë‚˜ì§€ ì•ŠëŠ”ë‹¤. ì„¸í¬ ë‚´ì—ì„œëŠ” ë‹¨ë°±ì§ˆ-DNA, ë‹¨ë°±ì§ˆ-RNA, ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ, í•­ì²´-í•­ì› ë“± ë‹¤ì–‘í•œ ìƒì²´ë¶„ì ê°„ ìƒí˜¸ì‘ìš©ì´ ì¼ì–´ë‚œë‹¤.

ê¸°ì¡´ ì ‘ê·¼ë²•ë“¤ì€ ê° ìƒí˜¸ì‘ìš© ìœ í˜•ë§ˆë‹¤ íŠ¹í™”ëœ ë„êµ¬ë¥¼ ê°œë°œí•´ì™”ë‹¤. ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ dockingì„ ìœ„í•œ AutoDock Vina, ë‹¨ë°±ì§ˆ-RNA ì˜ˆì¸¡ì„ ìœ„í•œ RoseTTAFold2NA, í•­ì²´ êµ¬ì¡° ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹í™” ëª¨ë¸ ë“±. ê·¸ëŸ¬ë‚˜ ì´ëŸ° ë¶„ì ˆëœ ì ‘ê·¼ë²•ì€ ëª‡ ê°€ì§€ ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆë‹¤.

ì²«ì§¸, **ì¼ë°˜í™” ëŠ¥ë ¥ì˜ ë¶€ì¡±**ì´ë‹¤. ê° ë„êµ¬ëŠ” íŠ¹ì • ìƒí˜¸ì‘ìš© ìœ í˜•ì—ë§Œ ì‘ë™í•˜ë©°, ì—¬ëŸ¬ ìœ í˜•ì˜ ë¶„ìê°€ ë™ì‹œì— ì¡´ì¬í•˜ëŠ” ë³µì¡í•œ ë³µí•©ì²´ë¥¼ ë‹¤ë£° ìˆ˜ ì—†ë‹¤. ë‘˜ì§¸, **ì •í™•ë„ì˜ í•œê³„**ë‹¤. íŠ¹íˆ ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œ docking ê°™ì€ ê²½ìš° ë¬¼ë¦¬ ê¸°ë°˜ ë°©ë²•ë“¤ì´ ì—¬ì „íˆ ì‚¬ìš©ë˜ì§€ë§Œ, ì •í™•ë„ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì•„ ì‹¤ì œ ì•½ë¬¼ ê°œë°œì— ì œí•œì ì´ë‹¤. ì…‹ì§¸, **ë°ì´í„° íš¨ìœ¨ì„± ë¬¸ì œ**ë‹¤. ê° íŠ¹í™” ëª¨ë¸ì€ í•´ë‹¹ ë¶„ì•¼ì˜ ë°ì´í„°ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ, ë‹¤ë¥¸ ìƒì²´ë¶„ì ìƒí˜¸ì‘ìš©ì—ì„œ ì–»ì€ êµ¬ì¡°ì  ì§€ì‹ì„ í™œìš©í•˜ì§€ ëª»í•œë‹¤.

AlphaFold 2ë¥¼ ë‹¨ìˆœíˆ í™•ì¥í•´ì„œ ì´ ë¬¸ì œë“¤ì„ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ? AF2ì˜ êµ¬ì¡° ëª¨ë“ˆì€ ì•„ë¯¸ë…¸ì‚° íŠ¹í™”ëœ í”„ë ˆì„(backbone frame)ê³¼ side-chain torsion angle í‘œí˜„ì„ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ë‹¨ë°±ì§ˆì—ëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ì„ì˜ì˜ í™”í•™ êµ¬ì¡°ë¥¼ ê°€ì§„ ë¦¬ê°„ë“œë‚˜ í•µì‚°ì˜ ë‹¤ì–‘í•œ í˜•íƒœì—ëŠ” ì ìš©í•˜ê¸° ì–´ë µë‹¤. ë˜í•œ AF2ëŠ” stereochemical violationì„ ë§‰ê¸° ìœ„í•´ ë³µì¡í•˜ê²Œ ì¡°ì •ëœ penaltyë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì´ëŠ” ì¼ë°˜ì ì¸ ë¶„ì ê·¸ë˜í”„ë¡œ í™•ì¥í•˜ê¸° ì–´ë µë‹¤.

## Key Idea

AlphaFold 3ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” **diffusion ê¸°ë°˜ ìƒì„± ëª¨ë¸ì„ ì‚¬ìš©í•´ ì›ì ì¢Œí‘œë¥¼ ì§ì ‘ ì˜ˆì¸¡**í•˜ëŠ” ê²ƒì´ë‹¤. AF2ì˜ IPA (Invariant Point Attention) ê¸°ë°˜ structure moduleì„ ë²„ë¦¬ê³ , ëŒ€ì‹  ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì›ì ì¢Œí‘œë¥¼ ë°›ì•„ ì‹¤ì œ ì¢Œí‘œë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” diffusion moduleì„ ë„ì…í–ˆë‹¤.

ì´ ë³€í™”ëŠ” ë‹¨ìˆœí•œ ì•„í‚¤í…ì²˜ êµì²´ê°€ ì•„ë‹ˆë¼ ì² í•™ì  ì „í™˜ì´ë‹¤. AF2ëŠ” "ë‹¨ë°±ì§ˆì˜ êµ¬ì¡°ì  ì œì•½ì„ ëª…ì‹œì ìœ¼ë¡œ ì¸ì½”ë”©"í•˜ëŠ” ì ‘ê·¼ì´ì—ˆë‹¤ë©´, AF3ëŠ” "ëª¨ë¸ì´ ë°ì´í„°ë¡œë¶€í„° ëª¨ë“  ìƒì²´ë¶„ìì˜ ê¸°í•˜í•™ì  ì œì•½ì„ í•™ìŠµ"í•˜ë„ë¡ í•œë‹¤. Diffusion ëª¨ë¸ì€ ë‹¤ì–‘í•œ noise levelì—ì„œ êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ëŠ”ë°, ë‚®ì€ ë…¸ì´ì¦ˆì—ì„œëŠ” local stereochemistryë¥¼, ë†’ì€ ë…¸ì´ì¦ˆì—ì„œëŠ” large-scale êµ¬ì¡°ë¥¼ í•™ìŠµí•œë‹¤. ì´ëŠ” stereochemical violation penaltyë‚˜ torsion angle í‘œí˜„ ì—†ì´ë„ í™”í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•œ êµ¬ì¡°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

ë‘ ë²ˆì§¸ í•µì‹¬ì€ **MSA ì²˜ë¦¬ì˜ ê°„ì†Œí™”**ë‹¤. AF2ì˜ evoformerëŠ” MSAë¥¼ ì§‘ì¤‘ì ìœ¼ë¡œ ì²˜ë¦¬í–ˆì§€ë§Œ, AF3ëŠ” ì´ë¥¼ pairformerë¡œ êµì²´í•˜ì—¬ MSA ì²˜ë¦¬ë¥¼ ìµœì†Œí™”í•˜ê³  pair representationì— ì§‘ì¤‘í•œë‹¤. ì´ëŠ” ë‹¨ë°±ì§ˆ ê°„ ì§„í™” ì •ë³´ëŠ” í’ë¶€í•˜ì§€ë§Œ, ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œë‚˜ ë‹¨ë°±ì§ˆ-RNA ê°„ì—ëŠ” cross-entity evolutionary informationì´ ê±°ì˜ ì—†ëŠ” í˜„ì‹¤ì„ ë°˜ì˜í•œë‹¤. 

ì„¸ ë²ˆì§¸ëŠ” **cross-distillation**ì„ í†µí•œ hallucination ë°©ì§€ë‹¤. ìƒì„± ëª¨ë¸ì€ unstructured regionì—ì„œë„ ê·¸ëŸ´ë“¯í•œ êµ¬ì¡°ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ê²½í–¥(hallucination)ì´ ìˆë‹¤. AF3ëŠ” AlphaFold-Multimer v.2.3ë¡œ ì˜ˆì¸¡í•œ êµ¬ì¡°ë¥¼ training dataì— ì¶”ê°€í•˜ì—¬, unstructured regionì„ ê¸´ extended loopìœ¼ë¡œ í‘œí˜„í•˜ë„ë¡ í•™ìŠµí•œë‹¤.

> í•µì‹¬ì€ "ë²”ìš©ì„±ì„ ìœ„í•œ ë‹¨ìˆœí™”"ë‹¤. ë‹¨ë°±ì§ˆ íŠ¹í™” ì œì•½ì„ ì œê±°í•˜ê³ , diffusionì„ í†µí•´ ëª¨ë“  ìƒì²´ë¶„ìì˜ ê¸°í•˜í•™ì„ ë™ë“±í•˜ê²Œ ë‹¤ë£¬ë‹¤.
{: .prompt-tip }

## How it works

### 4.1 Overview

AlphaFold 3ì˜ ì „ì²´ ì•„í‚¤í…ì²˜ëŠ” í¬ê²Œ ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ëœë‹¤: (1) **Input embedding** â€” ì„œì—´, MSA, template ì •ë³´ë¥¼ embedding, (2) **Pairformer trunk** â€” pairì™€ single representationì„ ì§„í™”ì‹œí‚´, (3) **Diffusion module** â€” ì›ì ì¢Œí‘œë¥¼ ìƒì„±.

```mermaid
graph TD
    A[Input: Sequences, MSA, Templates] --> B[Input Embedding]
    B --> C[MSA Embedding 4 blocks]
    C --> D[Pairformer 48 blocks]
    D --> E[Diffusion Module]
    E --> F[Atom Coordinates]
    
    D --> G[Confidence Head]
    G --> H[pLDDT, PAE, PDE]
    
    style A fill:#fff4e6
    style F fill:#e8f5e9
    style H fill:#e3f2fd
```

AF2ì™€ ë¹„êµí–ˆì„ ë•Œ ê°€ì¥ í° ë³€í™”ëŠ”:
- **Evoformer (48 blocks) â†’ MSA embedding (4 blocks) + Pairformer (48 blocks)**: MSA ì²˜ë¦¬ë¥¼ ëŒ€í­ ì¶•ì†Œ
- **Structure module (IPA-based) â†’ Diffusion module**: ì•„ë¯¸ë…¸ì‚° frame ëŒ€ì‹  ì›ì ì¢Œí‘œ ì§ì ‘ ì˜ˆì¸¡
- **Deterministic prediction â†’ Generative sampling**: ë‹¨ì¼ ì˜ˆì¸¡ì´ ì•„ë‹Œ ë¶„í¬ë¡œë¶€í„° ìƒ˜í”Œë§

<details>
<summary>ğŸ“ Overall Architecture Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class AlphaFold3(nn.Module):
    """AlphaFold 3 overall architecture"""
    def __init__(self, config):
        super().__init__()
        # Input processing
        self.input_embedder = InputEmbedder(config)
        
        # MSA processing (reduced from AF2)
        self.msa_module = MSAModule(
            n_blocks=4,  # AF2 had 48 evoformer blocks
            pair_dim=128,
            msa_dim=64
        )
        
        # Pairformer trunk (replaces evoformer)
        self.pairformer = Pairformer(
            n_blocks=48,
            pair_dim=128,
            single_dim=384
        )
        
        # Diffusion-based structure module
        self.diffusion_module = DiffusionModule(
            pair_dim=128,
            single_dim=384,
            atom_encoder_depth=3,
            token_transformer_depth=24,
            atom_decoder_depth=3
        )
        
        # Confidence prediction
        self.confidence_head = ConfidenceHead(
            pair_dim=128,
            predict_plddt=True,
            predict_pae=True,
            predict_pde=True
        )
    
    def forward(self, batch, num_diffusion_steps=200):
        """
        Args:
            batch: dict containing sequences, MSA, templates, etc.
            num_diffusion_steps: number of denoising steps
        Returns:
            atom_positions: (n_atoms, 3) final coordinates
            confidences: dict of pLDDT, PAE, PDE
        """
        # Step 1: Input embedding
        # Creates initial pair (n, n, 128) and single (n, 384) representations
        # n = number of tokens (residues + atoms)
        input_feats = self.input_embedder(batch)
        pair_repr = input_feats['pair']  # (n, n, 128)
        single_repr = input_feats['single']  # (n, 384)
        msa_repr = input_feats['msa']  # (n_seq, n, 64)
        
        # Step 2: MSA processing (lightweight)
        # Updates pair representation using MSA
        pair_repr = self.msa_module(
            msa_repr=msa_repr,
            pair_repr=pair_repr
        )
        # MSA is discarded after this point (unlike AF2)
        
        # Step 3: Pairformer trunk
        # Iteratively refines pair and single representations
        for block in self.pairformer.blocks:
            pair_repr, single_repr = block(pair_repr, single_repr)
        
        # Step 4: Diffusion-based structure generation
        # Start from random noise
        atom_pos_noised = torch.randn(n_atoms, 3)
        
        # Iterative denoising
        for t in reversed(range(num_diffusion_steps)):
            atom_pos_noised = self.diffusion_module.denoise_step(
                atom_pos_noised,
                t,
                pair_repr,
                single_repr,
                input_feats
            )
        
        atom_positions = atom_pos_noised  # Final denoised coordinates
        
        # Step 5: Confidence prediction
        # Uses pair representation to predict accuracy metrics
        confidences = self.confidence_head(pair_repr, atom_positions)
        
        return atom_positions, confidences
```

</details>

### 4.2 Representation

**Token representation**: AF3ëŠ” polymer residueì™€ atomì„ **token**ìœ¼ë¡œ í‘œí˜„í•œë‹¤. ë‹¨ë°±ì§ˆ/í•µì‚°ì˜ ê²½ìš° ê° residueê°€ í•˜ë‚˜ì˜ token, ë¦¬ê°„ë“œì˜ ê²½ìš° ê° heavy atomì´ í•˜ë‚˜ì˜ tokenì´ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ìµœëŒ€ 5,120 tokensê¹Œì§€ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë©°, ì´ëŠ” ìˆ˜ì²œ ê°œì˜ residueë¥¼ ê°€ì§„ ë³µí•©ì²´ë„ ë‹¤ë£° ìˆ˜ ìˆë‹¤.

**Input features** (Supplementary Table 5):
- **Single representation** (n, 384): ê° tokenì˜ íŠ¹ì„± (residue type, atom type, charge ë“±)
- **Pair representation** (n, n, 128): token ê°„ ê´€ê³„ (distance, bond type, template ì •ë³´ ë“±)
- **MSA representation** (n_seq, n, 64): ì§„í™” ì •ë³´ (ë‹¨ë°±ì§ˆë§Œ í•´ë‹¹)

Template êµ¬ì¡°ê°€ ìˆëŠ” ê²½ìš°, templateì˜ ì›ì ê°„ ê±°ë¦¬ì™€ ë‹¨ìœ„ ë²¡í„°ê°€ pair representationì— ì¸ì½”ë”©ëœë‹¤. ë¦¬ê°„ë“œì˜ ê²½ìš° SMILES ë¬¸ìì—´ë¡œë¶€í„° RDKitì„ ì‚¬ìš©í•´ 3D conformerë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ "reference position" featureë¡œ ì‚¬ìš©í•œë‹¤ (ë‹¨, training set cutoff ì´í›„ ë°ì´í„°ëŠ” ì œì™¸).

![AlphaFold 3 Architecture](/assets/img/posts/alphafold3-accurate-biomolecular-interactions/fig2.png)
_Figure 2: AF3ì˜ í•µì‹¬ ëª¨ë“ˆë“¤. (a) Pairformer module, (b) Diffusion module, (c) Training setup, (d) Training curves_

### 4.3 Core Architecture

#### Pairformer Module

PairformerëŠ” AF2ì˜ evoformerë¥¼ ëŒ€ì²´í•˜ë©°, **MSA representationì„ ìœ ì§€í•˜ì§€ ì•Šê³ ** pairì™€ single representationë§Œ ì²˜ë¦¬í•œë‹¤. ê° blockì€ ë‹¤ìŒìœ¼ë¡œ êµ¬ì„±ëœë‹¤:

1. **Triangle multiplicative update** (pair â†’ pair): ì‚¼ê° ê´€ê³„ë¥¼ í†µí•´ pair representation ì—…ë°ì´íŠ¸
2. **Triangle self-attention** (pair â†’ pair): pairì˜ ê° edgeì— ëŒ€í•´ attention
3. **Pair-to-single transition**: pair ì •ë³´ë¥¼ singleë¡œ ì§‘ê³„
4. **Single self-attention with pair bias**: singleì— ëŒ€í•œ attention (pairê°€ biasë¡œ ì‘ìš©)
5. **Single-to-pair transition**: ì—…ë°ì´íŠ¸ëœ single ì •ë³´ë¥¼ ë‹¤ì‹œ pairë¡œ ì „íŒŒ

<details>
<summary>ğŸ“ Pairformer Block Implementation (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class PairformerBlock(nn.Module):
    """
    Single pairformer block that updates pair and single representations
    without maintaining MSA (unlike AF2 evoformer)
    """
    def __init__(self, pair_dim=128, single_dim=384, n_heads=8):
        super().__init__()
        self.pair_dim = pair_dim
        self.single_dim = single_dim
        
        # Pair representation updates
        self.triangle_mult_outgoing = TriangleMultiplication(
            dim=pair_dim, mode='outgoing'
        )
        self.triangle_mult_incoming = TriangleMultiplication(
            dim=pair_dim, mode='incoming'
        )
        self.triangle_attn_start = TriangleAttention(
            dim=pair_dim, orientation='per_row'
        )
        self.triangle_attn_end = TriangleAttention(
            dim=pair_dim, orientation='per_column'
        )
        
        # Pair to single
        self.pair_to_single = nn.Linear(pair_dim, single_dim)
        
        # Single representation update
        self.single_self_attn = SingleSelfAttention(
            dim=single_dim,
            pair_dim=pair_dim,
            n_heads=n_heads
        )
        
        # Single to pair
        self.single_to_pair = nn.Linear(single_dim * 2, pair_dim)
        
    def forward(self, pair_repr, single_repr):
        """
        Args:
            pair_repr: (n, n, pair_dim) pairwise representation
            single_repr: (n, single_dim) per-token representation
        Returns:
            updated pair_repr, single_repr
        """
        n = pair_repr.shape[0]
        
        # --- Pair updates (triangle operations) ---
        # These enforce consistency: if i-j and j-k are close,
        # then i-k should also be considered
        pair_update1 = self.triangle_mult_outgoing(pair_repr)
        pair_update2 = self.triangle_mult_incoming(pair_repr)
        pair_update3 = self.triangle_attn_start(pair_repr)
        pair_update4 = self.triangle_attn_end(pair_repr)
        pair_repr = pair_repr + pair_update1 + pair_update2 + pair_update3 + pair_update4
        
        # --- Pair to single ---
        # Aggregate pairwise information for each token
        # Average over all partners: mean over axis 1
        pair_aggregated = pair_repr.mean(dim=1)  # (n, pair_dim)
        single_update1 = self.pair_to_single(pair_aggregated)
        single_repr = single_repr + single_update1
        
        # --- Single update with pair bias ---
        # Self-attention over tokens, but attention logits are biased
        # by pairwise representation
        single_update2 = self.single_self_attn(single_repr, pair_bias=pair_repr)
        single_repr = single_repr + single_update2
        
        # --- Single to pair ---
        # Outer sum: combine single_i and single_j for each pair (i,j)
        single_i = single_repr.unsqueeze(1).expand(n, n, -1)  # (n, n, single_dim)
        single_j = single_repr.unsqueeze(0).expand(n, n, -1)  # (n, n, single_dim)
        single_outer = torch.cat([single_i, single_j], dim=-1)  # (n, n, 2*single_dim)
        pair_update = self.single_to_pair(single_outer)
        pair_repr = pair_repr + pair_update
        
        return pair_repr, single_repr


class TriangleMultiplication(nn.Module):
    """
    Triangle multiplicative update from AF2
    Enforces geometric consistency: d(i,k) ~ d(i,j) + d(j,k)
    """
    def __init__(self, dim, mode='outgoing'):
        super().__init__()
        self.mode = mode
        self.layer_norm = nn.LayerNorm(dim)
        self.left_proj = nn.Linear(dim, dim)
        self.right_proj = nn.Linear(dim, dim)
        self.gate = nn.Linear(dim, dim)
        self.out_proj = nn.Linear(dim, dim)
        
    def forward(self, pair_repr):
        """
        Args:
            pair_repr: (n, n, dim)
        Returns:
            update: (n, n, dim)
        """
        pair_repr = self.layer_norm(pair_repr)
        
        left = self.left_proj(pair_repr)  # (n, n, dim)
        right = self.right_proj(pair_repr)  # (n, n, dim)
        gate = torch.sigmoid(self.gate(pair_repr))
        
        # Triangle equation: sum over intermediate node j
        if self.mode == 'outgoing':
            # out[i,k] = sum_j left[i,j] * right[j,k]
            update = torch.einsum('ijd,jkd->ikd', left, right)
        else:  # incoming
            # out[i,k] = sum_j left[j,i] * right[k,j]
            update = torch.einsum('jid,kjd->ikd', left, right)
        
        update = self.out_proj(update)
        return gate * update


class SingleSelfAttention(nn.Module):
    """Self-attention on single representation with pair bias"""
    def __init__(self, dim, pair_dim, n_heads=8):
        super().__init__()
        self.n_heads = n_heads
        self.head_dim = dim // n_heads
        
        self.q_proj = nn.Linear(dim, dim)
        self.k_proj = nn.Linear(dim, dim)
        self.v_proj = nn.Linear(dim, dim)
        self.pair_bias_proj = nn.Linear(pair_dim, n_heads)
        self.out_proj = nn.Linear(dim, dim)
        
    def forward(self, single_repr, pair_bias):
        """
        Args:
            single_repr: (n, dim)
            pair_bias: (n, n, pair_dim)
        Returns:
            output: (n, dim)
        """
        n = single_repr.shape[0]
        
        # Q, K, V projections
        Q = self.q_proj(single_repr).view(n, self.n_heads, self.head_dim)
        K = self.k_proj(single_repr).view(n, self.n_heads, self.head_dim)
        V = self.v_proj(single_repr).view(n, self.n_heads, self.head_dim)
        
        # Attention logits: Q @ K^T
        attn_logits = torch.einsum('ihd,jhd->hij', Q, K) / math.sqrt(self.head_dim)
        
        # Add pair bias (n, n, pair_dim) -> (n, n, n_heads)
        bias = self.pair_bias_proj(pair_bias).permute(2, 0, 1)  # (n_heads, n, n)
        attn_logits = attn_logits + bias
        
        # Softmax and apply to V
        attn_weights = F.softmax(attn_logits, dim=1)  # (n_heads, n, n)
        output = torch.einsum('hij,jhd->ihd', attn_weights, V)
        output = output.reshape(n, -1)  # (n, dim)
        
        return self.out_proj(output)
```

</details>

#### Diffusion Module

Diffusion moduleì€ AF3ì˜ ê°€ì¥ í˜ì‹ ì ì¸ ë¶€ë¶„ì´ë‹¤. ë…¸ì´ì¦ˆê°€ ì¶”ê°€ëœ ì›ì ì¢Œí‘œë¥¼ ì…ë ¥ë°›ì•„ ì‹¤ì œ ì¢Œí‘œë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµëœë‹¤. ì´ëŠ” **score-based generative model**ì˜ ì¼ì¢…ìœ¼ë¡œ, ë‹¤ìŒ ì„¸ ë‹¨ê³„ë¡œ êµ¬ì„±ëœë‹¤:

1. **Atom encoder**: ê° tokenì— ì†í•œ ì›ìë“¤ì˜ noised ì¢Œí‘œë¥¼ ì¸ì½”ë”©
2. **Token transformer**: token-levelì—ì„œ pair/single representationê³¼ ìƒí˜¸ì‘ìš©
3. **Atom decoder**: token representationì„ ë‹¤ì‹œ ì›ì ì¢Œí‘œë¡œ ë””ì½”ë”©

**Training objective**:

$$
\mathcal{L}_{\text{diffusion}} = \mathbb{E}_{t, \epsilon, \mathbf{x}_0} \left[ \left\| \epsilon - \epsilon_\theta(\mathbf{x}_t, t, c) \right\|^2 \right]
$$

ì—¬ê¸°ì„œ $\mathbf{x}_0$ëŠ” ì‹¤ì œ ì›ì ì¢Œí‘œ, $\mathbf{x}_t = \sqrt{\bar{\alpha}_t} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon$ëŠ” noised ì¢Œí‘œ, $\epsilon_\theta$ëŠ” ì˜ˆì¸¡ ë…¸ì´ì¦ˆ, $c$ëŠ” conditioning (pair, single representation).

ë…¼ë¬¸ì€ standard diffusion processë¥¼ ë”°ë¥´ì§€ë§Œ, ì¤‘ìš”í•œ ì ì€ **rotation/translation invarianceë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°•ì œí•˜ì§€ ì•ŠëŠ”ë‹¤**ëŠ” ê²ƒì´ë‹¤. ëŒ€ì‹  random rotation/translation augmentationì„ í•™ìŠµ ì‹œ ì ìš©í•˜ì—¬ ì•”ë¬µì ìœ¼ë¡œ í•™ìŠµí•œë‹¤.

<details>
<summary>ğŸ“ Diffusion Module Implementation (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
class DiffusionModule(nn.Module):
    """
    Diffusion-based structure module for AF3
    Predicts atom coordinates directly via iterative denoising
    """
    def __init__(
        self,
        pair_dim=128,
        single_dim=384,
        atom_encoder_depth=3,
        token_transformer_depth=24,
        atom_decoder_depth=3,
        atom_channels=128
    ):
        super().__init__()
        self.atom_encoder = AtomEncoder(
            depth=atom_encoder_depth,
            channels=atom_channels
        )
        self.token_transformer = TokenTransformer(
            depth=token_transformer_depth,
            single_dim=single_dim,
            pair_dim=pair_dim,
            atom_channels=atom_channels
        )
        self.atom_decoder = AtomDecoder(
            depth=atom_decoder_depth,
            channels=atom_channels
        )
        
        # Noise schedule (similar to DDPM)
        self.register_buffer('betas', self._cosine_beta_schedule(1000))
        self.register_buffer('alphas', 1.0 - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        
    def _cosine_beta_schedule(self, timesteps, s=0.008):
        """Cosine schedule as in Improved DDPM"""
        steps = timesteps + 1
        x = torch.linspace(0, timesteps, steps)
        alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * math.pi * 0.5) ** 2
        alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
        betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
        return torch.clip(betas, 0.0001, 0.9999)
    
    def forward_diffusion(self, x_0, t, noise=None):
        """
        Add noise to ground truth coordinates (training)
        Args:
            x_0: (n_atoms, 3) ground truth coordinates
            t: (batch,) timestep
            noise: (n_atoms, 3) optional noise (sampled if None)
        Returns:
            x_t: noised coordinates
            noise: the noise that was added
        """
        if noise is None:
            noise = torch.randn_like(x_0)
        
        sqrt_alpha_cumprod = torch.sqrt(self.alphas_cumprod[t])
        sqrt_one_minus_alpha_cumprod = torch.sqrt(1.0 - self.alphas_cumprod[t])
        
        # x_t = sqrt(Î±Ì„_t) * x_0 + sqrt(1 - Î±Ì„_t) * Îµ
        x_t = sqrt_alpha_cumprod * x_0 + sqrt_one_minus_alpha_cumprod * noise
        return x_t, noise
    
    def denoise_step(self, x_t, t, pair_repr, single_repr, input_feats):
        """
        Single denoising step (inference)
        Args:
            x_t: (n_atoms, 3) noised coordinates at timestep t
            t: current timestep
            pair_repr: (n_tokens, n_tokens, pair_dim)
            single_repr: (n_tokens, single_dim)
            input_feats: additional input features
        Returns:
            x_{t-1}: denoised coordinates
        """
        # Predict noise Îµ_Î¸(x_t, t, c)
        pred_noise = self.predict_noise(
            x_t, t, pair_repr, single_repr, input_feats
        )
        
        # DDPM reverse process
        alpha = self.alphas[t]
        alpha_cumprod = self.alphas_cumprod[t]
        alpha_cumprod_prev = self.alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0)
        
        # Predicted x_0
        pred_x0 = (x_t - torch.sqrt(1 - alpha_cumprod) * pred_noise) / torch.sqrt(alpha_cumprod)
        
        # Direction pointing to x_t
        dir_xt = torch.sqrt(1 - alpha_cumprod_prev) * pred_noise
        
        # Compute x_{t-1}
        x_prev = torch.sqrt(alpha_cumprod_prev) * pred_x0 + dir_xt
        
        if t > 0:
            noise = torch.randn_like(x_t)
            sigma = torch.sqrt((1 - alpha_cumprod_prev) / (1 - alpha_cumprod) * (1 - alpha))
            x_prev = x_prev + sigma * noise
        
        return x_prev
    
    def predict_noise(self, x_t, t, pair_repr, single_repr, input_feats):
        """
        Predict noise Îµ_Î¸ given noised coordinates and conditioning
        Args:
            x_t: (n_atoms, 3) noised atom coordinates
            t: timestep
            pair_repr: (n_tokens, n_tokens, pair_dim) conditioning
            single_repr: (n_tokens, single_dim) conditioning
            input_feats: dict with token_to_atom mapping
        Returns:
            pred_noise: (n_atoms, 3) predicted noise
        """
        # Embed timestep
        t_emb = self.timestep_embedding(t)  # (emb_dim,)
        
        # Step 1: Atom encoder
        # Encode per-atom positions into per-token representation
        # Each token may have multiple atoms (e.g., residue with side chain)
        atom_feats = self.atom_encoder(
            x_t, 
            input_feats['atom_to_token'],  # maps each atom to its token
            t_emb
        )  # (n_tokens, atom_channels)
        
        # Step 2: Token transformer
        # Update token representation using pair/single context
        token_repr = self.token_transformer(
            atom_feats,
            pair_repr,
            single_repr,
            t_emb
        )  # (n_tokens, atom_channels)
        
        # Step 3: Atom decoder
        # Decode token representation back to per-atom noise predictions
        pred_noise = self.atom_decoder(
            token_repr,
            input_feats['token_to_atom'],  # maps each token to its atoms
            t_emb
        )  # (n_atoms, 3)
        
        return pred_noise
    
    @staticmethod
    def timestep_embedding(timesteps, dim=128):
        """Sinusoidal timestep embedding"""
        half_dim = dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim) * -emb)
        emb = timesteps[:, None] * emb[None, :]
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)
        return emb


class AtomEncoder(nn.Module):
    """Encode noised atom positions into token-level features"""
    def __init__(self, depth=3, channels=128):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(3 + channels, channels),
                nn.ReLU(),
                nn.Linear(channels, channels)
            ) for _ in range(depth)
        ])
        
    def forward(self, atom_pos, atom_to_token, t_emb):
        """
        Args:
            atom_pos: (n_atoms, 3) noised positions
            atom_to_token: (n_atoms,) token index for each atom
            t_emb: (emb_dim,) timestep embedding
        Returns:
            token_feats: (n_tokens, channels)
        """
        n_tokens = atom_to_token.max() + 1
        
        # Initial atom features: concatenate position + timestep embedding
        atom_feats = torch.cat([atom_pos, t_emb.expand(len(atom_pos), -1)], dim=-1)
        
        # Process through layers
        for layer in self.layers:
            atom_feats = layer(atom_feats)
        
        # Aggregate atoms to tokens (mean pooling)
        token_feats = torch.zeros(n_tokens, atom_feats.shape[-1])
        for i, tok_idx in enumerate(atom_to_token):
            token_feats[tok_idx] += atom_feats[i]
        
        # Normalize by number of atoms per token
        token_counts = torch.bincount(atom_to_token, minlength=n_tokens)
        token_feats = token_feats / token_counts.unsqueeze(-1).clamp(min=1)
        
        return token_feats


class AtomDecoder(nn.Module):
    """Decode token features to per-atom noise predictions"""
    def __init__(self, depth=3, channels=128):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Sequential(
                nn.Linear(channels, channels),
                nn.ReLU()
            ) for _ in range(depth)
        ])
        self.final = nn.Linear(channels, 3)  # Output: 3D displacement
        
    def forward(self, token_feats, token_to_atom, t_emb):
        """
        Args:
            token_feats: (n_tokens, channels)
            token_to_atom: list of lists, token_to_atom[i] = atom indices for token i
            t_emb: (emb_dim,) timestep embedding
        Returns:
            atom_noise: (n_atoms, 3)
        """
        # Broadcast token features to atoms
        atom_feats = token_feats[token_to_atom]  # (n_atoms, channels)
        
        # Process through layers
        for layer in self.layers:
            atom_feats = layer(atom_feats)
        
        # Final projection to 3D coordinates
        atom_noise = self.final(atom_feats)  # (n_atoms, 3)
        
        return atom_noise
```

</details>

### 4.4 Key Innovation

AF3ì˜ ì°¨ë³„í™” í¬ì¸íŠ¸ëŠ” ì„¸ ê°€ì§€ë‹¤:

**1. Direct coordinate prediction without equivariance**: AF2ëŠ” SE(3)-equivariant IPAë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ, AF3ëŠ” rotation/translation invarianceë¥¼ ëª…ì‹œì ìœ¼ë¡œ ê°•ì œí•˜ì§€ ì•ŠëŠ”ë‹¤. ëŒ€ì‹  í•™ìŠµ ì‹œ random augmentationì„ ì ìš©í•´ ì•”ë¬µì ìœ¼ë¡œ í•™ìŠµí•œë‹¤. ì´ëŠ” ì•„í‚¤í…ì²˜ë¥¼ í¬ê²Œ ë‹¨ìˆœí™”í•˜ë©´ì„œë„ ì„±ëŠ¥ ì €í•˜ê°€ ì—†ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.

**2. Multiscale learning through diffusion**: Diffusion ê³¼ì •ì€ ìì—°ìŠ¤ëŸ½ê²Œ multiscale learningì„ ìœ ë„í•œë‹¤. ë†’ì€ ë…¸ì´ì¦ˆ ë ˆë²¨ì—ì„œëŠ” ì „ì²´ì ì¸ foldë¥¼ í•™ìŠµí•˜ê³ , ë‚®ì€ ë…¸ì´ì¦ˆ ë ˆë²¨ì—ì„œëŠ” side-chain packingê³¼ stereochemistryë¥¼ í•™ìŠµí•œë‹¤. ì´ëŠ” stereochemical violation loss ì—†ì´ë„ í™”í•™ì ìœ¼ë¡œ íƒ€ë‹¹í•œ êµ¬ì¡°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆê²Œ í•œë‹¤.

**3. Cross-distillation for disorder prediction**: ìƒì„± ëª¨ë¸ì˜ hallucination ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´, AlphaFold-Multimer v.2.3ì˜ ì˜ˆì¸¡ì„ training dataì— í¬í•¨í•œë‹¤. AF-Mì€ disordered regionì„ extended loopìœ¼ë¡œ í‘œí˜„í•˜ë¯€ë¡œ, AF3ë„ ì´ë¥¼ í•™ìŠµí•˜ì—¬ hallucinationì„ ì¤„ì¸ë‹¤.

### 4.5 Training & Inference

**Training loss**ëŠ” ì—¬ëŸ¬ ìš”ì†Œë¡œ êµ¬ì„±ëœë‹¤:

$$
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{diffusion}} + \lambda_{\text{conf}} \mathcal{L}_{\text{confidence}} + \lambda_{\text{dist}} \mathcal{L}_{\text{distogram}}
$$

- $\mathcal{L}_{\text{diffusion}}$: ìœ„ì—ì„œ ì„¤ëª…í•œ denoising loss
- $\mathcal{L}_{\text{confidence}}$: pLDDT, PAE, PDE ì˜ˆì¸¡ loss
- $\mathcal{L}_{\text{distogram}}$: auxiliary distogram prediction (AF2ì™€ ë™ì¼)

**Confidence prediction**ì€ diffusion í•™ìŠµê³¼ ë³„ë„ë¡œ ì§„í–‰ëœë‹¤. Diffusion í•™ìŠµ ì¤‘ì—ëŠ” single stepë§Œ í•™ìŠµí•˜ë¯€ë¡œ, full structureë¥¼ ìƒì„±í•  ìˆ˜ ì—†ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ **mini-rollout** ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤: í•™ìŠµ ì¤‘ ì¼ë¶€ stepì—ì„œ larger step sizeë¡œ ì „ì²´ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³ , ì´ë¥¼ ground truthì™€ ë¹„êµí•˜ì—¬ confidence headë¥¼ í•™ìŠµí•œë‹¤.

<details>
<summary>ğŸ“ Training Loop Pseudocode (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

```python
def train_alphafold3(model, dataloader, optimizer, config):
    """
    Training loop for AlphaFold 3
    """
    model.train()
    
    for epoch in range(config.num_epochs):
        for batch in dataloader:
            # batch contains:
            # - sequences, MSA, templates
            # - ground_truth_coords: (n_atoms, 3)
            # - metadata (chain boundaries, atom types, etc.)
            
            optimizer.zero_grad()
            
            # --- Forward pass through trunk ---
            # Input embedding + MSA module + Pairformer
            input_feats = model.input_embedder(batch)
            pair_repr = input_feats['pair']  # (n, n, 128)
            single_repr = input_feats['single']  # (n, 384)
            msa_repr = input_feats['msa']  # (n_seq, n, 64)
            
            # Update pair using MSA
            updated_pair = model.msa_module(msa_repr, pair_repr)
            # Refine with pairformer
            final_pair, final_single = model.pairformer(updated_pair, single_repr)
            
            # --- Diffusion training ---
            # Sample random timestep for each example in batch
            B = len(batch['ground_truth_coords'])
            t = torch.randint(0, config.diffusion_steps, (B,))
            
            # Add noise to ground truth
            x_0 = batch['ground_truth_coords']  # (n_atoms, 3)
            noise = torch.randn_like(x_0)
            x_t, noise = model.diffusion_module.forward_diffusion(x_0, t, noise)
            
            # Predict noise
            pred_noise = model.diffusion_module.predict_noise(
                x_t, t, pair_repr, single_repr, input_feats
            )
            
            # Diffusion loss (MSE on predicted noise)
            loss_diffusion = F.mse_loss(pred_noise, noise)
            
            # --- Mini-rollout for confidence training ---
            # Perform full denoising with larger step size
            # (only for some batches to save compute)
            if random.random() < config.confidence_training_freq:
                with torch.no_grad():
                    # Start from high noise
                    x_rollout = torch.randn_like(x_0)
                    rollout_steps = config.diffusion_steps // 10  # Larger steps
                    
                    for step in range(rollout_steps):
                        t_rollout = torch.full((B,), step * 10)
                        x_rollout = model.diffusion_module.denoise_step(
                            x_rollout, t_rollout, final_pair, final_single, input_feats
                        )
                
                # Predict confidence metrics from final pair
                conf_metrics = model.confidence_head(final_pair, x_rollout)
                
                # Compute ground truth metrics
                with torch.no_grad():
                    # Align prediction to ground truth (handle symmetry)
                    x_aligned = align_structures(x_rollout, x_0, batch['symmetries'])
                    
                    # Compute per-atom LDDT
                    true_plddt = compute_lddt(x_aligned, x_0, inclusion_radius=15.0)
                    
                    # Compute PAE (predicted aligned error)
                    true_pae = compute_pae(x_aligned, x_0)
                    
                    # Compute PDE (distance error)
                    true_pde = compute_pde(x_aligned, x_0)
                
                # Confidence losses (regression)
                loss_plddt = F.mse_loss(conf_metrics['plddt'], true_plddt)
                loss_pae = F.mse_loss(conf_metrics['pae'], true_pae)
                loss_pde = F.mse_loss(conf_metrics['pde'], true_pde)
                
                loss_confidence = loss_plddt + loss_pae + loss_pde
            else:
                loss_confidence = 0.0
            
            # --- Auxiliary distogram loss (from AF2) ---
            # Predict binned distance distribution from final pair
            pred_distogram = model.distogram_head(final_pair)
            true_distances = torch.cdist(x_0, x_0)  # (n_atoms, n_atoms)
            true_distogram = bin_distances(true_distances, num_bins=64, max_dist=22.0)
            loss_distogram = F.cross_entropy(
                pred_distogram.reshape(-1, 64),
                true_distogram.reshape(-1)
            )
            
            # --- Total loss ---
            loss = (
                config.lambda_diffusion * loss_diffusion +
                config.lambda_confidence * loss_confidence +
                config.lambda_distogram * loss_distogram
            )
            
            loss.backward()
            
            # Gradient clipping (important for stability)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
        print(f"Epoch {epoch}, Loss: {loss.item():.4f}")


def inference_alphafold3(model, input_data, num_seeds=5, num_samples=5):
    """
    Inference with multiple seeds and diffusion samples
    
    Args:
        model: trained AlphaFold 3 model
        input_data: sequences, MSA, templates, etc.
        num_seeds: number of model seeds (different random init)
        num_samples: number of diffusion samples per seed
    
    Returns:
        best_structure: highest confidence structure
        all_structures: list of all generated structures with confidences
    """
    model.eval()
    all_structures = []
    
    with torch.no_grad():
        # Process input
        input_feats = model.input_embedder(input_data)
        pair_repr = input_feats['pair']
        single_repr = input_feats['single']
        msa_repr = input_feats['msa']
        
        pair_repr = model.msa_module(msa_repr, pair_repr)
        pair_repr, single_repr = model.pairformer(pair_repr, single_repr)
        
        # Generate multiple samples
        for seed in range(num_seeds):
            torch.manual_seed(seed)
            
            for sample in range(num_samples):
                # Start from random noise
                n_atoms = input_feats['num_atoms']
                x_t = torch.randn(n_atoms, 3)
                
                # Iterative denoising (typically 200 steps)
                for t in reversed(range(200)):
                    x_t = model.diffusion_module.denoise_step(
                        x_t, t, pair_repr, single_repr, input_feats
                    )
                
                # Final structure
                final_coords = x_t
                
                # Predict confidence
                confidences = model.confidence_head(pair_repr, final_coords)
                
                all_structures.append({
                    'coords': final_coords,
                    'plddt': confidences['plddt'],
                    'pae': confidences['pae'],
                    'ipTM': compute_iptm(confidences['pae'], input_feats)
                })
        
        # Rank by confidence (ipTM for interfaces, pLDDT for monomers)
        all_structures.sort(
            key=lambda x: x['ipTM'] if input_feats['has_interface'] else x['plddt'].mean(),
            reverse=True
        )
        
        best_structure = all_structures[0]
    
    return best_structure, all_structures
```

</details>

**Three-stage training**:
1. **Initial training** (384 tokens): í•™ìŠµ ì´ˆë°˜, ì‘ì€ ë³µí•©ì²´ì—ì„œ ë¹ ë¥´ê²Œ ìˆ˜ë ´
2. **Fine-tuning 1** (640 tokens): crop size ì¦ê°€, protein-protein interface ì„±ëŠ¥ í–¥ìƒ
3. **Fine-tuning 2** (768 tokens): ìµœì¢… fine-tuning, í° ë³µí•©ì²´ ì„±ëŠ¥ ê°œì„ 

ì´ˆê¸° í•™ìŠµì—ì„œëŠ” local structure (intrachain LDDT)ê°€ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•˜ì§€ë§Œ (20,000 stepsì—ì„œ 97% ìµœëŒ€ì¹˜), global structure (protein-protein interface)ëŠ” í›¨ì”¬ ëŠë¦¬ê²Œ í•™ìŠµëœë‹¤ (60,000 steps). ì´ëŠ” MSAê°€ ì œê³µí•˜ëŠ” ì§„í™” ì •ë³´ê°€ intrachain structureì—ëŠ” ê°•ë ¥í•˜ì§€ë§Œ, interchainì—ëŠ” ì•½í•˜ê¸° ë•Œë¬¸ì´ë‹¤.

**Inference**: ê¸°ë³¸ì ìœ¼ë¡œ 5ê°œì˜ model seed Ã— 5ê°œì˜ diffusion sample = 25ê°œ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ê³ , confidence metricìœ¼ë¡œ top-1ì„ ì„ íƒí•œë‹¤. í•­ì²´-í•­ì› ë³µí•©ì²´ëŠ” íŠ¹íˆ ì–´ë ¤ì›Œì„œ, 1,000ê°œì˜ seedë¥¼ ì‚¬ìš©í•´ì•¼ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í•œë‹¤.

## Results

AlphaFold 3ì˜ ì„±ëŠ¥ì„ ì„¸ ê°€ì§€ ì£¼ìš” ë²¤ì¹˜ë§ˆí¬ì—ì„œ í‰ê°€í–ˆë‹¤: PoseBusters (protein-ligand), recent PDB set (protein-nucleic, modifications, protein-protein), CASP15 RNA.

![AlphaFold 3 Performance](/assets/img/posts/alphafold3-accurate-biomolecular-interactions/fig1.png)
_Figure 1: AF3ì˜ ë‹¤ì–‘í•œ ìƒì²´ë¶„ì ë³µí•©ì²´ì— ëŒ€í•œ ì„±ëŠ¥. (c) PoseBusters, recent PDB, CASP15 RNAì—ì„œì˜ ì •í™•ë„ ë¹„êµ_

**Protein-ligand binding (PoseBusters)**: PoseBustersëŠ” 428ê°œì˜ ìµœì‹  protein-ligand êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ë©°, pocket-aligned ligand RMSD < 2Ã…ë¥¼ ì„±ê³µìœ¼ë¡œ ì •ì˜í•œë‹¤. AF3ëŠ” **76.3%**ì˜ ì„±ê³µë¥ ì„ ë‹¬ì„±í•˜ì—¬, ì „í†µì  docking toolì¸ AutoDock Vina (45.2%)ì™€ deep learning ê¸°ë°˜ RoseTTAFold All-Atom (56.1%)ì„ í¬ê²Œ ì•ì„ ë‹¤ (Fisher's exact test, P < 10â»Â¹â°).

ì¤‘ìš”í•œ ì ì€ AF3ê°€ **blind docking**ì´ë¼ëŠ” ê²ƒì´ë‹¤. Vinaë‚˜ Gold ê°™ì€ ì „í†µì  ë„êµ¬ëŠ” ì‹¤í—˜ì ìœ¼ë¡œ ê²°ì •ëœ protein structureë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì§€ë§Œ, AF3ëŠ” ì˜¤ì§ ì„œì—´ê³¼ SMILESë§Œ ì‚¬ìš©í•œë‹¤. ì‹¤ì œ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ì—ì„œëŠ” AF3ê°€ í›¨ì”¬ ìœ ìš©í•˜ë‹¤.

| Method | Input | Success (%) | P-value |
|--------|-------|-------------|---------|
| **AF3** | Sequence + SMILES | **76.3** | - |
| RF All-Atom | Sequence + SMILES | 56.1 | 4.45Ã—10â»Â²âµ |
| Vina | Structure + SMILES | 45.2 | 2.27Ã—10â»Â¹Â³ |
| Gold | Structure + SMILES | 51.3 | 8.31Ã—10â»Â¹â¸ |

**Protein-nucleic acid interactions**: Recent PDB setì—ì„œ protein-RNAì™€ protein-DNA ë³µí•©ì²´ë¥¼ í‰ê°€í–ˆë‹¤ (<1000 residues). AF3ëŠ” interface LDDT **79.4%**ë¥¼ ë‹¬ì„±í•˜ì—¬ RoseTTAFold2NA (72.1%)ë³´ë‹¤ ë†’ë‹¤ (P = 2.78Ã—10â»Â³). CASP15 RNA targetsì—ì„œë„ í‰ê·  LDDT **72.8%**ë¡œ AIchemy_RNA (68.5%)ì„ ì•ì„°ì§€ë§Œ, ì¸ê°„ ì „ë¬¸ê°€ê°€ ê°œì…í•œ AIchemy_RNA2 (77.3%)ë³´ë‹¤ëŠ” ë‚®ë‹¤.

**Covalent modifications**: Glycosylation, bonded ligands, modified residuesì˜ pocket RMSD < 2Ã… ì„±ê³µë¥ ì€ ê°ê° **46.1%** (n=167), **58.7%** (n=89), **74.3%** (n=156)ì´ë‹¤. ì´ëŠ” baselineì´ ì—†ì–´ ì ˆëŒ€ í‰ê°€ëŠ” ì–´ë µì§€ë§Œ, AlphaFold-Multimer v.2.3ì´ ì´ëŸ° modificationì„ ì „í˜€ ë‹¤ë£¨ì§€ ëª»í•˜ëŠ” ê²ƒê³¼ ë¹„êµí•˜ë©´ í° ì§„ì „ì´ë‹¤.

**Protein-protein and antibody-antigen**: Protein-protein interfaceì—ì„œ DockQ > 0.23 ì„±ê³µë¥ ì€ **78.9%**ë¡œ AlphaFold-Multimer v.2.3 (73.4%)ë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ë†’ë‹¤ (P = 1.8Ã—10â»Â¹â¸). íŠ¹íˆ í•­ì²´-í•­ì› ë³µí•©ì²´ëŠ” **69.2%** (1,000 seeds ì‚¬ìš©)ë¡œ AF-M v.2.3 (56.9%)ë³´ë‹¤ í¬ê²Œ ê°œì„ ë˜ì—ˆë‹¤ (P = 6.5Ã—10â»âµ).

> Protein monomer LDDTë„ í‰ê·  **87.3%**ë¡œ AF-M v.2.3 (85.1%)ë³´ë‹¤ í–¥ìƒë˜ì—ˆë‹¤ (P = 1.7Ã—10â»Â³â´). AF3ëŠ” ë²”ìš©ì„±ì„ ì–»ìœ¼ë©´ì„œë„ ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìœ ì§€í–ˆë‹¤.
{: .prompt-tip }

**Confidence calibration**: AF3ì˜ confidence metrics (pLDDT, ipTM, PAE)ëŠ” ì‹¤ì œ ì •í™•ë„ì™€ ì˜ correlationëœë‹¤. ipTM > 0.8ì¸ protein-protein interfaceëŠ” í‰ê·  DockQ 0.75, ipTM < 0.4ëŠ” DockQ 0.15ë¡œ ëšœë ·í•˜ê²Œ êµ¬ë¶„ëœë‹¤. pLDDT > 90ì¸ ligandëŠ” 90% ì´ìƒì´ RMSD < 2Ã…ë¥¼ ë‹¬ì„±í•œë‹¤.

![Confidence Calibration](/assets/img/posts/alphafold3-accurate-biomolecular-interactions/fig4.png)
_Figure 4: AF3 confidence metrics (ipTM, pLDDT)ëŠ” ì‹¤ì œ accuracy (DockQ, LDDT)ì™€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì¸ë‹¤._

## Discussion

AlphaFold 3ëŠ” ìƒì²´ë¶„ì êµ¬ì¡° ì˜ˆì¸¡ì„ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ë¡œ í†µí•©í–ˆë‹¤ëŠ” ì ì—ì„œ ì¤‘ìš”í•œ ì§„ì „ì´ë‹¤. ë…¼ë¬¸ì€ ëª‡ ê°€ì§€ í•µì‹¬ ë°œê²¬ì„ ê°•ì¡°í•œë‹¤.

**ì²«ì§¸, cross-entity evolutionary informationì´ ì—†ì–´ë„ ë†’ì€ ì •í™•ë„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆë‹¤.** ë‹¨ë°±ì§ˆ-ë‹¨ë°±ì§ˆ ìƒí˜¸ì‘ìš©ì€ MSAì—ì„œ co-evolution signalì„ ì°¾ì„ ìˆ˜ ìˆì§€ë§Œ, ë‹¨ë°±ì§ˆ-ë¦¬ê°„ë“œë‚˜ ë‹¨ë°±ì§ˆ-RNAì—ëŠ” ê·¸ëŸ° signalì´ ì—†ë‹¤. ê·¸ëŸ¼ì—ë„ AF3ê°€ ëª¨ë“  interaction typeì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤ëŠ” ê²ƒì€, ëª¨ë¸ì´ chemistryì™€ physicsë¥¼ ì§ì ‘ í•™ìŠµí–ˆë‹¤ëŠ” ì˜ë¯¸ë‹¤.

**ë‘˜ì§¸, í•­ì²´-í•­ì› ì˜ˆì¸¡ ê²°ê³¼ëŠ” MSA-free predictionì˜ ê°€ëŠ¥ì„±ì„ ì‹œì‚¬í•œë‹¤.** í•­ì²´ëŠ” highly variableí•˜ì—¬ MSA depthê°€ ë‚®ì€ë°ë„, AF3ê°€ AlphaFold-Multimerë³´ë‹¤ í›¨ì”¬ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¸ë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë‹¨ìˆœíˆ homologyë¥¼ ì°¾ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ë¬¼ë¦¬ì  ìƒí˜¸ì‘ìš© ì›ë¦¬ë¥¼ í•™ìŠµí–ˆìŒì„ ë³´ì—¬ì¤€ë‹¤.

**ì…‹ì§¸, ë‹¨ë°±ì§ˆ êµ¬ì¡° ì˜ˆì¸¡ê³¼ ligand dockingì˜ ì¸ìœ„ì  ë¶„ë¦¬ê°€ ë” ì´ìƒ í•„ìš” ì—†ë‹¤.** ì „í†µì ìœ¼ë¡œ ë‹¨ë°±ì§ˆ êµ¬ì¡°ëŠ” AlphaFoldë¡œ, ë¦¬ê°„ë“œ dockingì€ AutoDockìœ¼ë¡œ ë”°ë¡œ ì§„í–‰í–ˆë‹¤. AF3ëŠ” ì´ë¥¼ end-to-endë¡œ í†µí•©í•˜ì—¬, ë” ë†’ì€ ì •í™•ë„ì™€ ê°„ë‹¨í•œ ì›Œí¬í”Œë¡œìš°ë¥¼ ì œê³µí•œë‹¤.

ë…¼ë¬¸ì€ ë˜í•œ **í•œê³„**ë¥¼ ëª…í™•íˆ ë°íŒë‹¤:

1. **Stereochemistry violations**: Chirality violationì´ 4.4%ì—ì„œ ë°œìƒí•˜ë©°, ë“œë¬¼ê²Œ ì „ì²´ chainì´ ê²¹ì¹˜ëŠ” clashingë„ ê´€ì°°ëœë‹¤ (íŠ¹íˆ í° protein-nucleic complex).

2. **Hallucination**: Disordered regionì—ì„œ spurious structureë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. Cross-distillationìœ¼ë¡œ ë§ì´ ì¤„ì—ˆì§€ë§Œ ì™„ì „íˆ ì œê±°ë˜ì§€ëŠ” ì•Šì•˜ë‹¤.

3. **Conformational diversity ë¶€ì¡±**: Diffusion samplingìœ¼ë¡œ ì—¬ëŸ¬ êµ¬ì¡°ë¥¼ ìƒì„±í•˜ì§€ë§Œ, ì´ê²ƒì´ solution ensembleì„ ëŒ€í‘œí•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ cereblonì€ apo stateì—ì„œ open conformationì„ ê°€ì ¸ì•¼ í•˜ëŠ”ë°, AF3ëŠ” í•­ìƒ closed conformationì„ ì˜ˆì¸¡í•œë‹¤.

4. **Antibody predictionì˜ ë†’ì€ computational cost**: í•­ì²´-í•­ì›ì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ë ¤ë©´ 1,000ê°œì˜ seedê°€ í•„ìš”í•˜ì—¬, ë‹¤ë¥¸ targetì— ë¹„í•´ 200ë°° ë§ì€ ê³„ì‚°ì´ í•„ìš”í•˜ë‹¤.

ë…¼ë¬¸ì€ í–¥í›„ ë°©í–¥ìœ¼ë¡œ **cryo-EMê³¼ tomographyì˜ ë°œì „**ì„ ì–¸ê¸‰í•œë‹¤. ì‹¤í—˜ êµ¬ì¡° ê²°ì • ê¸°ìˆ ì´ ë°œì „í•˜ë©´ì„œ ë” ë§ì€, ë” ë‹¤ì–‘í•œ ë³µí•©ì²´ êµ¬ì¡°ê°€ PDBì— ì¶”ê°€ë  ê²ƒì´ê³ , ì´ëŠ” AF3 ê°™ì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ë”ìš± í–¥ìƒì‹œí‚¬ ê²ƒì´ë¼ê³  ì „ë§í•œë‹¤.

> ì €ìë“¤ì€ "structural modelling will continue to improve not only due to advances in deep learning but also because continuing methodological advances in experimental structure determination"ì´ë¼ê³  ê°•ì¡°í•˜ë©°, ì‹¤í—˜ê³¼ ê³„ì‚°ì˜ ì„ ìˆœí™˜ ë°œì „ì„ ê¸°ëŒ€í•œë‹¤.
{: .prompt-info }

## TL;DR

- AlphaFold 3ëŠ” diffusion ê¸°ë°˜ ì•„í‚¤í…ì²˜ë¡œ ë‹¨ë°±ì§ˆ, í•µì‚°, ë¦¬ê°„ë“œ, ì´ì˜¨ ë“± ëª¨ë“  ìƒì²´ë¶„ì ë³µí•©ì²´ì˜ êµ¬ì¡°ë¥¼ ë‹¨ì¼ í”„ë ˆì„ì›Œí¬ì—ì„œ ì˜ˆì¸¡í•œë‹¤.
- AF2ì˜ IPA structure moduleì„ diffusion moduleë¡œ êµì²´í•˜ê³ , evoformerë¥¼ pairformerë¡œ ê°„ì†Œí™”í•˜ì—¬ MSA ì˜ì¡´ì„±ì„ ì¤„ì˜€ë‹¤.
- PoseBusters (protein-ligand)ì—ì„œ 76.3% ì„±ê³µë¥ ë¡œ AutoDock Vina (45.2%)ì™€ RoseTTAFold All-Atom (56.1%)ì„ í¬ê²Œ ì•ì„œë©°, í•­ì²´-í•­ì› ì˜ˆì¸¡ë„ AlphaFold-Multimer v.2.3 ëŒ€ë¹„ 12.3%p í–¥ìƒë˜ì—ˆë‹¤.

## Paper Info

| í•­ëª© | ë‚´ìš© |
|---|---|
| **Title** | Accurate structure prediction of biomolecular interactions with AlphaFold 3 |
| **Authors** | Josh Abramson et al. (Google DeepMind) |
| **Venue** | Nature (2024) |
| **Published** | 8 May 2024 |
| **Paper** | [Nature](https://www.nature.com/articles/s41586-024-07487-w) |
| **Code** | AlphaFold Server (inference only, weights not released) |

---

> ì´ ê¸€ì€ LLM(Large Language Model)ì˜ ë„ì›€ì„ ë°›ì•„ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. 
> ë…¼ë¬¸ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë‚˜, ë¶€ì •í™•í•œ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
> ì˜¤ë¥˜ ì§€ì ì´ë‚˜ í”¼ë“œë°±ì€ ì–¸ì œë“  í™˜ì˜í•©ë‹ˆë‹¤.
{: .prompt-info }
